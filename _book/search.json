[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA 545 Tutorial",
    "section": "",
    "text": "Preface\nThis book contains notes and materials for the Calvin University course DATA 545 (Applied Regression Modeling).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "1  Installation",
    "section": "",
    "text": "1.1 Motivation\nOne way to access R and RStudio is via an account at https://r.stem.calvin.edu/ (or if not a Calvin student, at posit.cloud). To use RStudio on one of these servers, all you need to do is log in, with nothing to install or maintain.\nFor many, this cloud approach is a great way to use RStudio, and they have no reason to install a standalone copy of the software on a personal computer. If you are happy using the server, exit this tutorial now and continue happily using the server!\nIf you have concerns about your internet bandwidth, speed, usage limits, or firewalls, or if you want to be able to work on assignments for this class somewhere without internet access, or if you need to analyze really large datasets or fit complex models, you may want to install R and RStudio and work locally instead of on the server.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "install-r.html#motivation",
    "href": "install-r.html#motivation",
    "title": "1  Installation",
    "section": "",
    "text": "Pros and Cons\nThe benefits of downloading your own copy are that you can work offline and should not be subject to any (hopefully rare) server-related errors, freezing, etc.\nThe negatives of downloading your own copy are that you have to maintain it yourself, installing and updating packages and software.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "install-r.html#goal",
    "href": "install-r.html#goal",
    "title": "1  Installation",
    "section": "1.2 Goal",
    "text": "1.2 Goal\nThis document will guide you through the process of installing R, RStudio, and other necessary R packages on your own computer, if you choose to do so. Again, there is no course requirement to do this.\nThe process will have three stages, which work best in order:\n\nInstall R\nInstall RStudio\nInstall necessary R packages from within RStudio",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "install-r.html#downloadinstall-1-of-2-r",
    "href": "install-r.html#downloadinstall-1-of-2-r",
    "title": "1  Installation",
    "section": "1.3 Download/Install 1 of 2: R",
    "text": "1.3 Download/Install 1 of 2: R\nR downloads are available from https://cran.r-project.org/.\n\n\n\n\n\n\n\n\n\n\nSelect the download that matches your operating system and hardware (Mac OS, Windows, Linux, etc.)\nYou only need the “base” version.\nDownload the installer and run it. You may want to choose not to create shortcuts, since you will access R only through RStudio.\n\n\nMac with Homebrew\nWindows and Linux users: skip this section.\n\nIf working on Mac OS and already using Homebrew to manage software packages, you can skip the manual download above and just run:\n\n\nbrew install r\n\n\nIf you want to get Homebrew and install this way on a mac, there are detailed instructions online – scroll down to “Instructions for Mac Users”. Note that you don’t necessarily need OpenBLAS for this course (as recommended on the linked website); it does not really matter either way.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "install-r.html#downloadinstall-2-of-2-rstudio",
    "href": "install-r.html#downloadinstall-2-of-2-rstudio",
    "title": "1  Installation",
    "section": "1.4 Download/Install 2 of 2: RStudio",
    "text": "1.4 Download/Install 2 of 2: RStudio\nOnce you have installed R, you next need to install RStudio.\n\nDownloads of RStudio are available at https://rstudio.com/products/rstudio/download/.\nYou should select the free version of RStudio Desktop.\nDownload and install the version that matches your operating system\n\n\n\n\n\n\n\n\n\n\n\nMac with Homebrew\nWindows and Linux users: you can’t use Homebrew.\n\nIf using a mac and Homebrew, you can alternatively install RStudio via:\n\n\nbrew cask install rstudio",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "install-r.html#install-33-packages",
    "href": "install-r.html#install-33-packages",
    "title": "1  Installation",
    "section": "1.5 Install 3/3: Packages",
    "text": "1.5 Install 3/3: Packages\nIn addition to base R and the RStudio IDE, we use a few add-on packages that you will need to install yourself.\n\nOpen RStudio\nIn your RStudio Console window, which is on the lower left by default, type (or copy and paste) the code below and click “Enter” to run it:\n\n\ninstall.packages(c('rmarkdown', # reproducible research documents\n                   'tidyverse', 'remotes', # graphics and data wrangling\n                   'pander', # formatting tables\n                   'glmmTMB', 'mgcv', # fitting regression models\n                   'car', 'ggeffects'#,  working with fitted models\n                   # optional additions:\n                   # 'mosaic', # formula-based summary stats and resampling\n                   # 'openintro', # datasets\n                   # 'shiny', 'plotly', 'gganimate', 'leaflet' # interactive graphics/maps\n                   )) \n# if desired, for function to ggplot ACFs:\nremotes::install_github('stacyderuiter/s245')\n\n\nIn addition to the packages you listed specifically, a number of dependencies (other packages that the packages you requested require to work) will be installed.\nThe amount of time it takes will depend on your computer and internet connection speed, but as long as it finishes without any messages that literally say “Error: …”, it worked!.\nIf RStudio prompts you to update packages or install additional dependencies, it’s usually a good idea to do so.\nIf R asks you if you want to install a certain package “from source” blah, blah, “is newer…” usually you can answer yes (or no) and it will work either way.\nIf you get an error or have any questions, get in touch with your professor.\n\n\nTeX for PDF generation\nTo enable generation of PDF output from Rmarkdown documents, there is a little more code to run. (If you don’t know what this means yet, you will soon - and you do probably need to be able to do it.)\nThis one has two steps: installing the package, and then using the package to install the PDF-generation utility.\nIf you already have TeX/LaTeX/MikTeX installed on your computer, you can probably skip this installation (but it won’t hurt).\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX\n\n\n\nPoint-and-Click Option\nIf you would like to install the packages interactively instead of on the command line (as already shown above), you can click the Packages tab on the lower right in RStudio, then click Install at the top of the tab. Enter the names of the packages you want to install in the middle “Packages” blank, and leave the rest of the default options, then click “Install”.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "install-r.html#you-did-it",
    "href": "install-r.html#you-did-it",
    "title": "1  Installation",
    "section": "1.6 You did it!",
    "text": "1.6 You did it!\nIf you complete all three steps above, you should have a working version of RStudio on your machine. To use it, just open RStudio; it should look nearly identical to the RStudio Server version you have been using online.\nYou don’t ever have to open or access R directly; RStudio does it all for you.\nIn case of any errors or problems, contact me (stacy.deruiter at calvin.edu) anytime and I’ll do my best to help.\n(Don’t contact school help desks; they don’t support this software).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installation</span>"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "2  R Basics",
    "section": "",
    "text": "2.1 Your Mission\nThe purpose of this tutorial is to help you start to get familiar with the way R works, and some basic R commands…even if you haven’t yet installed R on your computer or made a posit.cloud account.\nThis tutorial environment uses webr, which lets you read some helpful information, then immediately practice writing and running your own R code, all in your web browser.\nHere’s hoping it provides a nice, gentle introduction in a controlled environment!",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#communicating-with-r",
    "href": "r-basics.html#communicating-with-r",
    "title": "2  R Basics",
    "section": "2.2 Communicating with R",
    "text": "2.2 Communicating with R\nYou will do most of your work in R with code or commands. Instead of pointing and clicking, you will type one or more lines of code, which R will execute (doing the work you have asked it to do).\nThen, R will return the results of whatever operation you asked it to do - sometimes producing a plot, other times creating a plot.\nSometimes executing code has almost no visible effect (no plot or text output is produced), but instead some object is created and stored in R’s environment for later use.\n\nTwo Key Questions\nTo get R (or any software) to do something for you, there are two important questions you must be able to answer. Before continuing, think about what those questions might be.\n\n\nThe Questions\nTo get R (or any software) to do a job for you, there are two important questions you must be able to answer:\n\n1. What do you want the computer to do?\n\n\n2. What must the computer know in order to do that?\n\n\n\nProviding R with the information it needs\nR functions provide R with the answer to the first question: what do you want the computer to do?\nMost functions in R have short, but descriptive names that describe what they do. For example, R has some functions to do basic mathematical operations: the function sqrt() computes the square root of a number, and the function round() rounds a number (by default, it rounds to the nearest integer).\nBut just giving R a function is not enough: you also need to answer the second question (what information does R need to do the job?). For example, if you want to use the function round(), you also need to provide R with the number you want to round!\nWe will provide answers to our two questions by filling in the boxes of a basic template:\n\n\nfunction (  information1  ,  information2  , …)\n\n\n \n(The ... indicates that there may be some additional input arguments (input information we could provide to R) we could add eventually. Some functions need only one input, but if a function takes more than one argument, they are separated by commas. They have names, and if named (like: function(input_name = value, input2_name = 'value')) they can be in any order.\n\n\nUsing simple functions\nLet’s practice what you just learned, trying out the mathematical sqrt() and round() functions.\nEdit the code below to compute the square root of 64:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nConsider using the sqrt() function:\nsqrt(___)\n\n\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nThe input information that sqrt() needs to make your calculation is the number you want the square root of: 64.\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nsqrt(64)\n\n\n\n\nNow try computing the square root of 44, and then rounding it to the nearest integer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nYou’ll need to use two functions this time:\nThe sqrt() function, and then the round() function.\nsqrt(___)\nround(___)\n\n\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nThe input information that sqrt() needs to make your calculation is the number you want the square root of: 44. Run that code first, to get the input you will need for round()…\nsqrt(44)\nround(___)\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nsqrt(44)\nround(6.63325)\nCan you do it all in one go? Well…yes!\nround(sqrt(44))\nThere’s also an easier-to-read way to do that, using a pipe operator |&gt;. It takes the output of one operation and passes it as input to the next. You can read it as |&gt; = “and then…” so we could do:\nsqrt(44) |&gt;\n  round()\n\nTake the square root of 44, and then\nround the result.\n\n(More on pipes later!)\n\n\n\n\n\n\nStoring information in R: variables\nIn the last section, you computed the square root of 44 and then rounded it, perhaps like this:\n\nsqrt(44)\n\n[1] 6.63325\n\nround(6.63325)\n\n[1] 7\n\n\nBut to do that, you probably had to first find the root, make a note of the result, and then provide that number to the round function. What a pain!\nA very useful option, if you have value (or a variable, dataset, or other R object) that you will want to use later on, is to store it as a named object in R. In the previous example, you might want to store the square root of 44 in a variable called my_root; then you can provide my_root to the round() function without checking the result of the sqrt() calculation first:\n\nmy_root &lt;- sqrt(44)\nround(my_root)\n\n[1] 7\n\n\nNotice that to assign a name to the results of some R code, you use the symbol &lt;-. You can think of it as an assignment arrow – it points from a value or item toward a name and assigns the name to the thing.\nTry editing the code to change the name of the variable from my_root to something else, then run your new code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure you change the name my_root in both places.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nyour_new_name &lt;- sqrt(44)\nround(your_new_name)\n\n\n\n\n\n\nWhat if I have a list of numbers to store?\nSometime you might want to create a variable that contains more than one number. You can use the function c() to concatenate a list of numbers:\n\nmy_fave_numbers &lt;- c(4, 44, 16)\nmy_fave_numbers\n\n[1]  4 44 16\n\n\n(First we stored the list of numbers, calling it my_fave_numbers; then we printed the results to the screen by simply typing the variable name my_fave_numbers).\nTry making a list of your three favorite numbers, then using the function sum to add them all up:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFirst use c() to concatenate your chosen numbers (separated by commas).\nDon’t forget to use &lt;- to assign your list of numbers a name!\nThen, use sum() to add them up.\nmy_numbers &lt;- c(___,___,___)\nsum(___)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is just one possible solution.\nmy_numbers &lt;- c(4, 16, 44)\nsum(my_numbers)\nNotice you could also nest sum() and c(), or use the pipe operator |&gt; to calculate the numeric answer, but then you would not have the object my_numbers available for later use…\nsum(c(4, 16, 44))\n# or \nc(4, 16, 44) |&gt;\n  sum()\n\n\n\n\n\n\nWhat about data that are not numeric?\nR can work with categorical data as well as numeric data. For example, we could create a list of words and store it as a variable if we wanted (feel free to try changing the words if you want):\n\n\n\n\n\n\n\n\n\n\nWhat if I have a LOT more data to store?\nc() works great for creating small lists of just a few values, but it is not a good way to enter or store large data tables - there is lots of potential for user error. In this course, you will usually be given a dataset already in electronic form; if you need to create one, you would turn to spreadsheet or database software. Either way you read the existing data file into R directly.\nIn R, these larger datasets are stored as objects called data.frames. The next sections will get you started using them.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#how-should-data-tables-be-organized-for-statistical-analysis",
    "href": "r-basics.html#how-should-data-tables-be-organized-for-statistical-analysis",
    "title": "2  R Basics",
    "section": "2.3 How should data tables be organized for statistical analysis?",
    "text": "2.3 How should data tables be organized for statistical analysis?\nA comprehensive guide to good practices for formatting data tables is available at http://kbroman.org/dataorg/.\nA few key points to keep in mind:\n\nThis data table is for the computer to read, not for humans! So eliminate formatting designed to make it “pretty” (color coding, shading, fonts…)\nUse short, simple variable names that do not contain any spaces or special characters (like ?, $, %, -, etc.)\nOrganize the table so there is one column for every variable, and one row for every observation (person/place/thing for which data were collected).\nUse informative variable values rather than arbitrary numeric codes. For example, a variable Color should have values ‘red’, ‘white’, and ‘blue’ rather than 1, 2, and 3.\n\nYou will have chances to practice making your own data files and importing them into R outside this tutorial.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#using-built-in-datasets-in-r",
    "href": "r-basics.html#using-built-in-datasets-in-r",
    "title": "2  R Basics",
    "section": "2.4 Using built-in datasets in R",
    "text": "2.4 Using built-in datasets in R\nR has a number of built-in datasets that are accessible to you as soon as you start RStudio.\nIn addition to the datasets that are included with base R, there are add-on packages for R that contain additional software tools and sometimes datasets.\nTo use datasets contained in a package, you have to load the package by running the command:\n\nlibrary(packagename) \n\n\nExample of loading a package\nFor example, we will practice looking at a dataset from the package mosaic.\nBefore we can access the data, we have to load the package. The code might look like this:\n\nlibrary(mosaic)\n\n(Nothing obvious will happen when you run this code…it basically just gives R permission to access the package, so there is often no output visible.)\n\n\nViewing a dataset\nThe mosaic package includes a dataset called HELPrct.\nIf you just run the dataset name (HELPrct) as a command, R will print some (or all - egad!) of the dataset out to the screen! (So don’t…)\nBut…how can we extract selected, useful information about a dataset?\n\n\nGathering information about a dataset\nThere are a few functions that make it easier to take a quick look at a dataset:\n\nhead() prints out the first few rows of the dataset.\nnames() prints out the names of the variables (columns) in the dataset\ndplyr::glimpse() (function glimpse() from package dplyr) gives an short list-like overview of the dataset\nskimr::skim() (function skim() from the package skimr) prints out more detailed graphical summary information about a dataset\nnrow() reports the number of rows (observations or cases) in the dataset\nncol() reports the number of columns (variables) in the dataset\n\nTry applying each of these functions to the HELPrct data and see what the output looks like each time:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe input for each of the functions is the name of the dataset: HELPrct.\nhead(HELPrct)\nnames(HELPrct)\nnrow(HELPrct)\nncol(HELPrct)\nskimr::skim(HELPrct)\ndplyr::glimpse(HELPrct)\nIn this case, the point is usually to view the information on-screen, not to store it for later use, so we have not used &lt;- at all to store any output for later use or reference.\n\n\n\n\n\n\nGetting more help\nYou can get help related to R function, and built-in R datasets, using a special function: ?. Just type ? followed by the name of the function or dataset you want help on:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor example, if you want to know about the function nrow():\n?nrow",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#reading-in-data-from-a-file",
    "href": "r-basics.html#reading-in-data-from-a-file",
    "title": "2  R Basics",
    "section": "2.5 Reading in data from a file",
    "text": "2.5 Reading in data from a file\nFor this class, you will often be asked to analyze data that is stored in files that are available online - usually in csv format. It’s simple to read them into R. For example, we can read in the file MI_lead.csv, which is stored at https://sldr.netlify.app/data/MI_lead.csv using the function read_csv() (from package readr or super-package tidyverse):\n\nlibrary(readr) # the readr package contains the read_csv() function\nMI_lead &lt;- read_csv(file = 'https://sldr.netlify.app/data/MI_lead.csv')\n\n\nThe most common mistakes\nThe code below contains several of the most common mistakes students make when they try to read in a data file. See if you can find and correct them all!\nThe code below - if corrected - would (on posit.cloud or in standalone R/RStudio) run without an error and read in some baseball statistics from the file http://stacyderuiter.github.io/teachingdata/data-raw/baseball.csv.\nHere in this tutorial, it may give the error: ! curl package not installed, falling back to using url() – there’s not a straightforward fix, sorry, but try it on the server if you want to prove to yourself that it works!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThink about:\n\nIs the filename or URL spelled correctly, with no typos?\nIs the filename or URL in quotation marks (either ” or ’ work equally)?\nIs the URL complete (including the file extension “.csv”)\nWas &lt;- used to assign a name to the dataset once read in? (Otherwise it will just be uselessly printed to the screen and not available for later use!)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nbaseball_data &lt;- read_csv(file = 'http://stacyderuiter.github.io/teachingdata/data-raw/baseball.csv')\n\n\n\n\n\n\nWhat about local files?\nThe same function, read_csv(), can be used to read in a local file. You just need to change the input to read_csv() – instead of a URL, you provide a path and filename (in quotes). For example, the input file = 'https://sldr.netlify.app/data/MI_lead.csv' might become file = 'C:\\\\Data\\\\MI_lead.csv'.\nWe won’t do an example in this tutorial because it’s not straightforward to work with local files within a tutorial environment, but you can practice it once you are working independently in RStudio.\nIf you are working on the server r.cs.calvin.edu, you will have to upload files to your cloud space on the server before you can read them in (RStudio on the server cannot access files on your computer’s hard drive). Look in the “Files” tab on the lower right, and then click “Upload.”\n\n\nNamed input arguments\nThe input argument we provided to R is the URL (in quotes – either single or double quotes are fine). But notice that this time, we gave the input argument a name, “file”, and specified its value with an equal sign.\nThis is not required - the command works fine without it:\n\nMI_lead &lt;- read_csv('https://sldr.netlify.app/data/MI_lead.csv')\n\nHowever, if a function has more than just one input argument, it’s good to get in the habit of providing names for the inputs. If you provide names, then the order in which you list the inputs doesn’t matter; without names, the order matters and you have to use ? to figure out what order R expects!\n\n\nRenaming variables in a dataset\nThis is an advanced topic, so don’t worry if it seems complicated; for now, it just nice to realize some of the power R has to clean up and reorganize data.\nWhat if we didn’t like the names of the MI_lead variables? For example, a new user of the dataset might not know that that ELL stands for “elevated lead levels” and that ELL2005 gives the proportion of tested kids who had elevated lead levels in the year 2005.\nIf we wanted to use a clearer (though longer) variable name, we might prefer “prop_elevated_lead_2005” instead of “ELL2005” – more letters to type, but a bit easier to understand for a new user. How can we tell R we want to rename a variable?\nWe use the code:\n\nMI_lead &lt;- MI_lead |&gt;\n  rename(prop_elevated_lead_2005 = ELL2005)\n\nglimpse(MI_lead)\n\nThe code above uses some tools you’ve seen, and some more advanced ones you haven’t seen yet. The symbol |&gt; is called a “pipe” and basically means “and then…” Translated into words, the code above tells R:\n\nMake a dataset called MI_lead by starting with the dataset MI_lead.\nNext, take the results do something more with them (|&gt;) …\nrename() a variable. What I want to rename is the variable ELL2005. Its new name should be prop_elevated_lead_2005.”\n\nSee…you can already start to make sense of even some pretty complicated (and useful) code.\nNote: If you give R several commands, not connected by pipes, it will do the first, then the second, then the third, and so on. R doesn’t need the pipe for permission to continue! Instead, the pipe tells R to take the results from the first command, and use them as the input or starting material for the next command.\n\n\nCheck out the data\nOK, back to business - simple functions and datasets in R.\nIt’s your turn to practice now. Use one of the functions you have learned so far to extract some information about the MI_lead dataset.\nHow many rows are in the dataset? How many variables (columns)?\nWhat are the variables named, and what are their values like?\nRemember, ? won’t work on MI_lead because it’s not a built-in R dataset. Also, the dataset MI_lead is already read in for you, here…so you don’t need to use read_csv().",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#review",
    "href": "r-basics.html#review",
    "title": "2  R Basics",
    "section": "2.6 Review",
    "text": "2.6 Review\nWhat have you learned so far? More than you think!\n\nFunctions in R\nYou’ve learned that R code is made up of functions, which are generally named descriptively according to the job they do. Functions have one or more input arguments, which is where you provide R with all the data and information it needs to do the job. The syntax for calling a function uses the template:\n\n\nfunction (  information1  ,  information2  , …)\n\n\n \n\n\nVariables in R\nYou’ve practiced creating variables in R using c(), and saving information (or the results of a computation) using the assignment arrow &lt;-.\n\n\nDatasets in R\nYou’ve considered several different ways to get datasets to work with in R: you can use datasets that are built in to R or R packages, or you can use read_csv() to read in data files stored in .csv format.\n\n\nVocabulary\nYou should now be able to define and work with some R-related terms:\n\ncode or commands that R can execute\nfunction and inputs or arguments\nassignment arrow: &lt;-\npipe = “and then…”: |&gt; (note: |&gt; is an older way of writing a pipe, and it does basically the same thing as |&gt;)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#congratulations",
    "href": "r-basics.html#congratulations",
    "title": "2  R Basics",
    "section": "2.7 Congratulations!",
    "text": "2.7 Congratulations!\nYou just completed your first tutorial on R, and wrote some of your own R code. I knew you could do it…\nWant more help and practice? Consider checking out outside resources from posit: https://posit.cloud/learn/primers",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html",
    "href": "how-to-quarto.html",
    "title": "3  Using Quarto",
    "section": "",
    "text": "3.1 Instructions\nWhile you work through this chapter, you will create a Quarto (.qmd) document.\nQuarto lets you combine R code, output, and text in a single document that can be rendered in HTML, PDF, Word and more formats.\nIt’s like magic: you save all your text and R code in a simple file; when you’re ready, push a button and it’s compiled into an output document with nicely formatted text, code (optional to include, but for this class you always will), and all the figures and tables generated by your code.\nSince all the data analysis and results are automatically included in the compiled output document, your work is reproducible and it’s easy to re-do analysis if the data change, or if a mistake is uncovered.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#reference-materials",
    "href": "how-to-quarto.html#reference-materials",
    "title": "3  Using Quarto",
    "section": "3.2 Reference Materials",
    "text": "3.2 Reference Materials\nFor more details on using Quarto, and detailed documentation, see https://Quarto.org/docs/guide/.\nQuarto and posit also provide substantial resources for learners. This tutorial is tailored to our course, including just the stuff you need and not much you won’t use frequently. But if you want even more about Quarto, you might check out:\n\nTutorials for beginners at https://Quarto.org/docs/get-started/hello/rstudio.html (Hello, Quarto! and Computations are most relevant.)\nDetailed documentation at https://Quarto.org/docs/guide/.\n\n\nOptional Video\nIf you love video introductions, consider also this 23-minute offering from posit and Mine Cetinkaya-Rundel:",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#logistics",
    "href": "how-to-quarto.html#logistics",
    "title": "3  Using Quarto",
    "section": "3.3 Logistics",
    "text": "3.3 Logistics\nTo create a .qmd file, you will have to work in RStudio (outside this tutorial environment). So, as you work on this tutorial, you will probably switch back and forth between the tutorial itself and an RStudio session on your computer or on the server at https://r.stem.calvin.edu (or if not at Calvin, at posit.cloud).\nHistorical Note: The precursor of the Quarto document is the Rmarkdown (.rmd) document (and even older - the Sweave document). If you know and love one of those, you may use it, but probably best to upgrade to Quarto, which is superceding them.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#getting-started",
    "href": "how-to-quarto.html#getting-started",
    "title": "3  Using Quarto",
    "section": "3.4 Getting Started",
    "text": "3.4 Getting Started\n\nLogging in to RStudio\nLog in to your account at https://r.stem.calvin.edu (or if not at Calvin, at posit.cloud).\nOr, if you installed R on your own machine, open RStudio.\n\n\nPanels\nWhen you open RStudio, you will see at least three different panels: The Console is on the left. On the upper right are Environment, History and maybe more; on the lower right are Files, Plots, and Packages. Explore a little to try to see what is there!\nFiles shows you the files saved in your personal space on the server. You can organize, upload, and delete files and folders.\n\n\nExecuting code in R\nYou can do things in R by typing commands in the Console panel.\nHowever, working that way makes it hard to keep a record of your work (and hard to redo things if anything changes or if a mistake was made).\nFor this class, you will instead work in Quarto files, which can contain text, R code, and R output (such as figures).\nAfter you have opened a file (like an RMarkdown file) on the RStudio server, the Console panel will be on the lower left and the newly opened file will be on the top left. Let’s learn how to do it…",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#quarto-qmd-files",
    "href": "how-to-quarto.html#quarto-qmd-files",
    "title": "3  Using Quarto",
    "section": "3.5 Quarto (qmd) Files",
    "text": "3.5 Quarto (qmd) Files\n\nQuarto files are stand-alone!\nEvery Quarto file (qmd file) must be completely stand-alone. It doesn’t share any information with the Console or the Environment that you see in your RStudio session. All R code that you need to do whatever you are trying to do must be included in the qmd file itself!\nFor example, if you use the point-and-click user interface in the RStudio Environment tab to import a data file, that dataset will not be available when rendering your qmd file.\nSimilarly, if you load the mosaic package by typing in the Console window,\n\nlibrary(mosaic)\n\nmosaic functions and data will not be available to use within the qmd file.\n\n\nSo: Keep your qmd files stand-alone! (You have no choice, actually…)\n\n\n\n\nCreate a Quarto file\nIn RStudio, navigate to File -&gt; New File -&gt; Quarto Document…, or click on the white rectangle with a green circle+ :\n\n\n\n\n\n\n\n\n\nand select Quarto from the drop-down menu.\nChoose html or pdf output.\n(Why not Word? Too much temptation to make changes and do formatting after the fact in Word…which makes your work no-longer-reproducible. In qmd, you have documented everything you’ve done. If you make changes after rendering to Word, that’s not true anymore.)\n\n\nSave your qmd file\nSave your file by clicking on the disk icon at the top of the file tab (give it a clear file name like deruiter_quarto_practice.qmd).\nDo your best to avoid spaces and special characters in your file names.\nIf on a server, the file will be saved to the cloud, not to your computer.\nAll your files will be accessible in the RStudio Files tab (lower right panel) whenever you log into RStudio, regardless of which computer you are using. You may organize them into directories (folders) if you want.\n\n\n\n\n\n\n\n\n\n\n\nRender!\nHow do qmd files actually work? What’s so cool about them?\nClick on the fat blue arrow next to the word “Render” at the top of the file window.\n\n\n\n\n\n\n\n\n\nCheck out the rendered html or pdf result, and compare it to the original Quarto file.\nWow!\n\n\nSource vs. Visual Editor\nLook to the upper right corner of your qmd file. You should see some buttons that allow you to toggle between “Source” and “Visual” editor modes.\n\n\n\n\n\n\n\n\n\nIn your own file, toggle back and forth a few times. The Source mode lets you see (and type) the straight-up markdown – which is probably nice if you’re already used to it, and annoying or mystifying if not. The Visual mode is more of a what-you-see-is-what-you-get (like the rendered version), point-and-click type interface. You may use whichever you prefer.\nBe aware that if you are going to copy/paste between documents, you probably want to do so in Source mode.\n\n\nPersonalize your Markdown file\nAt the top of the Quarto file, there is a section called the “YAML header”. It starts and ends with 3 dashes - - -.\n\n\nIn this part of the file, be very careful what you type: a stray space or character will lead to an error.\n\n\nThis is where you can enter an appropriate title, author(s), and date (within the quotation marks). You can also choose the format you want to render to (usually pdf or html – not in quotes).\n\n\n\n\n\n\n\n\n\nCustomize your YAML header in your own Quarto doc, and then render again to see the effect.\nMake sure you do this for every assignment! (No prof or boss likes getting submissions called “Untitled”…)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#quarto-yaml-settings",
    "href": "how-to-quarto.html#quarto-yaml-settings",
    "title": "3  Using Quarto",
    "section": "3.6 Quarto YAML settings",
    "text": "3.6 Quarto YAML settings\n\nPDF or html?\nFor our course, you can choose to render to either an html file or a PDF file.\nSo, you’ll have either format: pdf or format: html in your YAML header. You can also try format: typst to render PDF files a bit faster (learn more about typst output format online).\nBut if you choose html, there’s an important change you have to make to the YAML header to ensure your html file is stand-alone. Meaning: you want all images, etc. to be embedded in the one file rather than stored in an accompanying folder. Otherwise, when you (say) upload the file on Moodle or email it, all the images and graphs will be omitted…yikes! Yes, embedding these makes the file larger, but if you are sharing the rendered html document, you need to.\nIf rendering to html, it is essential that you specify the setting embed-resources: true!\nSo, make sure you add embed-resources: true after the entry format: html: in your YAML header, exactly as shown below.\nMake sure to keep the spacing and line breaks just as shown.\nThe indents are each two spaces, so there are 2 spaces before html: and 4 before embed-resources:.\n\n\n\n\n\n\n\n\n\n\n\nCode tools\nNote that the YAML header shown above also had a second option activated for rendered html files: code-tools: true.\n\n\n\n\n\n\n\n\n\nWhat does this one do?\nIt adds a button “Code” at the top right of your file.\n\n\n\n\n\n\n\n\n\nIf you click it, you can view and copy the source code (basically, the contents of the original qmd file before rendering). This is not a bad option, for example for homework, as it allows me to see every detail of the settings you used and may help me troubleshoot any issues.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#text-and-code-in-quarto",
    "href": "how-to-quarto.html#text-and-code-in-quarto",
    "title": "3  Using Quarto",
    "section": "3.7 Text and Code in Quarto",
    "text": "3.7 Text and Code in Quarto\n\nText\nThe Quarto file is where you save all the R commands you want to use, plus any text commenting on the work you are doing and the results you get. Parts of the file with a plain white background are normal text.\nYou can format the text. For example, enclosing a word in asterisks will generate italics, so *my text* in the qmd file will become my text in the PDF. Using two asterisks instead of one will generate boldface, so **my text** becomes my text. You can also make bulleted lists, numbered lists, section headers, and more. For example,\n#### Some Text\nbecomes\n\nSome Text\n(a sub-section header). Fewer hashtags make the text even larger, and more make it smaller.\nCaution! Forgetting the space after the last hashtag will format your text verbatim rather than as a header (#fail). Failing to leave a blank line before the header can also make formatting fail.\nCheck out the Quarto Markdown Basics reference at https://quarto.org/docs/authoring/markdown-basics.html for more examples of how to format text in Quarto.\nBefore moving on, try a few of the tricks you just learned in your qmd file. Make it pretty!\n\n\n\nqmd file anatomy: R code chunks\nAn qmd file can (of course!) contain one or more R code chunks. These sections of the file have a grey background onscreen. In Source mode, each one begins with\n```{r}\nand ends with\n```\nlike so:\n\n\n\n\n\n\n\n\n\nIn Visual mode you can’t see the `:\n\n\n\n\n\n\n\n\n\n\n\nHow to add a new R code chunk to your file\nTo add a code chunk to your file in Source editor mode, you have three options.\n\nYou can type in the header and footer by hand to start and end the chunk.\nYou can click on the “add chunk” button at the top right. It’s a green box with the C inside (at the top of the qmd file; choose the first option, “R”, in the pulldown) to insert an empty chunk.\nYou can use a keyboard shortcut: Windows, Ctrl + Alt + I or OS X, Cmd + Option + I\n\nWhen you click the Render button, code in code chunks will be run, and any output will be included in the document.\n\n\n\n\n\n\n\n\n\n\n\nSetup Chunk\nConsider using the first R code chunk in a qmd file to specify settings (for graphics, display, etc.). In this chunk, you can also give R permission to use certain packages (software toolkits) with\n\nlibrary(packagename) \n\nFor example, we will use the ggformula package for graphics. So, verify that the first R code chunk in your file includes the line library(ggformula).\nYou can also specify options for each R code chunk - these go at the top, prefaced by #|. A typical setup chunk for our course might look like:\n\n\n\n\n\n\n\n\n\nNotice that several packages are loaded (that we will use frequently). theme_set() is used to specify some settings for graph output, and knitr::opts_chunk$set() is used to specify whether or not to include R code in the rendered file (Yes please: use echo: true!) and specify the default figure size.\nThere are tons more options and settings, and you can explore them at https://yihui.org/renderr/options/#chunk-options.\nBut for now, if you use something like the setup chunk shown above, it should work well and have what you need for almost all work in this course.\n\n\nThe settings chunk is invisible!\nIf you look carefully at the rendered output, you will see that the setup chunk does not appear there. That’s intentional - when you load packages with library(), they often print a lot of long and pretty useless messages, which you want to omit from your rendered document.\nThis is achieved by having the setting include: false\nHowever, for our course, no chunk other than the setup chunk should have the setting “include: false” (or echo: false for that matter). Generally, anyone evaluating your coursework needs to see all the code you used, not just its output.\n\n\nClean Up\nAt this point, you probably want to get rid of all the extra content in the template.\nIf you haven’t put a setup chunk into your own qmd file…do it now! Here’s another reminder of how it would look:\n\n\n\n\n\n\n\n\n\nNext, Delete everything in the file other than the YAML header and your setup R code chunk.\nNow the clutter is gone and you have space to include your own R code and text.\n(Before going further, make sure it still renders.)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#run-r-code",
    "href": "how-to-quarto.html#run-r-code",
    "title": "3  Using Quarto",
    "section": "3.8 Run R Code",
    "text": "3.8 Run R Code\nThere are multiple ways to run and test R code from a markdown file. Sometimes you want to render the whole file and get the PDF or HTML; other times you want to run just a specific bit of code to make sure it’s working correctly.\n\nRunning R Code from a qmd file: Render the file\nEvery time you render the file, all R code will be run automatically.\nA side note: PDF or HTML? Which is preferable?\nI think PDFs are a little more portable and a good default option, and their formatting is best for anything you are going to print out or share via email (especially with less technically inclined folks).\nHowever, later in the semester we may see how to create some pretty cool interactive graphics and/or tables in R, and these can only be rendered in HTML. For this class, you may use either one. (But not Word, remember? Because you’ll lose reproducibility…)\n\n\nRunning R Code from a qmd file: Run Menu\nYou can also use shortcuts/buttons to run specific chunk(s). Here is one way to do it (option 1): Use the Run pulldown menu at the top of the file. (Choose the option you want based on what you are trying to do).\n\n\n\n\n\n\n\n\n\n\n\nRunning Code from a qmd file: Shortcut Button\nHere is another way to use shortcuts/buttons to run only a specific chunk (option 2): Click on the green arrow at the upper right of a code chunk to run the chunk.\n\n\n\n\n\n\n\n\n\n\n\nRunning Code from a qmd file: Copy and Paste\nFinally, here’s a third way to use shortcuts/buttons (option 3):\nCopy the code you want to run, paste to the console window, and hit Enter.\n(Or, place your cursor in the line you want to run and hit ctrl + enter (Windows) or cmd + enter (Mac).)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#downloading-files-from-rstudio",
    "href": "how-to-quarto.html#downloading-files-from-rstudio",
    "title": "3  Using Quarto",
    "section": "3.9 Downloading files from RStudio",
    "text": "3.9 Downloading files from RStudio\nYou will have to download your files if you want a copy on your own computer, or to be able to upload a copy to Moodle to turn in.\nTo download, go to the File tab, check the box for the file you want, then select More - Export. from the menu at the top of the File tab.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#quarto-files-stand-alone",
    "href": "how-to-quarto.html#quarto-files-stand-alone",
    "title": "3  Using Quarto",
    "section": "3.10 Quarto Files Stand Alone!",
    "text": "3.10 Quarto Files Stand Alone!\nWe already covered this once, but we’re covering it again because it’s one of the most common student mistakes in qmd files!\nIf you run R code in the console or the RStudio GUI (for example, reading in a data set by pasting code into the console or using the Import Dataset button in the Environment tab), you won’t be able to use the results in your markdown file.\nAny and all commands you need, including reading in data, need to be included in the file.\nThe reverse is also true. If you run just one R code chunk in a qmd file using the “run” buttons mentioned above, or by copy-pasting into the console, you are effectively running that code in the console.\nIf R gives an error saying it cannot find a certain funtion, variable, or dataset, the most likely fix is to run the preceding code chunks (especially setup!) before the one you’re stuck on.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#data-from-a-url",
    "href": "how-to-quarto.html#data-from-a-url",
    "title": "3  Using Quarto",
    "section": "3.11 Data from a URL",
    "text": "3.11 Data from a URL\nYou can load online datafiles in .csv format into R using the function read_csv(). The input to read_csv() is the full url where the file is located, in quotation marks. (Single or double quotes – it doesn’t matter which you choose, as they are equivalent in R.)\nFor example, we will consider a dataset with counts of the numbers of birds of different species seen at different locations in Hawai’i. It is stored at https://sldr.netlify.app/data/hawaii_birds.csv, and can be read into R using the command below.\n\nhi_birds &lt;- read_csv('https://sldr.netlify.app/data/hawaii_birds.csv')\n\n\nWhen you read in data, store it to a named object\nNote that we didn’t just run the read_csv() function – we assigned the results a name so that instead of printing the data table to the screen, R stores the dataset for later use.\n\nhi_birds &lt;- read_csv('https://sldr.netlify.app/data/hawaii_birds.csv')\n\nHere, we assigned the name hi_birds to the dataset using an “assignment arrow” &lt;- (the “arrow” points from the object toward the name).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#data-from-google-sheets",
    "href": "how-to-quarto.html#data-from-google-sheets",
    "title": "3  Using Quarto",
    "section": "3.12 Data from Google Sheets",
    "text": "3.12 Data from Google Sheets\nThere’s also a simple way to read in data from a Google Sheet.\nFirst, go to the Google Sheet online to prepare it by “publishing it online”.\nIn the File menu, choose “Publish to the Web”:\n\n\n\n\n\n\n\n\n\nIn the pop-up window, choose to publish your “Entire Document” as a .csv file:\n\n\n\n\n\n\n\n\n\nFinally, copy the resulting link.\n\n\n\n\n\n\n\n\n\nYou can use read_csv() with this link as input to read your data into R.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#data-from-a-file",
    "href": "how-to-quarto.html#data-from-a-file",
    "title": "3  Using Quarto",
    "section": "3.13 Data from a File",
    "text": "3.13 Data from a File\nYou can also upload your own data file to posit.cloud, or save it to your computer if you installed R/RStudio, and then read it in to R using read_csv(). The basic process is:\n\nUse spreadsheet software to create the data table\nSave the file as a csv file\nUpload the csv file if working on posit.cloud\nUse the read_csv() function to read the file into R",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "how-to-quarto.html#r-functions",
    "href": "how-to-quarto.html#r-functions",
    "title": "3  Using Quarto",
    "section": "3.14 R functions",
    "text": "3.14 R functions\nAfter reading the data in, you can use R functions to have a look at it, for example:\n\nhead(hi_birds)\nglimpse(hi_birds)\nnrow(hi_birds)\n\nTry each of the lines of code above in R. What do the functions head(), glimpse(), and nrow() do? Try to figure it out based on the output they produce.\nIf you get stuck, consult R’s built-in help files. Remember, you can access the help for a function by running the code ?functionName – for example, if you want help on head(), run:\n\n?head",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "r-results.html",
    "href": "r-results.html",
    "title": "4  R Results in Quarto Text",
    "section": "",
    "text": "4.1 Including results of R calculations in your text\nYou may want to include the results of R calculations in the TEXT part of a report. Then, if the calculated value changes, the text can be automatically updated to match.\nLet’s say you compute the mean of some kids’ foot lengths:\nmean(~length, data = KidsFeet)\n\n[1] 24.72308",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Results in Quarto Text</span>"
    ]
  },
  {
    "objectID": "r-results.html#including-results-of-r-calculations-in-your-text",
    "href": "r-results.html#including-results-of-r-calculations-in-your-text",
    "title": "4  R Results in Quarto Text",
    "section": "",
    "text": "Simple but Inefficient\nYou may want to cite the result in the text part of your file…so you would type:\nThe mean length of the kids’ feet was ` r mean(~length, data=KidsFeet) ` cm.\nTo get:\nThe mean length of the kids’ feet was 24.7230769 cm.\n\n\nSide Note: back-ticks\nThose accent marks (before the “r” and at the end of the R-code stuff) are not normal single quotes or apostrophes; they are “back-ticks” or “graves” ( ` ), just like those used to help define the start and end of R code chunks in your Quarto file. There should not actually be a space between the ` and the r.\n\n\nMore Efficient\nIt’s annoying (and sometimes not really practical) to (re)type the entire R command in the text part of your file. An option is to save the quantity you want to refer to as a variable in R:\n\nmean_length &lt;- mean(~length, data = KidsFeet)\n\nThen you can write: The mean foot length of the kids was ` r mean_length` cm.\nTo get: The mean foot length of the kids was 24.7230769 cm.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Results in Quarto Text</span>"
    ]
  },
  {
    "objectID": "r-results.html#rounding",
    "href": "r-results.html#rounding",
    "title": "4  R Results in Quarto Text",
    "section": "4.2 Rounding",
    "text": "4.2 Rounding\nWhat if you want to report numeric values with a more reasonable number of decimal places? Use : The mean foot length of the kids was ` r round(mean_length, digits = 2)` cm\nand you get: The mean foot length of the kids was 24.72 cm\nYou can also consider using the function signif() if you want to specify the number of significant digits rather than the number of decimal places.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Results in Quarto Text</span>"
    ]
  },
  {
    "objectID": "r-results.html#r-results-with-more-than-one-value-inside",
    "href": "r-results.html#r-results-with-more-than-one-value-inside",
    "title": "4  R Results in Quarto Text",
    "section": "4.3 R results with more than one value inside",
    "text": "4.3 R results with more than one value inside\nWhat if you want to cite a value from an object that contains more than one value?\n\nVectors\nFor example, what if you computed means for both boys and girls? The output would be a vector of two means, then.\nYou can use hard brackets ( [ … ] ) to refer to the first, second, etc. entries. For example:\n\ngirlboy.means &lt;- mean(~ length | sex,\n                      data = KidsFeet)\n\nYou type: The girls’ mean foot length was ` r girlboy.means[“G”] `, and the boys’ was ` r girlboy.means[“B”] `\nto get: The girls’ mean foot length was 24.3210526, and the boys’ was 25.105.\nYou can also use numeric indices – for example, ` r girlboy.means[2] ` instead of ` r girlboy.means[“G”] ` to get the girls’ value – but using names when you can is often safer because you don’t have to worry about whether things are stored in the order you think that they are!\n\n\nMatrices, Tables, data.frames, tibbles…\nIf you are referring to a data table or other object with multiple rows and columns, you can use the syntax [row.numbers, column.numbers] to extract a row, a column, or a specific value of interest. If you leave either row.numbers or column.numbers blank, all rows/columns will be included.\nFor example, consider a table showing some data from a survey of intro stat students (Ticket tells whether they have gotten a speeding ticket while driving a car, and Texted tells whether they have texted while driving a car):\n\nstudent_survey &lt;- read.csv('https://sldr.netlify.app/data/IntroStatStudents.csv', \n              na.strings = list('', 'NA'))\ntally(~Ticket | Texted, \n      data = student_survey, \n      format = 'proportion')\n\n               Texted\nTicket                  No        Yes\n  I don't drive 0.03703704 0.00000000\n  No            0.77777778 0.71900826\n  Yes           0.14814815 0.28099174\n  &lt;NA&gt;          0.03703704 0.00000000\n\n\nWhat if we want to print just the first column of data?\n(Note: Don’t count the row and column names when numbering the rows and columns.)\n\ntally(~Ticket | Texted, \n      data = student_survey,\n      format = 'proportion')[,1]\n\nI don't drive            No           Yes          &lt;NA&gt; \n   0.03703704    0.77777778    0.14814815    0.03703704 \n\n\nOr better (and clearer…)\n\ntally(~Ticket | Texted, \n      data = student_survey,\n      format = 'proportion')[, \"No\"]\n\nI don't drive            No           Yes          &lt;NA&gt; \n   0.03703704    0.77777778    0.14814815    0.03703704 \n\n\nWhat about the third row (for people who have gotten a ticket)?\n\ntally(~Ticket | Texted, \n      data = student_survey, \n      format = 'proportion')[\"Yes\",]\n\n       No       Yes \n0.1481481 0.2809917 \n\n\nWhat about the proportion of students with tickets, among those who’ve texted while driving? (Row 3, Column 2 = row “Yes” and column “Yes”)? Let’s first save the table so we don’t have to recompute…\n\ndriver_table &lt;- tally(~Ticket | Texted, \n      data = student_survey, \n      format = 'proportion')\n\nType: The proportion of students who have texted while driving who have gotten a speeding ticket is ` r driver_table[“Yes”,“Yes”] `.\nTo get: The proportion of students who have texted while driving who have gotten a speeding ticket is 0.2809917.\n(Like before, if it’s possible to use names instead of numeric indices, try to do so!)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R Results in Quarto Text</span>"
    ]
  },
  {
    "objectID": "math-notation.html",
    "href": "math-notation.html",
    "title": "5  Math Notation in Quarto",
    "section": "",
    "text": "5.1 Greek Letters, common symbols, subscripts and superscripts\nYou might be wondering…\nBasically, you enclose the name of the symbol you want with $...$\n(if you use LaTeX, this will be very familiar):\nFor other Greek letters, just spell out the name of the letter that you want (following the models above). If you want a capital Greek letter, capitalize the first letter of its name when you write it out (e.g. Sigma instead of sigma).\nNote: Avoid spaces before the final $ or after the initial $.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Math Notation in Quarto</span>"
    ]
  },
  {
    "objectID": "math-notation.html#greek-letters-common-symbols-subscripts-and-superscripts",
    "href": "math-notation.html#greek-letters-common-symbols-subscripts-and-superscripts",
    "title": "5  Math Notation in Quarto",
    "section": "",
    "text": "How can I include Greek letters and other symbols in the text part of my Quarto (or RMarkdown) document?\n\n\n\n\n\n\n\nType this in qmd:\nTo get this when rendered:\n\n\n\n\n$\\hat{p}$\n\\(\\hat{p}\\)\n\n\n$\\bar{x}$\n\\(\\bar{x}\\)\n\n\n$\\alpha$\n\\(\\alpha\\)\n\n\n$\\beta$\n\\(\\beta\\)\n\n\n$\\gamma$\n\\(\\gamma\\)\n\n\n$\\Gamma$\n\\(\\Gamma\\)\n\n\n$\\mu$\n\\(\\mu\\)\n\n\n$\\sigma$\n\\(\\sigma\\)\n\n\n$\\sigma^2$\n\\(\\sigma^2\\)\n\n\n$\\rho$\n\\(\\rho\\)\n\n\n$\\epsilon$\n\\(\\epsilon\\)\n\n\n$\\sim$\n\\(\\sim\\)\n\n\n$\\mu_D$\n\\(\\mu_D\\)\n\n\n$\\mu_{longsubscript}$\n\\(\\mu_{longsubscript}\\)\n\n\n$\\hat{p}_{longsubscript}$\n\\(\\hat{p}_{longsubscript}\\)\n\n\n$\\mu\\neq 0$\n\\(\\mu \\neq 0\\)\n\n\n$\\mu\\geq 5$\n\\(\\mu \\geq 5\\)\n\n\n$\\mu\\leq 1$\n\\(\\mu \\leq 1\\)\n\n\n$\\cup$\n\\(\\cup\\)\n\n\n$\\cap$\n\\(\\cap\\)\n\n\n$\\vert$\n\\(\\vert\\)\n\n\n$\\sim$\n\\(\\sim\\)\n\n\n$\\frac{numerator}{denominator}$\n\\(\\frac{numerator}{denominator}\\)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Math Notation in Quarto</span>"
    ]
  },
  {
    "objectID": "math-notation.html#summations-and-products",
    "href": "math-notation.html#summations-and-products",
    "title": "5  Math Notation in Quarto",
    "section": "5.2 Summations and Products",
    "text": "5.2 Summations and Products\n\n\n\nType This:\nTo get this in your PDF:\n\n\n\n\n$\\sum_{i=1}^{n} x_i$\n\\(\\sum_{i=1}^{n} x_i\\)\n\n\n$\\prod_{i=1}^{n} f(i)}$\n\\(\\prod_{i=1}^{n} f(i)\\)\n\n\n\nThese will format as seen above if used in inline math mode (enclosed in single $s). If you put them in display math mode by using two $$ at the start and end instead of just one, then the result will be displayed centered on its own line and the limits of the summation/product will be above/below the \\(\\Sigma\\) or \\(\\Pi\\):\n\n\n\\[\\prod_{i=1}^{n} f(i)\\]",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Math Notation in Quarto</span>"
    ]
  },
  {
    "objectID": "math-notation.html#long-equations",
    "href": "math-notation.html#long-equations",
    "title": "5  Math Notation in Quarto",
    "section": "5.3 Long equations",
    "text": "5.3 Long equations\nYou can use double $ to bracket equations you want to display on a line of their own. Inside can be multiple mathematical expressions. For example:\n$$\\hat{y} = \\beta_0 + \\beta_1x_1\\,$$\n\n$$\\hat{epsilon} = \\hat{y}_{i} - y_i$$\n\n$$\\epsilon \\sim N(0, \\sigma)$$\ngives\n\\[\\hat{y} = \\beta_0 + \\beta_1x_1\\]\n\\[\\hat{\\epsilon}_{i} = \\hat{y}_{i} - y_i\\]\n\\[\\epsilon \\sim N(0, \\sigma)\\]\nNote: Avoid spaces before the final $ or after the initial $. Also note, the equation will NOT be inside a code chunk…I only did that here because it’s hard to get the un-rendered source version to appear neatly in the text part of a rendered Quarto file otherwise.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Math Notation in Quarto</span>"
    ]
  },
  {
    "objectID": "graphics-design.html",
    "href": "graphics-design.html",
    "title": "Designing Effective Visualizations",
    "section": "",
    "text": "Section Learning Outcomes\nAfter this section, you will be able to:",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#section-learning-outcomes",
    "href": "graphics-design.html#section-learning-outcomes",
    "title": "Designing Effective Visualizations",
    "section": "",
    "text": "Critique statistical graphics based on design principles.\nRecognize common misleading design choices for data visualizations\nRecognize data visualization that tells a true story, identifying elements that emphasize the main finding and make the figure easy to interpret at a glance",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#reference-materials",
    "href": "graphics-design.html#reference-materials",
    "title": "Designing Effective Visualizations",
    "section": "Reference Materials",
    "text": "Reference Materials\n\nBeyond Multiple Linear Regression Ch. 1.5\nEcological Models & Data in R Ch. 2 discusses graphics, but is not recommended as the approach to reading in data, writing R code, and generating graphs in R is very different to that used in this course.\nA comprehensive, and free, supplemental reference is Fundamentals of Data Visualization by Claus Wilke\n\nIt’s suggested that you refer to the above materials as needed after doing this section, with particular focus on the topics you found most challenging.",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#inspiration",
    "href": "graphics-design.html#inspiration",
    "title": "Designing Effective Visualizations",
    "section": "Inspiration",
    "text": "Inspiration\n\nAbove all, show the data.\nE. Tufte, The Visual Display of Quantitative Information\n\nBut…\n\nThe Numbers Don’t Speak for Themselves.\nC. D’Ignazio and L. Klein, Data Feminism\n\nIn visualizing data, we use graphics to gain and communicate an honest understanding of data in context.",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#motivation-imagine-first",
    "href": "graphics-design.html#motivation-imagine-first",
    "title": "Designing Effective Visualizations",
    "section": "Motivation: Imagine First!",
    "text": "Motivation: Imagine First!\nFigures are a crucial tool for exploring your data and communicating what you learn from the data.\nWhether you are doing a quick check to assess basic features of a dataset or creating a key figure for an important presentation, the best practice is to work thoughtfully.\n\nThe I.C.E.E. method:\n\nImagine how you want your graph to look, before you\nCode. Once you have the basic starting point,\nEvaluate your work, and\nElaborate (refine it).\n\nRepeat until the figure is as awesome as it needs to be.\n\n\nNO To Mindless Copy/Paste\nToo many of us fall into the trap of starting to write code (or copy/pasting it!) before pausing to think carefully about the desired outcome, then settling for the first vaguely relevant result (or delighting in the unintended outcome…).\n\n\nYou can do better than mindless copying! Only mindful copy-pasting allowed.\n\n\nThis section provides some advice to get you started. It can also provide inspiration for constructive critique of others’ graphics.\nHere we focus only on the I_EE parts of the process, where you design and assess graphics. Code will come later.",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#appearance-goals",
    "href": "graphics-design.html#appearance-goals",
    "title": "Designing Effective Visualizations",
    "section": "Appearance Goals",
    "text": "Appearance Goals\nSpecifically, how exactly should a graphic look? There are so many choices: color, size, text and more. What are best practices for creating something beautiful, that represents the data honestly, and is easy to understand?\nThis section will provide some rules of thumb to help you Evaluate statistical graphics. It will also teach you to spot common problems and suggest ways to fix them, allowing you to provide constructive critique (to yourself or to others!) about how to Elaborate and refine data visualizations.\n\nYou still have your freedom!\nAs you digest all these rules and tips, you may wonder: “Do I have to always obey every one?” Well…No, of course not. Be creative!\nSometimes it’s OK to break these rules when you have thought it through and with a good justification.\nA good justification means that in your particular case, breaking a certain rule will make your graph more informative, easier to understand, or better at telling the story you’re highlighting.",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graphics-design.html#learning-objectives",
    "href": "graphics-design.html#learning-objectives",
    "title": "Designing Effective Visualizations",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis section will give you some basic tools to:\n\nGraph data with integrity, avoiding misleading design choices\nTell the right story, including elements that emphasize your main finding and make your figure easy to interpret at a glance",
    "crumbs": [
      "Designing Effective Visualizations"
    ]
  },
  {
    "objectID": "graph-design-no-junk.html",
    "href": "graph-design-no-junk.html",
    "title": "6  What is Necessary?",
    "section": "",
    "text": "6.1 Bye, Junk!\nOur first principle is: if it doesn’t need to be in your graph, it shouldn’t be there. Keep things as simple as possible. What are some justifications for a need to include an element in a plot?\nIf you need it, include it, but if not, keep it simple!\nImagine you are using very expensive ink to print every element of the graph. Is every drop of in you’re using really worth it? If not, take it out. As influential data visualization thinker Edward Tufte put it in The Visual Display of Quantitative Information,\nIn other words, don’t let annotations, labels, grids, etc. overwhelm the visual impact of your data – Don’t do this:\nCartoon source: https://freshspectrum.com/data-ink-ratio/",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>What is *Necessary*?</span>"
    ]
  },
  {
    "objectID": "graph-design-no-junk.html#bye-junk",
    "href": "graph-design-no-junk.html#bye-junk",
    "title": "6  What is Necessary?",
    "section": "",
    "text": "It is crucial to the story you are telling, or the research question you are answering.\nIt emphasizes your main point. For example, some plots may not need color, and in others it may add crucial visual contrast to highlight a main point.\nIt makes the graph easier to read and understand\nIt makes the main message of the graph more memorable\n\n\n\n\nA large share of ink on a graphic should present data-information, the ink changing as the data change. Data-ink is the non-erasable core of a graphic…\n\n\n\n\n\nCheck: Critique the Pie\nThe figure below, from a Forbes article on mobile operating system crashes, is pretty awful.\n\n\nA Conclusion?\nWhat is one main conclusion from the graph above?\n(It’s pretty confusing to interpret, so you may have to study carefully to find something…)\n\n\nRemove…What?\nNow that you have identified one main conclusion from the graph…\nWhat is one element of the plot that:\n\nobscures that conclusion,\nis NOT necessary, and\ncould be removed to improve the plot?\n\nAnswer constructively - as if the person who made the plot was incredibly smart and someone you admire, and to whom you wanted to be kind but helpful.",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>What is *Necessary*?</span>"
    ]
  },
  {
    "objectID": "graph-design-no-junk.html#grids-and-boxes",
    "href": "graph-design-no-junk.html#grids-and-boxes",
    "title": "6  What is Necessary?",
    "section": "6.2 Grids and Boxes",
    "text": "6.2 Grids and Boxes\nShould your graphics include boxes, axis lines, and grid lines?\nWell, it depends…\n\nRemoving unnecessary axes, grids, and labels yields a cleaner plot that may be easier to take in at a glance – there is less to distract from the main story\nBut… omitting needed baselines, tick marks, gridlines, and labels can cause confusion and make it hard to identify categories or estimate numeric values\nScientific graphics usually need axis lines, with tick marks\nIf a viewer will need to refer to an axis to estimate heights of bars or locations of points, then consider using gridlines for that axis.\nInstead of an entire grid, it may be more effective to include single lines indicating important threshold values\nConsider using a color that nearly blends into the background for grid lines, so that they detract as little attention as possible from the data\n\n\nExample\nStorytelling with Data provide an example of a cluttered figure where the trend over time pops out more as unnecessary grids and boxes are made less visible, then removed:\n\nOptionally, if you’d like more examples, read S. Few’s 3.5-page article on when grid lines are helpful.",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>What is *Necessary*?</span>"
    ]
  },
  {
    "objectID": "graph-design-color.html",
    "href": "graph-design-color.html",
    "title": "7  Color",
    "section": "",
    "text": "7.1 Using Color\nColor - used with care - can be an incredibly effective part of a data visualization.\nThe video below, created by Storytelling with Data, gives explanations and examples.",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Color</span>"
    ]
  },
  {
    "objectID": "graph-design-color.html#using-color",
    "href": "graph-design-color.html#using-color",
    "title": "7  Color",
    "section": "",
    "text": "Ensure your color choices highlight the story you want to tell\n\nConsider using black and grey to help some elements fade into the background - for example, grid lines and labels that must be present but aren’t the most important elements.\nOr, color all groups but the one you want to highlight in grey, and use a bold color for the “main” group…\n\nChoose color combinations that look good and are distinguishable by color-blind viewers and in greyscale\n\nDefaulting to pre-defined color palettes provided by your software may be better than haphazardly choosing colors manually\nConsider being redundant - use size and/or shape as well as color to indicate groups so figures are legible in greyscale, too.\n\nUse color consistently.\nExample: if “young” cases are red in one graph, don’t use red for “old” in the next graph. And if many graphs are colored by the same grouping variable, use the same colors in all of them.\n\n\n\nIf you have time, watch from 11:48 to 28:41 (about 17 minutes). This segment will play automatically in the clip below.\nIf you’re in a rush, the most important sections (about 10 minutes) are:\n\n13:57 - 15:12 (Sparing use of color)\n18:44 - 21:25; see also the infographic of color in culture\n22:25 - 23:10 (Color blindness - to view your graphs as someone with color blindness would, take a screen shot and try the simulator online\n23:50 - 28:41 (Consistency)\n\n\n\n\n\nWhich of the following are lessons from the Storytelling with Data video on Being Clever with Color? Mark all correct answers “TRUE”.\nColor grabs attention. TRUEFALSE\nColor signals where to look. TRUEFALSE\nColor should be used sparingly. TRUEFALSE\nToo much color, and everything is highlighted - the viewer does not know what to pay attention to. TRUEFALSE\nColor can show quantitative values, too, not just categories. TRUEFALSE\nColors have tone and meaning. TRUEFALSE\nNot everyone can see colors. TRUEFALSE\nUse color consistently. TRUEFALSE\nSimple black and white is always the best choice. TRUEFALSE\n\n\n\nClick for explanations of solutions above.\n\n\nThe human eye is naturally drawn to colors.\nSince color grabs attention, we expect it to direct us toward the most important stuff that is worthy of our attention.\nBut…Too much color, and everything is highlighted - the viewer does not know what to pay attention to.\nAlso, remember that the meaning and interpretation of colors varies by culture.\nSince some people can not see color, use color-blind friendly palettes and redundant coding (shape, text) where possible without cluttering the figure.\nInconsistent use of color can be confusing and distracting.\nSometimes black and white is great - but often color helps you tell a story!",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Color</span>"
    ]
  },
  {
    "objectID": "graph-design-text-elements.html",
    "href": "graph-design-text-elements.html",
    "title": "8  Text Elements",
    "section": "",
    "text": "8.1 Titles, Labels, Size\nWhen using text in a figure, ensure it is easy to read. Make sure no unneccessary text is included.\nRemember the melanoma rates over time figure we saw earlier?",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Elements</span>"
    ]
  },
  {
    "objectID": "graph-design-text-elements.html#titles-labels-size",
    "href": "graph-design-text-elements.html#titles-labels-size",
    "title": "8  Text Elements",
    "section": "",
    "text": "Default size of text in figures produced by statistical software is almost always too small. Make sure your text is big enough to be easily legible in the context where you will present it (on the page in a report, on a slide for a presentation, etc.)\nOther than the title of the vertical (y) axis, all the text in a plot should be horizontal. This makes it easier to read.\nAxis labels should be self-explanatory\n\nViewers should be able to guess what they mean accurately without looking at anything but the figure\nUse words instead of numeric codes or cryptic abbreviations\n\nAxis labels should also be as short as possible while remaining easy to understand\nEvery plot should have a title. Sometimes this might be a literal title at the top of the graph, but those are relatively rare. More often in scientific work, a text caption appears below the figure. The first phrase/sentence of the caption acts as the figure’s title\n\n\n\n\nWhat helpful changes did Storytelling with Data make to the text labels as they improved the figure?\nThe x axis labels are rotated so they are horizonal. TRUEFALSE\nThe title color is changed to blue and the axis labels to grey. TRUEFALSE\nThe box around the plot is removed. TRUEFALSE\n\n\n\nClick for explanations of solutions above.\n\n\nRotating axis labels so they are horizontal is generally an improvement. To make this happen, the number of tick marks and labels on the x axis was also reduced. Notice the labels are much easier to read.\nThe color changes helped too. The blue links the title with the trend it describes, and the grey makes the axis titles less prominent and lets the viewer focus on the data. Continue to the next section for more on using color…\nThe box is gone, and it is a big improvement to the plot! But technically, you were asked about changes to the text labels…",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Elements</span>"
    ]
  },
  {
    "objectID": "graph-design-text-elements.html#when-things-overlap",
    "href": "graph-design-text-elements.html#when-things-overlap",
    "title": "8  Text Elements",
    "section": "8.2 When Things Overlap",
    "text": "8.2 When Things Overlap\nEspecially when graphing variables with long category values, you may end up with ugly, illegible overlapping labels.\nSome solutions, in rough order of preference, are to:\n\nadjust the figure width or height so everything fits\nswitch x and y coordinates so the “long” labels are on the y axis (in R, this resizes the plot area so that labels fit); or,\nrotate the too-long labels, which eliminates the overlap but makes them harder to read than horizontal text.\nmake the font smaller (but this might make it annoyingly hard to read, or make your viz feel cluttered!)",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Elements</span>"
    ]
  },
  {
    "objectID": "graph-design-text-elements.html#when-text-runs-off-the-edge",
    "href": "graph-design-text-elements.html#when-text-runs-off-the-edge",
    "title": "8  Text Elements",
    "section": "8.3 When Text Runs “off the edge”",
    "text": "8.3 When Text Runs “off the edge”\nSometimes a title or axis label is too long and runs off the edge of the figure. Using a smaller font is not often an ideal solution. If you can’t just use a shorter label, consider adding line breaks.",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Elements</span>"
    ]
  },
  {
    "objectID": "graph-design-text-elements.html#examples",
    "href": "graph-design-text-elements.html#examples",
    "title": "8  Text Elements",
    "section": "8.4 Examples",
    "text": "8.4 Examples",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Elements</span>"
    ]
  },
  {
    "objectID": "graph-design-text-elements.html#lets-talk-about-titles",
    "href": "graph-design-text-elements.html#lets-talk-about-titles",
    "title": "8  Text Elements",
    "section": "8.5 Let’s Talk About Titles",
    "text": "8.5 Let’s Talk About Titles\nShould your graph have a title?\nWell…maybe.\nHear me out.\nWhether or not titles are useful, allowed, or expected is pretty discipline- and audience-specific. In journalism and some parts of business, they are used very often. In the peer-reviewed scientific literature, they are exceedingly rare. In a slide deck, they are often useful…unless they say the same thing as your slide title! Know your goals and your audience.\n\nAvoid Redundant Titles\nIf you do want to use a title, make sure it provides information or details that are not already present in other text elements. A title that restates the axis labels is usually a waste:\n\n\n\n\n\n\n\n\n\nNotice that we can argue about this a little. Could we have the title instead of the axis labels? Maybe, but if it’s a scientific publication, having the units of measure is going to be important.\nIs this title actually adding info because it clarifies that “Town,” “City,” etc. are “Locations”? I think it’s a stretch, but sometimes you can make such a case.\nGenerally, my advice is…\n\n\nIf the title is not adding something new and crucial and helping the reader decipher the main story of the plot, then omit it!\n\n\nSo what might an “informative” title look like?",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text Elements</span>"
    ]
  },
  {
    "objectID": "graph-design-summary.html",
    "href": "graph-design-summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "9.1 Critique Practice\nAfter reviewing the preceding sections, you should be able to articulate some principles for designing good visualizations with respect to use of space, axis limits, use of color, and text elements.\nIn fact, you probably already knew these principles - at least, you knew ’em when you saw ’em (you could have easily sorted some terrible and better graphs even if you couldn’t have said exactly what was terrible, or better, about them).\nNow, think about how you could apply these principles in your own work, or in providing feedback or advice to others…\nTry using what you have learned to provide a constructive critique of an example. That might mean pointing out specific successes or positives as well as areas for improvement, with concrete advice about how to improve and why.\nConsider the graphic below. At a glance, what do you think it means? Looking more carefully, what do you notice?\nPause to think: What changes, if any, would you suggest to the figure’s creator to make it clearer and easier to understand? Be sure to be constructive - gently explain any problems and suggest solutions.",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "graph-design-summary.html#video-review",
    "href": "graph-design-summary.html#video-review",
    "title": "9  Summary",
    "section": "9.2 Video Review",
    "text": "9.2 Video Review\nWow, that was quite a lot of information! If you could use a brief review from a different point of view, check out the optional video from Kristen Sosulski",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "graph-design-summary.html#recap-reflect-12-tips",
    "href": "graph-design-summary.html#recap-reflect-12-tips",
    "title": "9  Summary",
    "section": "9.3 Recap & Reflect: 12 Tips",
    "text": "9.3 Recap & Reflect: 12 Tips\nThe 4-minute video below summarizes design principles for data visualization in the form of 12 tips.\nAs you watch, make note of one or two tips that strike you (you’ll report your thoughts in the next section). Is there one that nicely summarizes an idea introduced earlier in the section? One you’re not sure about? One that you think is incredibly important? One that makes you say “Aha! Now I see why I loved/hated that visualization!”\n\n\n\nPause for Reflection\nTake a moment to reflect on what you learned. Which Tip do you remember most clearly, think is most important, or want to challenge? Consider making a few notes for yourself for the future (you’ll have to make and critique plenty of graphics in your homework assignments).",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "graph-design-summary.html#a-critique-checklist",
    "href": "graph-design-summary.html#a-critique-checklist",
    "title": "9  Summary",
    "section": "9.4 A Critique Checklist",
    "text": "9.4 A Critique Checklist\nIf working to improve your own visualization or attempting to give feedback on one another analyst made, you might consider using a checklist to guide you.\nBased heavily on the advice and template provided by Evergreen Data…\n\n\nCheck out our Graphics Critique Checklist!",
    "crumbs": [
      "Designing Effective Visualizations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "graphics-choose.html",
    "href": "graphics-choose.html",
    "title": "Choose a Graph Type",
    "section": "",
    "text": "Section Learning Outcomes\nAfter this tutorial, you will:\nNote: You do NOT have to memorize all the information in this tutorial. Review it now, but know you will probably return to this tutorial for later reference. Your goal should be to finish with a basic idea of which graph types should be used for which variable types. Notice that the “Gallery” sections in the navigation bar are labeled by which variable types are to be shown!\nAt the end, you might want to finish with your own notes filling in a table like the one below:\nVariables\nGraphs\n\n\n\n\nOne Quantitative\nhistogram, density plot, …\n\n\nOne Categorical\n…\n\n\n…\n…",
    "crumbs": [
      "Choose a Graph Type"
    ]
  },
  {
    "objectID": "graphics-choose.html#section-learning-outcomes",
    "href": "graphics-choose.html#section-learning-outcomes",
    "title": "Choose a Graph Type",
    "section": "",
    "text": "Distinguish variable types: quantitative, categorical (nominal, ordinal, interval, ratio); explanatory, response, covariate.\nChoose an appropriate graphical display for a specified combination of variables.\n(Continue to) critique statistical graphics based on design principles.",
    "crumbs": [
      "Choose a Graph Type"
    ]
  },
  {
    "objectID": "graphics-choose.html#text-reference",
    "href": "graphics-choose.html#text-reference",
    "title": "Choose a Graph Type",
    "section": "Text Reference",
    "text": "Text Reference\n\nBeyond Multiple Linear Regression Ch. 1.5\nEcological Models & Data in R Ch. 2 discusses graphics, but is not recommended as the approach to reading in data, writing R code, and generating graphs in R is very different to that used in this course.\nA comprehensive, and free, supplemental reference is Fundamentals of Data Visualization by Claus Wilke",
    "crumbs": [
      "Choose a Graph Type"
    ]
  },
  {
    "objectID": "graphics-choose.html#motivation-imagine-first",
    "href": "graphics-choose.html#motivation-imagine-first",
    "title": "Choose a Graph Type",
    "section": "Motivation: Imagine First!",
    "text": "Motivation: Imagine First!\nFigures are a crucial tool for exploring your data and communicating what you learn from the data.\nWhether you are doing a quick check to assess basic features of a dataset or creating a key figure for an important presentation, the best practice is to work thoughtfully. You already learned about creating graphics by I.C.E.E:\n\nThe I.C.E.E. method:\n\nImagine how you want your graph to look, before you\nCode. Once you have the basic starting point,\nEvaluate your work, and\nElaborate (refine it).\n\nRepeat until the figure is as awesome as it needs to be.\n\n\nLimiting Your Imagination\nThere is really no limit to the creative data visualizations you might dream up.\nBut there is a set of basic, workhorse graphics that statisticians and data scientists use most frequently. What are the common options and how do you choose among them?\nThe best choice depends on what kind of data you have, and also on what you want to do with it: what question are your trying to answer? What story will you tell?",
    "crumbs": [
      "Choose a Graph Type"
    ]
  },
  {
    "objectID": "graphics-choose.html#goals",
    "href": "graphics-choose.html#goals",
    "title": "Choose a Graph Type",
    "section": "Goals",
    "text": "Goals\nSpecifically, you will now focus on choosing the right type of visualization for the task at hand.\nNote that the graphs shown in this tutorial are over-simplified versions - icons, really - with missing labels, huge titles, and huge data elements. This is intentional, to evoke the look of each plot type rather than to present actual data.",
    "crumbs": [
      "Choose a Graph Type"
    ]
  },
  {
    "objectID": "graphics-choose-var-types.html",
    "href": "graphics-choose-var-types.html",
    "title": "10  Variable Types",
    "section": "",
    "text": "10.1 Distributions\nBefore designing a graphic, you need some data. Ideally, it will be in a tidy table, with one row per case and one column per variable.\nDifferent plots may be appropriate, depending on whether the variable is:\nThe video below gives a concise explanation of the different variable types you need to be able to recognize.\nSometimes, you need a plot that lets you see the distribution of a single variable:\nSometimes these graphs present the answer to a scientific question of interest, but often they are used during exploration or model assessment to better understand a dataset and:",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Variable Types</span>"
    ]
  },
  {
    "objectID": "graphics-choose-var-types.html#distributions",
    "href": "graphics-choose-var-types.html#distributions",
    "title": "10  Variable Types",
    "section": "",
    "text": "What values does it take on?\nHow often does each value occur?\n\n\n\nCheck the data\n\nAre there lots of missing values?\nAre missing values encoded as 999 or -1000 or some other impossible value, instead of being marked as missing via NA?\n\nVerify whether the variable’s distribution matches expectations (for example, symmetry, etc.)",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Variable Types</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-cat.html",
    "href": "graphics-choose-one-cat.html",
    "title": "11  Gallery: One Categorical Variable",
    "section": "",
    "text": "11.1 Consider your Audience\nTo show one categorical variable, we will mainly use bar charts. You might also consider lollipop/Cleveland dot plots, or pie graphs.",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gallery: One Categorical Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-cat.html#bar-graph",
    "href": "graphics-choose-one-cat.html#bar-graph",
    "title": "11  Gallery: One Categorical Variable",
    "section": "11.2 Bar Graph",
    "text": "11.2 Bar Graph\n\n\n\n\n\n\n\n\n\n\nCan show either counts, proportions, or percentages\nEasy to see which categories have more/fewer observations",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gallery: One Categorical Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-cat.html#cleveland-dot-lollipop-plot",
    "href": "graphics-choose-one-cat.html#cleveland-dot-lollipop-plot",
    "title": "11  Gallery: One Categorical Variable",
    "section": "11.3 Cleveland Dot / Lollipop Plot",
    "text": "11.3 Cleveland Dot / Lollipop Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMain difference is whether the “sticks” are drawn (Lollipop) or not (Cleveland Dot)\nMuch like a bar chart, but using dots or lollipops to mark the count or proportion in each category\nWork well when there are many categories to be ranked by frequency",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gallery: One Categorical Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-cat.html#pie-chart",
    "href": "graphics-choose-one-cat.html#pie-chart",
    "title": "11  Gallery: One Categorical Variable",
    "section": "11.4 Pie Chart",
    "text": "11.4 Pie Chart\n\n\n\n\n\n\n\n\n\n\nDisplay proportions, not counts\nUnpopular with many statisticians and data scientists because…\n\nHard to see which categories have more/fewer observation when proportions similar\nTemptation to clutter them up with annotation (for example, percentage for each slice)\nCan be inefficient use of space on rectangular page\n\nOften easier to interpret when there are few categories",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gallery: One Categorical Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-quant.html",
    "href": "graphics-choose-one-quant.html",
    "title": "12  Gallery: One Quantitative Variable",
    "section": "",
    "text": "12.1 Dotplot\nWhat are some ways to display the distribution of one quantitative variable?",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gallery: One Quantitative Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-quant.html#dotplot",
    "href": "graphics-choose-one-quant.html#dotplot",
    "title": "12  Gallery: One Quantitative Variable",
    "section": "",
    "text": "Intuitive representation: x-axis shows range of variable values, and dots are data points\nAs the idea is to show one dot per observation, may not work well for huge datasets",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gallery: One Quantitative Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-quant.html#histogram",
    "href": "graphics-choose-one-quant.html#histogram",
    "title": "12  Gallery: One Quantitative Variable",
    "section": "12.2 Histogram",
    "text": "12.2 Histogram\n\n\n\n\n\n\n\n\n\n\nRange of variable values is divided into bins, then height of each bar corresponds to the number of observations in the bin\nEffective way to examine the shape of a distribution\nChoosing the number of bins to use is tricky: too many, and the shape is jagged; too few over-smooths (peaks blend together). Not sure? Find a number of bins that is definitely too few, and one that is definitely too many, and then try to settle on an in-between value that best shows the real shape of the distribution without over-smoothing.",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gallery: One Quantitative Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-quant.html#density-plot",
    "href": "graphics-choose-one-quant.html#density-plot",
    "title": "12  Gallery: One Quantitative Variable",
    "section": "12.3 Density Plot",
    "text": "12.3 Density Plot\n\n\n\n\n\n\n\n\n\n\nLike a smoothed version of a histogram (obtained by kernel density estimation, if you want to look up mathematical details)\nCaution: for small datasets, the density plot may show “peaks” that really correspond to one or a few observations\nCan only show density (relative frequency of observation), not counts",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gallery: One Quantitative Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-quant.html#qq-plot",
    "href": "graphics-choose-one-quant.html#qq-plot",
    "title": "12  Gallery: One Quantitative Variable",
    "section": "12.4 QQ Plot",
    "text": "12.4 QQ Plot\n\n\n\n\n\n\n\n\n\n\n“Q-Q Plot” is short for “Quantile-Quantile Plot”\nIn some cases, we want to examine the shape of a variable’s distribution to see if it matches a theoretical expectation. For example: do the regression residuals match a normal distribution? (If that example doesn’t make sense to you now - it will later in the course, don’t worry.)\nQuantile-quantile plots are one way to make this comparison. They plot the quantiles of the data as a function of the same quantiles of the expected theoretical distribution; if there’s a good match, the points should follow a line with slope = 1.\nHow close to the straight line is “close enough”? That’s the tricky part…",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gallery: One Quantitative Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-one-quant.html#check-your-understanding-one-variable-plots",
    "href": "graphics-choose-one-quant.html#check-your-understanding-one-variable-plots",
    "title": "12  Gallery: One Quantitative Variable",
    "section": "12.5 Check Your Understanding: One-variable plots",
    "text": "12.5 Check Your Understanding: One-variable plots\n\n\nWhich plot would work best to show the distribution of 75 families’ household incomes?\n\n Lollipop plot Histogram Bar chart\n\nWhich plot would work best to show the distribution of 75 families’ postal codes?\n\n Bar chart Density plot Histogram Scatter plot\n\n\n\n\n\nClick for explanations of solutions above.\n\nLollipop plots and bar graphs work better for categorical variables – they show counts or proportions (or some other summary of counts in categories). By default, there would be one lollipop or bar for each unique value of income - what a mess! Histograms and density plots, on the other hand, show the distribution of one quantitative variable. (Scatter plots are usually used to show 2 quantitative variables.)",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Gallery: One Quantitative Variable</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-cat.html",
    "href": "graphics-choose-multiple-cat.html",
    "title": "13  Gallery: 2-3 Categorical Variables",
    "section": "",
    "text": "13.1 Stacked Bar Graph",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gallery: 2-3 Categorical Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-cat.html#stacked-bar-graph",
    "href": "graphics-choose-multiple-cat.html#stacked-bar-graph",
    "title": "13  Gallery: 2-3 Categorical Variables",
    "section": "",
    "text": "Similar to side-by-side bar\nCompared to side-by-side, it’s harder to compare proportions in each group within a category, but easier to estimate the proportion in each category.",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gallery: 2-3 Categorical Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-cat.html#faceted-bar-graph",
    "href": "graphics-choose-multiple-cat.html#faceted-bar-graph",
    "title": "13  Gallery: 2-3 Categorical Variables",
    "section": "13.2 Faceted Bar Graph",
    "text": "13.2 Faceted Bar Graph\n\n\n\n\n\n\n\n\n\n\nOne plot box – usually called a “panel” or “facet” – for each of a set of groups\nThink carefully about the question of interest and the relationship you want to highlight as you choose: should bar heights correspond to…\n\nNumber of observations?\nProportion of observations overall in the whole dataset?\nProportion of observations in the panel?\nSomething else?",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gallery: 2-3 Categorical Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-cat.html#combinations-stacked-bars-facets-etc.",
    "href": "graphics-choose-multiple-cat.html#combinations-stacked-bars-facets-etc.",
    "title": "13  Gallery: 2-3 Categorical Variables",
    "section": "13.3 Combinations (Stacked bars + Facets, etc.)",
    "text": "13.3 Combinations (Stacked bars + Facets, etc.)\nOf course, if you have 3 variables instead of just two, you can combine methods. Avoid it unless you are sure it is necessary and communicates clearly.\n\nBe sure that the resulting graph is not too complex to understand quickly, at a glance. Packing too much information into one graph sometimes means none of the info is actually communicated!\nAnd if showing proportions or percentages in such a display, be sure you understand what denominator is being used in the calculations – is it the fraction of the whole dataset, within facets, etc.?",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gallery: 2-3 Categorical Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-quant.html",
    "href": "graphics-choose-multiple-quant.html",
    "title": "14  Gallery: Multiple Quantitative Variables",
    "section": "",
    "text": "14.1 Scatter Plot",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Gallery: Multiple Quantitative Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-quant.html#scatter-plot",
    "href": "graphics-choose-multiple-quant.html#scatter-plot",
    "title": "14  Gallery: Multiple Quantitative Variables",
    "section": "",
    "text": "A scatterplot is the default for visualizing the relationship between two quantitative variables\nBe sure you actually have two quantitative variables! If not, another plot may be a better option.\n\n\n\nLet’s just reiterate: if one of your variables is actually or effectively categorical, a basic scatterplot is usually not ideal!",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Gallery: Multiple Quantitative Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-quant.html#line-plot",
    "href": "graphics-choose-multiple-quant.html#line-plot",
    "title": "14  Gallery: Multiple Quantitative Variables",
    "section": "14.2 Line Plot",
    "text": "14.2 Line Plot\n\n\n\n\n\n\n\n\n\n\nIf the x-axis variable is Time (or it otherwise makes sense to join the dots), a line can replace the dots, or be added to them\nMake sure connecting the dots makes sense in context and does not guide the eye to incorrect interpretations (for example, emphasizing outliers)",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Gallery: Multiple Quantitative Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-multiple-quant.html#quantitative-variables",
    "href": "graphics-choose-multiple-quant.html#quantitative-variables",
    "title": "14  Gallery: Multiple Quantitative Variables",
    "section": "14.3 >2 Quantitative Variables",
    "text": "14.3 &gt;2 Quantitative Variables\nWhat if you have three or four quantitative variables whose relationships you’re curious about?\nProceed with caution!\nIt’s possible to include 3+ variables on one plot, but ideally it should still be interpretable at a glance:\n\nWhat is the main point of the figure? Is it possible to make the point without showing all 3+ variables together?\nKeep things as simple as you can while still telling the story you want to tell.\n\n\nScatter + Size\n\n\n\n\n\n\n\n\n\n\nYou can adjust the size of each dot in a scatter plot to correspond to the value of a third variable\nThis is especially useful when the third variable measures the size of the population being represented – for example, a scatter plot of life expectancy vs income for many countries, with point size indicating population of each country\n\n\n\nScatter + Color\nYou can also color by a third quantitative variable:\n\n\n\n\n\n\n\n\n\nThis usually only works well visually if all three variables are clearly associate with each other, so that certain colors are clearly dominant in certain regions of the graph. Otherwise, you get a mishmash of colors all over, which can be distracting.\n\n\nAnimation\nIt may be possible to show a third quantitative variable via animation (this often works especially well if that third variable is actually time!)",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Gallery: Multiple Quantitative Variables</span>"
    ]
  },
  {
    "objectID": "graphics-choose-cat-quant-mix.html",
    "href": "graphics-choose-cat-quant-mix.html",
    "title": "15  Gallery: Mixed Quantitative + Categorical",
    "section": "",
    "text": "15.1 Distribution by Groups\nThere are several plots designed specifically to look at the distribution of a quantitative variable, grouped by a categorical variable.",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Gallery: Mixed Quantitative + Categorical</span>"
    ]
  },
  {
    "objectID": "graphics-choose-cat-quant-mix.html#distribution-by-groups",
    "href": "graphics-choose-cat-quant-mix.html#distribution-by-groups",
    "title": "15  Gallery: Mixed Quantitative + Categorical",
    "section": "",
    "text": "Boxplots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot shows a summary of the distribution. The box spans the middle half of the data, with the line marking the median. The “whisker” lines extend to cover the range of “most of” the data, with outliers shown individually\nFor details, check out this optional explanation of how boxplots are constructed from Introduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin.\nIf your dataset is too small to estimate the median and quartiles of the data accurately, consider showing all the observations (for example, using or overlaying a jitter plot)\n\n\n\nViolin Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese show a mirrored density plot for each group\nAs for density plots, make sure you have a large enough dataset so that the bumps in the density curve don’t represent just one or a couple of observations\n\n\n\nJitter Plots\n\n\n\n\n\n\n\n\n\n\nThese show all the points in every category, “jittered” (moved slightly away from the category axis) to reduce overplotting\nIf the dataset is too large, overplotting may still be a big problem\nJitter plots are often used as an additional layer on top of boxplots or violin plots to make the size of the dataset, and the locations of individual datapoints, more visible\n\n\n\nSina Plots\n\n\n\n\n\n\n\n\n\n\nThese show all the points in every category, arranged so that the width of the point cloud corresponds to the density of observations\nIf the dataset is too large, overplotting may become an issue\nA sina plot is a bit of a hybrid between a violin plot and a jitter plot; or, a more organized, less random version of a jitter plot.",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Gallery: Mixed Quantitative + Categorical</span>"
    ]
  },
  {
    "objectID": "graphics-choose-cat-quant-mix.html#facets",
    "href": "graphics-choose-cat-quant-mix.html#facets",
    "title": "15  Gallery: Mixed Quantitative + Categorical",
    "section": "15.2 Facets?",
    "text": "15.2 Facets?\nYou can also consider making multi-panel plots with one histogram, density plot, or dotplot per facet, but comparing between facets is usually harder than comparing boxplots or violin plots on a single axis.\nMulti-facet plots can show one panel per group, for any kind of plot seen so far: a bar graph for each group, a stacked bar for each group, a scatterplot for each group, a set of boxplots for each group, etc. etc.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding: Quant. + Cat.\nThere are some errors and inconsistencies in the chart below!\nCheck it out – can you find them?\n\n\n\nchart choice infographic",
    "crumbs": [
      "Choose a Graph Type",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Gallery: Mixed Quantitative + Categorical</span>"
    ]
  },
  {
    "objectID": "graphics-code.html",
    "href": "graphics-code.html",
    "title": "Data Visualization with ggformula",
    "section": "",
    "text": "Section Learning Outcomes\nAfter this section, you will:\n1.Choose, and use R to create, an appropriate graphical display for a specified (combination of) variables. 3. Plan and critique statistical graphics based on design principles.\nNote: You NEED NOT memorize all the information in this section.\nReview it now, but know you will probably return to this section for later reference.\nYour goal should be to finish with a basic idea of how ggformula (gf_...()) graphics functions work, and knowing where in this section to look for examples to follow.",
    "crumbs": [
      "Data Visualization with ggformula"
    ]
  },
  {
    "objectID": "graphics-code.html#background",
    "href": "graphics-code.html#background",
    "title": "Data Visualization with ggformula",
    "section": "Background",
    "text": "Background\n\nWhat’s a “graph”?\nA note about vocabulary: in this course, when we say “visualization” or “graph” or “plot” we mean a picture displaying data.\nSpreadsheets and tables presenting data are not included - If you are asked to “produce a graph” or “create a visualization” a table of summary statistics is not generally a desirable response.\nTables are not unimportant; they just don’t count, to us, as graphs, plots, or visualizations.\nOften a table might accompany a visualization, for additional reference and numeric values to cite…but we’d argue that it’s usually the picture that is memorable, and that tells the story.\n\n\nLibraries\nThere are many ways to generate graphics in R. Some popular packages for visualization in R include graphics (which is included in the most basic R installation) and ggplot2. If you don’t know any of those yet, don’t worry a bit (and maybe continue right to the next section). If you do know one of those other systems, read on…\nIn this course we will use ggformula, which is built on top of ggplot2 (so the graphs you create will be ggplot objects and can be modified as such). However, the syntax ggformula uses to specify input variables and settings is a bit different than ggplot2. The differences are desirable to us because:\n\nWe like consistency: ggformula input syntax is more like that of the functions we’ll be using to fit models to data\nWe often create graphics with several layers: Overlaying one graph on top of another is a bit easier in ggformula than in ggplot\n\nBecause of the variety of graphics packages in use in the R ecosystem, it is highly recommended that you not do a web search to find code examples to produce a certain graph. This section, course notes, and other course materials should have all the examples that you need (and if not, request additions!). In particular, mixing together code from different graphics packages generally leads to confusing disasters.\n\n\nMotivation: Realize the Dream!\nFigures are a crucial tool for exploring your data and communicating what you learn from the data.\nWhether you are doing a quick check to assess basic features of a dataset or creating a key figure for an important presentation, the best practice is to work thoughtfully. You already learned about creating graphics by I.C.E.E:\n\nThe I.C.E.E. method:\n\nImagine how you want your graph to look, before you\nCode. Once you have the basic starting point,\nEvaluate your work, and\nElaborate (refine it).\n\nRepeat until the figure is as awesome as it needs to be.\nWhat’s the missing piece that we left for last? That’s right: the code. Not most important, but crucial.\nSpecifically, you will now focus on implementation – you have a plan in mind; now how can you do it in R?\nThis section provides a set of code examples and practice exercises, but leaves out the details of design already covered in previous sections. Here, the assumption is you know what you want, and just need the technique to create it.\nTreat this section like a reference manual: it’s more important that you know how to look stuff up here than that you spend lots of time on every topic, and you don’t need to memorize. Complete the first 4 sections if you are new to ggformula, and then refer to the others as needed and as time allows. Get used to using this site as a “manual” to look stuff up on demand.\nGraph types marked with an (o) and with their section header in italics are ones that are optional and/or more advanced - skip if you wish.",
    "crumbs": [
      "Data Visualization with ggformula"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html",
    "href": "graphics-code-ggformula.html",
    "title": "16  ggformula and Formulas",
    "section": "",
    "text": "16.1 Two important questions\nNote: Much of the content in this part was originally written by Randall Pruim, a Calvin colleague who’s also the author of the ggformula package, with co-author Danny Kaplan. Thanks, Randy and Danny! You can view other reference material related to ggformula online.\nTo get R (or any software) to create the above plot (or do anything else, really), there are two important questions you must be able to answer. Before continuing, see if you can figure out what they are.\nWhat two questions do you have in mind?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#the-questions",
    "href": "graphics-code-ggformula.html#the-questions",
    "title": "16  ggformula and Formulas",
    "section": "16.2 The Questions",
    "text": "16.2 The Questions\nTo get R (or any software) to create the plot, there are two important questions you must be able to answer:\n\n1. What do you want the computer to do?\n\n\n2. What must the computer know in order to do that?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#answers-to-the-questions",
    "href": "graphics-code-ggformula.html#answers-to-the-questions",
    "title": "16  ggformula and Formulas",
    "section": "16.3 Answers to the questions",
    "text": "16.3 Answers to the questions\nTo make the scatter plot you saw before, the answers to our questions are\n\n1. What do you want the computer to do?\nA. Make a scatter plot (i.e., a plot consisting of points)\n\n\n2. What must the computer know in order to do that?\nA. The data used for the plot:\n\nThe variable to be plotted along the vertical (\\(y\\)) axis.\nThe variable to be plotted along the horizontal (\\(x\\)) axis.\nThe data set that contains the variables.\n\nWe just need to learn how to tell R these answers.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#graphics-with-formulas",
    "href": "graphics-code-ggformula.html#graphics-with-formulas",
    "title": "16  ggformula and Formulas",
    "section": "16.4 Graphics with Formulas",
    "text": "16.4 Graphics with Formulas",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#the-formula-template",
    "href": "graphics-code-ggformula.html#the-formula-template",
    "title": "16  ggformula and Formulas",
    "section": "16.5 The Formula Template",
    "text": "16.5 The Formula Template\nWe will provide answers to our two questions by filling in the boxes of this important template:\n\n\ngoal ( yyy ~ xxx , data = mydata )\n\n\n \nWe just need to identify which portions of our answers go into which boxes.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#the-name-of-the-game",
    "href": "graphics-code-ggformula.html#the-name-of-the-game",
    "title": "16  ggformula and Formulas",
    "section": "16.6 The Name of the Game",
    "text": "16.6 The Name of the Game\nIt is useful to provide names for the boxes:\n\n\ngoal (  y  ~  x  , data = mydata , …)\n\n\n \nThese names can help us remember which things go where. (The ... indicates that there are some additional input arguments we will add eventually.)\n\nOther versions\nSometimes we will add or subtract a bit from our formula. Here are some other forms we will eventually see.\n\n# simpler version\ngoal( ~ x, data = mydata )          \n# fancier version\ngoal( y ~ x | z , data = mydata )   \n# unified version\ngoal( formula , data = mydata )",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#questions-and-the-formula-template",
    "href": "graphics-code-ggformula.html#questions-and-the-formula-template",
    "title": "16  ggformula and Formulas",
    "section": "16.7 2 Questions and the Formula Template",
    "text": "16.7 2 Questions and the Formula Template\n \n\n\ngoal (  y  ~  x  , data = mydata )\n\n\n \n\nQ. What do you want R to do? A. goal\n\nYour answer to this question determines the function to use.\nFor a plot, the function will describe what sorts of marks to draw (points, in our example).\n\n\n\nQ. What must R know to do that? A. arguments\n\nYour answer to this question determines the inputs to the function.\nFor a plot, we must identify the variables and the data frame that contains them.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#assembling-the-pieces",
    "href": "graphics-code-ggformula.html#assembling-the-pieces",
    "title": "16  ggformula and Formulas",
    "section": "16.8 Assembling the pieces",
    "text": "16.8 Assembling the pieces\n\nTemplate\n \n\n\ngoal (  y  ~  x  , data = mydata )\n\n\n \n\n\nPieces\n\n\n\n\n\n\n\n\nbox\nfill in with\npurpose\n\n\n\n\n`goal`\n`gf_point`\nplot some points\n\n\n`y`\n`births`\ny-axis variable\n\n\n`x`\n`date`\nx-axis variable\n\n\n`mydata`\n`Births1978`\nname of data set\n\n\n\n\n\nYour Turn\nPut each piece in its place in the template below and then run the code to create the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIf you get an “object not found” or “could not find function” error message, that indicates that you have not correctly filled in one of the four boxes from the template.\nNote: R is case sensitive, so watch your capitalization.\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_point(births ~ date, data = Births1978)\n\n\n\n\nFor reference, here are the first three rows of Births1978.\n\n\n  births       date day_of_year\n1   7716 1978-01-01           1\n2   7543 1978-01-02           2\n3   8833 1978-01-03           3",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#using-formulas-to-describe-plots",
    "href": "graphics-code-ggformula.html#using-formulas-to-describe-plots",
    "title": "16  ggformula and Formulas",
    "section": "16.9 Using formulas to describe plots",
    "text": "16.9 Using formulas to describe plots\n\nThe tilde (wiggle)\nThe most distinctive feature of ggformula plots is the use of formulas to describe the positional information of a plot. Formulas in R always involve the tilde character, which is easy to overlook. It looks like this:\n \n\n\n  ~  \n\n\n \nMake sure you know where the tilde is located on your computer’s keyboard! It is often near the upper left-hand corner on US keyboards.\n\n\n\n\n\n\n\n\n\n\n\nFormula shapes\nMost gf_ functions take a formula that describes the positional attributes of the plot. Using one of these functions with no arguments will show you the “shape” of the formula it requires.\n\n\nGetting help on formula shapes\nRun this code to see the formula shape for gf_point().\n\n\n\n\n\n\n\n\nYou should see that gf_point()’s formula has the shape y ~ x, so the y-variable name goes before the tilde and the x-variable name goes after. (Think: “y depends on x”. Another way to remember the order is y ~ x: the y-axis label appears farther left than the x-axis label.)\n\n\nOrder matters in formulas!\nReverse the roles of the variables, changing births ~ date to date ~ births. How does the plot change?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-ggformula.html#style-note",
    "href": "graphics-code-ggformula.html#style-note",
    "title": "16  ggformula and Formulas",
    "section": "16.10 Style Note",
    "text": "16.10 Style Note\n\nSize Matters\nWhat is the largest key on your keyboard?\n\n return/enter caps lock delete the space bar the arrow keys\n\n\n\nUmm…so why did I ask you that?\n\nMaybe there is a reason that the space bar key is biggest – you should use it a lot!.\n\n\n\nR, People, and Spaces\nR is not very picky about spaces.\n\nAny number of spaces is equivalent to a single space.\nSometimes (but not always) spaces are optional.\n\nMy advice is to use spaces liberally. Even if R doesn’t care, it makes your code easier for people to read.\n\nAlways put a space around things like +, *, ~ etc. (This is a place where R doesn’t care whether you have a space or not, but I recommend you do.)\nAlways put a space after each comma\nNever put a space between a function name and its parentheses (write head(data) not head (data))\nUse spaces and line breaks to make your code easy to read.\n\nMimic the examples you see in this section. And if you want a more comprehensive code style guide for R, check out the tidyverse style guide.\nBonus points if you point out my own style inconsistencies to me so I can correct them!",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ggformula and Formulas</span>"
    ]
  },
  {
    "objectID": "graphics-code-data.html",
    "href": "graphics-code-data.html",
    "title": "17  Datasets Used",
    "section": "",
    "text": "17.1 Dataset Descriptions\nSeveral datasets will be used in this section.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Datasets Used</span>"
    ]
  },
  {
    "objectID": "graphics-code-data.html#dataset-descriptions",
    "href": "graphics-code-data.html#dataset-descriptions",
    "title": "17  Datasets Used",
    "section": "",
    "text": "The Births1978 dataset contains information about the number of babies born in the U.S.A. on each day of the year 1978\nThe university_teachers dataset gives the proportion of university instructors who held different job titles (for example, Tenured Professor or Part-time Instructor), for a selection of years.\nThe NHANES dataset contains measurements from 10,000 human subjects in the National Health and Nutrition Evaluation Survey. To learn more about the data, try one or more of these (Shown for NHANES, but can do for any dataset):\n\n?NHANES (only for NHANES and other built-in R datasets)\nnames(NHANES)\nglimpse(NHANES)\ninspect(NHANES)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Datasets Used</span>"
    ]
  },
  {
    "objectID": "graphics-code-data.html#plotting-functions",
    "href": "graphics-code-data.html#plotting-functions",
    "title": "17  Datasets Used",
    "section": "17.2 Plotting Functions",
    "text": "17.2 Plotting Functions\nThere are many gf_... functions in the ggfomula package that create different types of plots. Some are covered here; many are not.\nThere are also helper functions that can customize axis labels, make multi-panel plots, and more.\nJust to get an idea of what is included in the ggformula package, run the code below to get a list of all the gf_ functions that exist (not all are covered here):\n\n\n\n\n\n\n\n\nThe following sections give examples of how to use many of the gf_ functions to create graphics.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Datasets Used</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html",
    "href": "graphics-code-plots.html",
    "title": "18  Code Examples",
    "section": "",
    "text": "18.1 Histograms\nBelow, you’ll find code examples to create and customize many types of graphs using ggformula.\nBefore making any graphs, you may want to choose a theme – this sets options like the background color of the plot (grey or white?), the presence/absence of gridlines, and more.\nYou can follow the link to see examples of available themes.\nFor general use, I recommend you chose theme_minimal…put the code below in your qmd setup chunk (once) and all your plots will have the desired theme.\n(For technical reasons, to avoid having to set the theme in every single R chunk, many plots will be shown in the default theme in this section.)\nTry out the code below, and adjust the number of bins to better display the distribution.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#histograms",
    "href": "graphics-code-plots.html#histograms",
    "title": "18  Code Examples",
    "section": "",
    "text": "Histograms require a formula with only one variable in it: ~ x. (Notice that x goes on the right side.)\nYou can change the size of the bins using either bins (the number of bins) or binwidth (the width of the bins). Experiment with different bins, trying to find balance between too many and too few. (If you don’t provide bins or binwidth information, R will just make something up. You can usually do better if you take control.)\nTo get density instead of counts on the y-axis, switch from function gf_histogram() to gf_dhistogram().",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#density-plots",
    "href": "graphics-code-plots.html#density-plots",
    "title": "18  Code Examples",
    "section": "18.2 Density plots",
    "text": "18.2 Density plots\nA density plot is a smoothed contour tracing the shape of a dataset’s distribution. The gf_density() and gf_dens() functions produce these plots (in slightly different ways): gf_density() plots are filled-in, while gf_dens() just plots a line showing the shape of the distribution.\n\n\n\n\n\n\n\n\nOften density plots can be a nice way to show distributions of a quantitative variable for each category in a categorical variable. To do that, we add the fill input to a gf_density() call, with the form fill = ~ categorical_variable_name to change the color of the filled regions by category:\n\n\n\n\n\n\n\n\nIf you wanted to change the color of the lines in a gf_dens() plot, you would use color instead of fill:",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#boxplots",
    "href": "graphics-code-plots.html#boxplots",
    "title": "18  Code Examples",
    "section": "18.3 Boxplots",
    "text": "18.3 Boxplots\nBoxplots are most often used to allow a quick comparison of the distribution of a quantitative variable in different groups, as shown here.\n\n\n\n\n\n\n\n\nThe orientation input states whether it’s the “x” or “y” axis variable that defines the groupings. This should not be necessary - it should be determined automatically according to which variable is categorical - but there’s a bug (?) as of Sept. 2025 such that the gf_boxplot() function won’t run without it.\n\nWhat happens if you swap the “x” and the “y” in the formula for a boxplot? (Try it and see before answering.)\n\n Nothing. The plot looks the same. An error. The quantitative variable must always be “y”, after the ~. The coordinates flip (whichever variable is “y” in the formula ends up on the y-axis).\n\n\n\n\nClick for explanations of solution above.\n\nFor graphs where one variable is shown on the x axis and one on the y axis, swapping the order of the variables in the formula usually flips the coordinates.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#violin-plots-o",
    "href": "graphics-code-plots.html#violin-plots-o",
    "title": "18  Code Examples",
    "section": "18.4 Violin Plots (o)",
    "text": "18.4 Violin Plots (o)\nViolin plot construction is very similar to that of boxplots, detailed in the previous section.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#jitter-plots-o",
    "href": "graphics-code-plots.html#jitter-plots-o",
    "title": "18  Code Examples",
    "section": "18.5 Jitter Plots (o)",
    "text": "18.5 Jitter Plots (o)\nTo “jitter” is to slightly alter the location of points in a graph, so that instead of being overplotted, you can see individual ones more clearly.\nIt can be used on its own:\n\n\n\n\n\n\n\n\nBut more frequently is used as a layer in combination with boxplots or violin plots. We use a pipe (|&gt;) to add the jitter layer:\n\n\n\n\n\n\n\n\n\nWhat do you think might help the (awful) preceding violin/jitter plot to be more informative? Mark all correct answers TRUE.’\nMaybe adding the jitter plot just is not ideal for this data TRUEFALSE\nMaking the jittered points semi-transparent could help. TRUEFALSE\nMaking the dots in the jitter plot larger and adding color and shape by MaritalStatus could help. TRUEFALSE\nChanging the color of the jittered points so they fade into the background a bit could help. TRUEFALSE\nAdjusting the width of the jittered point-column (by making input width smaller than its default 0.4) could help. TRUEFALSE\n\n\n\nClick for explanations of solution above.\n\n\nJitter plots often work better with smaller datasets\nFor a larger dataset you may need to make the points semi-transparent (by setting input alpha, which ranges from 0-1, closer to 0).\nSometimes making the width of the jitter wider or narrower can make the plot easier to interpret (and more beautiful).\nIf a plot is already a bit overwhelming, the solution is usually not to add more variables and colors and distracting features!\n\n\n\nMaking the fixes\nCan you make it even better than this?\n\n\n\n\n\n\n\n\n\n\nBoxplots without Outliers\nIf you add a jitter plot on a boxplot, any outliers get plotted twice: once in the boxplot layer and once in the jitter layer. Not good. You can remove them from the boxplot in this case by setting outlier.shape = NA.\nTry running the example code, then adjust the jitter plot as you think is needed (color, transparency alpha, width).",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#sina-plots-o",
    "href": "graphics-code-plots.html#sina-plots-o",
    "title": "18  Code Examples",
    "section": "18.6 Sina Plots (o)",
    "text": "18.6 Sina Plots (o)\nSimilar to a jitter plot is a sina plot – the difference is that the sina plot shapes the dot cloud to indicate data density, rather than fitting all the dots into a rectange.\nWe need the additional package ggforce to make a sina plot using the gf_sina() function.\nCheck it out…and play a bit! What does the sina plot look like overlaid on boxplots? What if you adjust size, transparency alpha, or even color?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#ordering-groups-by-median",
    "href": "graphics-code-plots.html#ordering-groups-by-median",
    "title": "18  Code Examples",
    "section": "18.7 Ordering Groups by Median",
    "text": "18.7 Ordering Groups by Median\nWhen plotting boxplots or violinplots (etc.), R’s default is to order the levels of the categorical variable in alphabetical order.\nAlphabetical is so rarely the order you want!\nMore often, you should order by median (or mean) value, or by some intrinsic order.\nTake the boxplots from the last example:\n\n\n\n\n\n\n\n\n\nThe alphabetical order is nonsense. We can sort the categories by median percentage using the function fct_reorder() from the forcats package.\n\n\n\n\n\n\n\n\nThe first input to fct_reorder() is the categorical variable containing the groups; the second is the quantitative variable whose median you want to order by.\nIf you want to use some other function of the second variable, say the mean() instead of median, you add the input .fun:\n\n\n\n\n\n\n\n\n(Which ends up looking about the same, in this particular case.)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#ordering-groups-manually",
    "href": "graphics-code-plots.html#ordering-groups-manually",
    "title": "18  Code Examples",
    "section": "18.8 Ordering Groups Manually",
    "text": "18.8 Ordering Groups Manually\nIn some cases, you may want to use some human-meaningful ordering of groups. For example, we might order the teacher titles from least to most senior: Grad students, the part-time employees, then Full-time non-tenure-track, then Full-time tenure-track, then Full-time tenured.\nThere is no easy way to tell R the required order other than just listing it out.\nA function to carry out such re-ordering is fct_relevel().\nSince the code to reorganize the levels is a bit long to do inside the plotting call, and since we usually want the same ordering every time we use such a variable, we modify the variable in the dataset before plotting.\nAdd the missing levels to the code below, then run it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nuniversity_teachers &lt;- university_teachers |&gt;\n  mutate(faculty_type = fct_relevel(faculty_type,\n                                    \"Graduate Student Employees\",\n                                    \"Part-Time Faculty\",\n                                    \"Full-Time Non-Tenure-Track Faculty\",\n                                    \"Full-Time Tenure-Track Faculty\",\n                                    \"Full-Time Tenured Faculty\")\n         )\n gf_boxplot(faculty_type ~  percentage, \n           data = university_teachers,\n           orientation = \"y\")",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#ordering-groups-by-frequency",
    "href": "graphics-code-plots.html#ordering-groups-by-frequency",
    "title": "18  Code Examples",
    "section": "18.9 Ordering Groups by Frequency",
    "text": "18.9 Ordering Groups by Frequency\nFinally, we might order groups by the number of observations in each group (frequency).\nTo do this, we can use function fct_infreq().\nAdd a sina or jitter plot to the violins to verify the re-ordering:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_violin(BPSysAve ~ fct_infreq(MaritalStatus), data = NHANES) |&gt;\n  gf_jitter(color = 'grey', alpha = 0.1, width= 0.15)\n  \ngf_violin(BPSysAve ~ fct_infreq(MaritalStatus), data = NHANES) |&gt;\n  gf_sina(color = 'grey', alpha = 0.1, width= 0.15)\n(NA (missing) stays at the end, even if it’s commonly observed.)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#scatter-plot",
    "href": "graphics-code-plots.html#scatter-plot",
    "title": "18  Code Examples",
    "section": "18.10 Scatter Plot",
    "text": "18.10 Scatter Plot\nA simple scatter plot is created with gf_point() and has a formula like y ~ x.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#pointline-size",
    "href": "graphics-code-plots.html#pointline-size",
    "title": "18  Code Examples",
    "section": "18.11 Point/Line Size",
    "text": "18.11 Point/Line Size\nTo control the size of points and lines in scatter and line charts, use input size. It has a relative numeric value – larger than 1 means larger than the default.\nTry adjusting the size of the points in the plot below. You might want them bigger or smaller depending on the point you are trying to make.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#bubble-chart-o",
    "href": "graphics-code-plots.html#bubble-chart-o",
    "title": "18  Code Examples",
    "section": "18.12 Bubble Chart (o)",
    "text": "18.12 Bubble Chart (o)\nA bubble chart is a scatter plot where the size of the points is controlled by some third variable. This can be useful when the “dots” represent items that should be visually weighted differently - for example, one point per country (where countries have different population sizes) or one point per class (with different class sizes).\nWe use the input size = ~x where x is the name of the variable that will control the point size.\nNote that our example datasets don’t have any great examples of when this is useful – in the example below the bubble chart may not be necessary.\n\n\n\n\n\n\n\n\nWhy is this ineffective? One big reason is that the dataset is so big that the points overlap - making some of them bigger just makes it worse. Just so you can see a bubble chart, let’s redo the plot with a subsample of the data.\nThere is not usually any reason to do this with real data - you need to find a way to show it all!\nWe are showing a subset of the data here just to illustrate bubble plots, not because it’s ok to do this.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#line-plot",
    "href": "graphics-code-plots.html#line-plot",
    "title": "18  Code Examples",
    "section": "18.13 Line Plot",
    "text": "18.13 Line Plot\nTo plot a line instead of dots, simply use gf_line() instead of gf_point(). If you want the dots connected in the order of the rows of the dataset instead of in ascending x, you can replace gf_line() with gf_path().",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#multiple-layers-with-pipes",
    "href": "graphics-code-plots.html#multiple-layers-with-pipes",
    "title": "18  Code Examples",
    "section": "18.14 Multiple layers with pipes |>",
    "text": "18.14 Multiple layers with pipes |&gt;\nA single plot may have multiple layers. For example, you might want a scatter plot with a trend line overlaid on it, or a histogram with a standard normal curve overlaid.\nTo create a multi-layered plot, simply append |&gt; at the end of the code for one layer and follow that with another layer. (The |&gt; symbol is called a “pipe” because it sends the results of one operation on to the next operation for further processing. We often read |&gt; as “and then…”)\n\nExercise\n\nIf you run the following code as is, you will get two separate plots.\nCombine these two layers into a single plot by appending |&gt; at the end of the first line.\nTry adding another layer for a third variable.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#adding-lines",
    "href": "graphics-code-plots.html#adding-lines",
    "title": "18  Code Examples",
    "section": "18.15 Adding Lines",
    "text": "18.15 Adding Lines\nThere are three helper functions to help add lines to gf_ graphics:\n\ngf_vline(xintercept = ___) (vertical line)\ngf_hline(yintercept = ___) (horizontal line)\ngf_abline(slope = ___, intercept = ___) (straight line)\n\nFor example, add a line at x = 4, one at y = 2, and one at y = x (just as a demonstration - there is not a very good reason to add the x - 4 and y = 2 reference lines to this plot…)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#best-fit-line",
    "href": "graphics-code-plots.html#best-fit-line",
    "title": "18  Code Examples",
    "section": "18.16 Best fit line?",
    "text": "18.16 Best fit line?\nWe could add a (simple) linear regression line to a scatter plot using the function gf_lm(), or a smooth using gf_smooth().\n\nBUT PLEASE DON’T. JUST NO. ALMOST NEVER A GOOD PLAN.\nIt looks so nice! Why is it a bad idea?\n\n\n\n\n\n\n\n\nSo why am I not allowed to use gf_lm() and gf_smooth()?\n\n It is usually misleading and will contradict other parts of the data analysis I present. My prof is just mean and opinionated. Who even cares? The function has too many bugs and causes trouble.\n\n\n\nUmm…so why is it misleading?\n\nIn most cases, our response (y) variable is expected to be associated with not just one predictor. But unless ALL the variables whose relationships you’re interested in quantifying are ALL shown in the graph, the line gf_lm() draws won’t match up with the statistical analysis you’ll do with the same data.\nIn other words, you will be showing one thing and later saying (and maybe also showing) another contradictory thing. Not ideal!\nIn the case of the smooth, it gets even more complicated. If you are modeling a relationship as linear, then showing a curve contradicts your later analysis (just like the problem with gf_lm()). If you are fitting a GLM or a GAM (where the model might estimate a nonlinear relationship), it will still almost certainly be a different one from the one that gf_smooth() will draw.\nSO adding these lines usually makes you a liar: one part of your report ends up contradicting another.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#bar-graphs",
    "href": "graphics-code-plots.html#bar-graphs",
    "title": "18  Code Examples",
    "section": "18.17 Bar graphs",
    "text": "18.17 Bar graphs\nBar graphs help visualize the distribution of a categorical variable, and we can create them with gf_bar().\n\n\n\n\n\n\n\n\n\nPercents and Proportions?\nWhat if we want to show the percent or proportion in each category, rather than the number of observations? gf_percents() and gf_props() to the rescue! Try changing the function from gf_bar() to gf_percents() or gf_props() and see what happens.\nYou can also add |&gt; gf_refine(coord_flip()) to swap the axes. Try that too!\nBut you CANNOT use a formula of the form y ~ x! It might seem to run, but trust me, R is ignoring your y. It knows it already needs to use the y axis to show counts (or proportions or percents) – no room for another data variable there…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_props( ~ TVHrsDay, data = NHANES)\ngf_percents( ~ TVHrsDay, data = NHANES)\ngf_percents( ~ TVHrsDay, data = NHANES) |&gt;\n  gf_refine(coord_flip())",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#stacked-bar-graphs",
    "href": "graphics-code-plots.html#stacked-bar-graphs",
    "title": "18  Code Examples",
    "section": "18.18 Stacked bar graphs",
    "text": "18.18 Stacked bar graphs\nWhat if, instead of one figure panel per group, you want to see a stacked bar graph for the same data? Here’s an example. You use the input:\nfill= ~ variable_name\nto specify the name of the variable that defines the groups (here, Marijuana).\nTry to see what happens if you use gf_props() or gf_percents().\n\n\n\n\n\n\n\n\n\nWhen showing a stacked bar graph with proportions, what is the default DENOMINATOR used to computing the proportions? Use the graph you just made to figure it out.\n\n The number of observations in the stack (so the parts of one bar together sum to 1) The total number of observations in the dataset (so all the parts of all the bars together sum to 1) The number of observations in the “fill” variable group (so all bar-parts of the same color sum to 1)\n\n\nArgh! That’s not usually what you want…",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#changing-the-denominator",
    "href": "graphics-code-plots.html#changing-the-denominator",
    "title": "18  Code Examples",
    "section": "18.19 Changing the Denominator",
    "text": "18.19 Changing the Denominator\nYou can control the denominator used to compute bar graph proportions with the input denom. Its value should be a one-sided function of the form ~ x giving the role in the plot of the variable defining the groups to use as the denominator. Other options include fill and PANEL.\nIn our gf_props() plot above, we might pick x, so that the total proportion in each T.V. hours group sums to 1. Give it a try:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_props( ~ TVHrsDay, fill = ~ Marijuana, data = NHANES,\n          denom = ~ x)\n\n\n\n\nThis works generally. A shortcut in the stacked-bar-graph case is to use the input position = \"fill\" instead of denom:",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#side-by-side-bar-graphs",
    "href": "graphics-code-plots.html#side-by-side-bar-graphs",
    "title": "18  Code Examples",
    "section": "18.20 Side-by-Side bar graphs",
    "text": "18.20 Side-by-Side bar graphs\nWhat if, instead of stacked bars, you want side-by-side bars? Simply add the additional argument\n\nposition='dodge'.\n\n\ngf_bar( ~ TVHrsDay, \n        fill = ~ Marijuana,\n        data = NHANES, \n        position = 'dodge')\n\n\n\n\n\n\n\n\nNote that if you want to change the denominator, you can use position = 'dodge' and denom = ~x together, but you can’t have position be both “stack” and “fill”.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#pie-chart-o",
    "href": "graphics-code-plots.html#pie-chart-o",
    "title": "18  Code Examples",
    "section": "18.21 Pie Chart (o)",
    "text": "18.21 Pie Chart (o)\nMaking nice pie charts in R is a bit of work, because most of the plotting libraries are not set up to do it well…you have to force them to do your will. Don’t say you weren’t warned! But, with a little effort, you can make decent pie charts.\nA pie chart usually doesn’t have any background elements like axis labels or gridlines. To make one, we make a bar graph with gf_bar(), put it in polar coordinates, and ensure we are using a plot template with no background elements via theme_set(theme_void()).\nStrangely enough, we want to start with a stacked bar chart colored by our variable of interest. We include 1 rather than a variable name in the formula (because we want just one stacked bar), and we add the input width=1 because we want the single bar to take up the whole width of the graph. And we swap the y-axis into polar coordinates (try removing the gf_refine() line to see how it looks before pie-ification).\n\n\n\n\n\n\n\n\nAfter creating a pie chart, make sure you revoke the “void” theme by running the code below (or your later graphs will have no visible axes or axis labels…)\n\ntheme_set(theme_minimal()) \n\nYou can see examples of available themes at: https://ggplot2.tidyverse.org/reference/ggtheme.html",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#bar-graph-pre-tabulated-o",
    "href": "graphics-code-plots.html#bar-graph-pre-tabulated-o",
    "title": "18  Code Examples",
    "section": "18.22 Bar graph, pre-tabulated (o)",
    "text": "18.22 Bar graph, pre-tabulated (o)\nSometimes, you may be given data that is already tabulated. Instead of a dataset with one row for every case, you will have one row for every group, and a variable that gives the number of observations in each group. The university_teachers dataset is an example, with one row for each combination of job title and year.\n\nDT::datatable(university_teachers)\n\n\n\n\n\nWe can use the function gf_col to make a bar graph of pre-tabulated data. This function always expects the counts (or proportions or percentages) as the y part of the formula, and the group names as x (after the tilde).\n\n\n\n\n\n\n\n\nThis plot illustrates a common issue – category labels that overlap and become illegible. What can we do to fix it?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#axis-labels-that-dont-fit",
    "href": "graphics-code-plots.html#axis-labels-that-dont-fit",
    "title": "18  Code Examples",
    "section": "18.23 Axis Labels that Don’t Fit",
    "text": "18.23 Axis Labels that Don’t Fit\nSometimes - particularly for bar graphs of categorical variables with long category names - axis tick labels overlap in an ugly way. For example:\n\ngf_bar(~Education, data=NHANES)\n\n\n\n\n\n\n\n\n\nFlip the Axes\nOne simple way to solve this problem is to flip the x and y axes of the plot.\n\ngf_bar(~Education, data=NHANES) |&gt;\n  gf_refine(coord_flip())\n\n\n\n\n\n\n\n\n\n\nRotate the Labels\nAnother solution is to rotate the axis labels.\nThis is not a great solution since horizontal labels are easier to read and make your graph faster to digest!\nWe can do it, though, by modifying the angle and hjust values for the x tick labels in the plot’s theme. angle is the angle in degrees by which to rotate the labels, and hjust moves them up and down (positive hjust moves down, and negative moves up).\nFor example:\n\ngf_bar(~Education, data = NHANES) |&gt; \n    gf_theme(axis.text.x = element_text(angle = 65, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nYour Turn!\nThe dataset at http://sldr.netlify.app/data/MammalMetabolicRates.csv provides data on mammal metabolic rates. Read it in and make a bar graph of the number of observations per Order (or per Family, Genus, or Species) with legible axis tick labels.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nmmr &lt;- read_csv('http://sldr.netlify.app/data/MammalMetabolicRates.csv')\ngf_bar(~ Order, data = mmr) |&gt;\n  gf_refine(coord_flip())\n\n# to be extra: adjust the order of groups\nmmr &lt;- read_csv('http://sldr.netlify.app/data/MammalMetabolicRates.csv')\ngf_bar(~ fct_infreq(Order), data = mmr) |&gt;\n  gf_refine(coord_flip())\n# note: other solutions are possible.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#tabulating-data-o",
    "href": "graphics-code-plots.html#tabulating-data-o",
    "title": "18  Code Examples",
    "section": "18.24 Tabulating Data (o)",
    "text": "18.24 Tabulating Data (o)\nTo make a Cleveland dot-plot or lollipop plot, you need to switch from a dataset with one row per observation to one row per group that you want to plot. We will learn more about this kind of data wrangling later.\nHere, we want to group_by() the variable that defines the groups, and then summarize() within each group by computing the number of observations n() or the proportion or percentage of interest. Finally, always ungroup() at the end. Let’s try it with the mmr data.\n\nmmr_tab &lt;- mmr |&gt;\n  group_by(Order) |&gt;\n  summarize(n = n(), # n() is special function to compute n in group\n            prop = n() / nrow(mmr),\n            perc = prop / 100) |&gt;\n  ungroup()\n\nNote that I’m intentionally showing you how to do this for proportions / categorical variables and not for means of quantitative variables!\nThere’s a reason for that. Our best practice is the show all the data as much as we can. For more on why, see Bang goes the detonator plot!\nBut for a categorical variable - particularly a binary one - a proportion is pretty much “all” of the data, much more so than a mean summarizes “all” of the values of a quantitative variable!",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#cleveland-dotplot-o",
    "href": "graphics-code-plots.html#cleveland-dotplot-o",
    "title": "18  Code Examples",
    "section": "18.25 Cleveland Dotplot (o)",
    "text": "18.25 Cleveland Dotplot (o)\nNote: this requires tabulated data.\nWith tabulated data, a Cleveland dot plot is just a scatter plot. But we need to order n – by what?? Give it a try!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\ngf_point(fct_reorder(___, ___) ~ n, data = mmr_tab)\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_point(fct_reorder(Order, n) ~ n, data = mmr_tab)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#lollipop-plot-o",
    "href": "graphics-code-plots.html#lollipop-plot-o",
    "title": "18  Code Examples",
    "section": "18.26 Lollipop Plot (o)",
    "text": "18.26 Lollipop Plot (o)\nThis is just a Cleveland dot plot plus “sticks”; we use gf_segment() to add the sticks. The formula for gf_segment() has the form: y + yend ~ x + xend..",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#facets-multi-panel-plots",
    "href": "graphics-code-plots.html#facets-multi-panel-plots",
    "title": "18  Code Examples",
    "section": "18.27 Facets (Multi-panel plots)",
    "text": "18.27 Facets (Multi-panel plots)\nIf we want to look at all 20 years of birth data, overlaying the data is likely to put too much information in too little space and make it hard to tell which data is from which year. (Even with good color and symbol choices, deciphering 20 colors or 20 shapes is hard.) Instead, we can put each year in separate facet or sub-plot. By default the coordinate systems will be shared across the facets which can make comparisons across facets easier.\nThere are two ways to create facets. The simplest way is to add a vertical bar | to our formula.\n\n\n\n\n\n\n\n\nThe second way is to add on a facet command using |&gt;:\n\n\n\n\n\n\n\n\n\nPractice with facets\nEdit the plot below to:\n\nmake one facet for each day of the week (wday)\nmap color to year\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_point(births ~ day_of_year | wday, \n         data = Births, \n         size = 0.5, \n         color = ~ year)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#facet-grids",
    "href": "graphics-code-plots.html#facet-grids",
    "title": "18  Code Examples",
    "section": "18.28 Facet Grids",
    "text": "18.28 Facet Grids\nCreate a multi-panel plot that uses rows, or columns, or both in a fixed way. For example, you want to show a scatter plot of data for each of three years (three rows of facets) and four seasons (four columns of facets).\nTo do this, add another formula after the | in the formula input, as done below. Can you figure out what the formula does? If you need a hint, try changing year ~ wday to wday ~ year and see what happens…\n\n\n\n\n\n\n\n\n\nThe Facet Grid Formula\nHopefully, you figured out that the facet grid formula (the one to the right of the |) is interpreted as “row variable ~ column variable” – the resulting plot will have one row of facets for every value of the first variable, and one column of facets for every value of the second variable.\n\n\nPractice with the facet grid formula\nRecreate the plot below using gf_facet_grid(). This works much like gf_facet_wrap() and accepts a formula with one of three shapes:\n\ny ~ x (facets along both axes)\n~ x (facets only along x-axis)\ny ~ . (facets only along y-axis; notice the important dot in this one)\n\n(These three formula shapes can also be used on the right side of |.)",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#color-one-custom-choice",
    "href": "graphics-code-plots.html#color-one-custom-choice",
    "title": "18  Code Examples",
    "section": "18.29 Color: One custom choice",
    "text": "18.29 Color: One custom choice\nWe can manually set the color of the main element of a simple plot (like the line in gf_line(), or the points in gf_point()) using the ... part of our template.\n \n\n\ngoal (  y  ~  x  , data = mydata , …)\n\n\n \nThe general form for things in ... is attribute = value.\nFor example,\n\ncolor = \"red\" or fill = \"navy\" (note quotes) can be used to change the colors of things.\n(fill is typically used for regions that are “filled in” and color for dots and lines.)\n\nAfter running the code below, find the name of an R color at datanovia and change the points in the scatterplot to be that color.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#color-by-variable-values",
    "href": "graphics-code-plots.html#color-by-variable-values",
    "title": "18  Code Examples",
    "section": "18.30 Color: by variable values",
    "text": "18.30 Color: by variable values\nOften, rather than manually setting all elements to be one color, we want to map color to some variable (so that each value of that variable corresponds to a distinct color).\nTo do it, we provide the input color = ~variable to our plotting function. * color = ~wday maps color to the day of the week wday.\nFor example, to map color to wday in a time-series plot of 1978 births:\n\n\n\n\n\n\n\n\nThis works for continuous variables, too, but the color scale used will be continuous instead of distinct discrete colors that are easy to tell apart. Try mapping color to day_of_year in the same time-series plot as above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\ngf_point(births ~ date, data = Births1978, ...)\n\n\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\ngf_point(births ~ date, data = Births1978, color = _______)\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_point(births ~ date, data = Births1978, color = ~day_of_year)\n\n\n\n\n\nDoes it appear that the number of births is different on weekends and weekdays?\n\n more data would be needed to be able to answer no yes\n\nWhat actually happens if you omit the ~ before ~ wday? (Try it and see.)\n\n All the dots are the same color. It works just the same. The tilde is not required. There is an error message: 'wday' not found.\n\n\nEarlier, we saw how to change the fill color in density plots:\n\n\n\n\n\n\n\n\nIf you wanted to change the color of the lines in a gf_dens() plot, you would use color instead of fill:\n\n\n\n\n\n\n\n\nWe saw something similar in stacked bar graphs. So, you can color lines or fill shapes by a variable in many graph types - experiment!",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#change-color-palette",
    "href": "graphics-code-plots.html#change-color-palette",
    "title": "18  Code Examples",
    "section": "18.31 Change Color Palette",
    "text": "18.31 Change Color Palette\nWe can use gf_theme(scale_---_---( )).\nThe first --- is often either color or fill, to choose a color palette for the color or fill in the plot. (scale_fill_---() or scale_color_---()).\nThe second --- is the name of the type of color palette to use. There are lots of options, depending on the variable type and how you want to select the colors – type ?scale_color_ and let autofill show you all the choices!\nA good set of palettes to begin with is the RColorBrewer palettes:\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n\nThe middle group works better for categorical scales.\nTo choose one, note its name on the left. Then call (for example) scale_color_brewer(palette = 'Dark2'). You can add direction = -1 to reverse the order.\nTry the plot below, then test some other palette options (try reversing the order, too). Which do you prefer?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#color-one-group",
    "href": "graphics-code-plots.html#color-one-group",
    "title": "18  Code Examples",
    "section": "18.32 Color one group",
    "text": "18.32 Color one group\nSometimes you may wish to highlight the data for one particular group by greying out all other data. An easy way to do this is to create a variable that “keeps” only the group you want with fct_other(), then use gf_theme(scale_color_manual()) to set a manual color palette with two colors (grey and the other one you want).\nWhat if we want to highlight the proportion of teachers who are grad students? We make a new variable grad_stud that tells whether the faculty type is “Grad Student” or not, and color by it. We still group by faculty_type so that we get only one line per faculty type (try removing this to see the problem that happens).",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#axis-labels",
    "href": "graphics-code-plots.html#axis-labels",
    "title": "18  Code Examples",
    "section": "18.33 Axis labels",
    "text": "18.33 Axis labels\nOne of the most common customizations you will want to make to your plots will be to change the title, subtitle, and axis labels (and maybe add a caption). All these things can be done by chaining (|&gt;) the function gf_labs() with a plot layer.\nCheck out the example below, and try changing the text labels to ones that make sense to you. Note that all the input arguments to gf_labs are optional. So, for example, you could alter only the x-axis label by chaining the command gf_labs(x = 'My X Axis Label') with your plot.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#axis-limits",
    "href": "graphics-code-plots.html#axis-limits",
    "title": "18  Code Examples",
    "section": "18.34 Axis limits",
    "text": "18.34 Axis limits\nYou can also gf_lims() to set custom x and y axis limits.\nHow does adjusting the axis limits alter your interpretation of the plot?",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#grid-lines-o",
    "href": "graphics-code-plots.html#grid-lines-o",
    "title": "18  Code Examples",
    "section": "18.35 Grid Lines (o)",
    "text": "18.35 Grid Lines (o)\nGrid lines can be controlled using gf_theme() with inputs\n\npanel.grid\npanel.grid.major\npanel.grid.minor\npanel.grid.major.x\npanel.grid.major.y\npanel.grid.minor.x\npanel.grid.minor.y\n\nIf you don’t specify x or y or major or minor, your options apply to all.\nSetting any of these to element_blank() removes them. Try a few different grid options. How can you remove the minor gridlines and the vertical gridlines?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_path(percentage ~ year, color = ~faculty_type,\n        data = university_teachers) |&gt;\n  gf_theme(panel.grid.minor = element_blank(),\n           panel.grid.major.x = element_blank())",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#custom-legend-title-o",
    "href": "graphics-code-plots.html#custom-legend-title-o",
    "title": "18  Code Examples",
    "section": "18.36 Custom Legend Title (o)",
    "text": "18.36 Custom Legend Title (o)\nSometimes you might want to remove the legend title, or replace it with a more readable one. You can do it with gf_theme(). Try the plot below with and without the gf_theme() line to see how it changes. Replace \"Activity\" with \"\" (empty quotes) to remove the legend title entirely.\n\n\n\n\n\n\n\n\nNotice, we used the function scale_fill_discrete() to adjust the legend because we had a graph with a fill color tied to a discrete (categorical) variable, PhysActive. We would replace the function scale_fill_discrete() with another depending on variable type and whether our plot call includes color = ~variable or fill = ~variable:\n\nfill by categorical variable: scale_fill_discrete()\nfill by quantitative variable: scale_fill_continouous()\ncolor by categorical variable: scale_color_discrete()\ncolor by quantitative variable: scale_color_continuous()\n\nThere are more…type ?scale_ and let autofill show you all the options!",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#remove-legend",
    "href": "graphics-code-plots.html#remove-legend",
    "title": "18  Code Examples",
    "section": "18.37 (Re)Move Legend",
    "text": "18.37 (Re)Move Legend\nTo remove the legend entirely (make sure you really want to do this!) you chain from your plot layer to gf_theme() with input legend.position = 'none'.\n\n\n\n\n\n\n\n\nTo change the legend’s location, set legend.position to left, right, top, or bottom.",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#adjust-figure-size",
    "href": "graphics-code-plots.html#adjust-figure-size",
    "title": "18  Code Examples",
    "section": "18.38 Adjust Figure Size",
    "text": "18.38 Adjust Figure Size\nYou will almost certainly want to adjust figure sizes in your own RMarkdown documents. There are several ways - you can set a file-wide default in the header of the Rmd file as is done in some class files - or you can set the figure size for one R code chunk in the settings in the first lines of the chunk, as shown below.\nThe values of fig-width and fig-height are expected to be given in inches, by default.\nYou can also put these settings inside the {}:",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "graphics-code-plots.html#more",
    "href": "graphics-code-plots.html#more",
    "title": "18  Code Examples",
    "section": "18.39 More?",
    "text": "18.39 More?\nWhew! That was a lot. You made it!!\nRemember, you don’t have to store all this information in your head now (although that will come with practice) - you will not need to make graphs without being able to access reference information (like this section).\nThe organization of these examples may not be perfect but please keep in mind…they are searchable!",
    "crumbs": [
      "Data Visualization with ggformula",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Code Examples</span>"
    ]
  },
  {
    "objectID": "design-intro.html",
    "href": "design-intro.html",
    "title": "Study Design & Model Planning",
    "section": "",
    "text": "Preface: What’s a Model?\nWe will spend most of this course learning to fit different kinds of regression models. What are they?\nWe will learn as we go. But as a starting point…\nEssentially, we are looking to quantify relationships between our response and predictor(s) – accompanied by appropriate measures of the uncertainty of our estimates.",
    "crumbs": [
      "Study Design & Model Planning"
    ]
  },
  {
    "objectID": "design-intro.html#preface-whats-a-model",
    "href": "design-intro.html#preface-whats-a-model",
    "title": "Study Design & Model Planning",
    "section": "",
    "text": "In a regression model, we have one main response or outcome variable of interest\nWe want to assess whether our response variable is or isn’t associated with (some of) a suite of possible predictor variables (also sometimes known as covariates but we’ll avoid the word “covariates” because it sometimes means slightly different things in different disciplines).",
    "crumbs": [
      "Study Design & Model Planning"
    ]
  },
  {
    "objectID": "design-intro.html#section-learning-outcomes",
    "href": "design-intro.html#section-learning-outcomes",
    "title": "Study Design & Model Planning",
    "section": "Section Learning Outcomes",
    "text": "Section Learning Outcomes\nThis section, we will learn strategies for planning a regression model - how many variables, and which ones, should be included as predictors? How does an analyst decide? What principles underlie these decisions?\nBy the end of the section you will:\n\nDefine a causal diagram, and the variable types that can be depicted in one\nUse a causal diagram to determine which variables to include as predictors in a regression model\nApply the n/15 rule-of-thumb in model planning, to determine how many coefficients can be reliably estimated with a given dataset\nCombine a causal diagram, the n/15 rule, and a modeling goal to articulate a well-reasoned plan for a multiple linear regression model\n\nAlong the way, we will also review some introductory stat material on study design and types of experiments, to make sure we are all have the same vocabulary in place. You won’t be directly assessed on this material and so it is marked as optional.\n\n\n\n\n\n\n\n\n\nComic from xkcd",
    "crumbs": [
      "Study Design & Model Planning"
    ]
  },
  {
    "objectID": "design-intro.html#text-reference",
    "href": "design-intro.html#text-reference",
    "title": "Study Design & Model Planning",
    "section": "Text Reference",
    "text": "Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Ch. 1.4-1.6\nEcological Models & Data in R Ch. 9\nCourse Notes Chapter 1\nStatistical Modeling: A Fresh Approach Chapter 18\nSee also: A biologist’s guide to causal inference\n\nIt’s suggested that you read these chapters after reviewing this section, with particular focus on the topics you found most challenging.",
    "crumbs": [
      "Study Design & Model Planning"
    ]
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "19  Sample and Study Design",
    "section": "",
    "text": "19.1 Sampling Strategies\nBefore you can start modeling, you need data.\nIn our course, we focus on analyzing data (not collecting it), and we use datasets that have already been collected by others.\nBut to do analysis well, you need to consider the metadata – for example, who collected your data, and why? How was it obtained exactly? To know how these things might influence our analysis and conclusions, we might need a quick review of sampling strategies and study design.\nAlmost every dataset contains information on a sample of cases from a larger population.\nAnd if the sample is biased – that is, not representative of the full population in important ways – then we won’t be able to make any valid inferences from it. So the process used to choose a sample is crucial to the validity of all results! What are ways to choose a sample?",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sample and Study Design</span>"
    ]
  },
  {
    "objectID": "design.html#sampling-infographic",
    "href": "design.html#sampling-infographic",
    "title": "19  Sample and Study Design",
    "section": "19.2 Sampling Infographic",
    "text": "19.2 Sampling Infographic\nHere’s an infographic version of the main ideas here, in infographic form. This infographic includes a couple additional sampling approaches that are used in human health studies, and also introduces one more bit of terminology: probability vs. non-probability sampling. In probability methods, there is some element of random selection that is used; in non-probability samping, there is not.\nYou can also view the infographic on the web.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sample and Study Design</span>"
    ]
  },
  {
    "objectID": "design.html#study-design",
    "href": "design.html#study-design",
    "title": "19  Sample and Study Design",
    "section": "19.3 Study Design",
    "text": "19.3 Study Design\nA crucial distinction is between two broad classes of research studies: observational studies and experimental studies.\nSide note: many students in the sciences find that the statistical definition of an “experimental study” doesn’t mesh perfectly with the definition of an “experiment” that they learn in science courses. If it helps, you can think of “experiment” keeping the colloquial meaning it has in your field, and think of “experimental study” as a technical statistical descriptor of a study (as we will learn it today).\nYou probably have prior knowledge or a good intuitive guess as to the difference between studies that are observational and experimental. We’ll review key concepts more later on, but to start, take a chance to assess your existing hunches. Study the infographic below with this distinction in mind: Does the infographic confirm, modify, or add nuance to your initial idea of how “observational” and “experimental” studies differ? As a bonus, the image also details a few kinds of studies within each broad class, and orders them in terms of the strength of evidence they can provide.\n\n\n\n\n\n\n\n\n\n\nAn Alternative\nYou may spend less time with this one - but here’s an alternative infographic covering much of the same information. It’s organized differently, but consistent - dive in if you’d like to review again!\n\n\n\n\n\n\n\n\n\nOne thing to notice in this version is the emphasis on randomized experimental studies like clinical trials as a source of possible causal conclusions – the NIH notes that a randomized experiment has the best chance of allowing researchers to conclude that one thing causes another, while that is difficult to impossible in observational work. Why?\n\n\nWhy Randomization?\nRandomization is a key tactic for ensuring the results of an experimental study are reliable, and providing a strong basis for concluding that a particular treatment or intervention causes a certain outcome or result.\nRandomization means participants are assigned randomly to different “treatments;” they are randomly chosen to receive different assigned values of the key variable(s) that researchers are controlling or manipulating. If this procedure is not followed, then the different treatment groups may differ systematically in some important feature (like gender, motivation, initial health status - anything that might influence the outcome). Here’s one more video that explains the issue quite clearly, in the context of human randomized controlled trials:\n\n\nYou can also watch it outside the tutorial.\n\n\nRandom SAMPLE vs RANDOMIZATION\nCareful! Many students confuse random sampling with randomization, or use one term when they mean the other. Check your understanding: which is which?\n\n\nWhat is the difference between random sampling and randomization?\n\n Randomization has to do with how cases to study are randomly chosen from the population, while random sampling has to do with how already-chosen participants are randomly assigned to experimental treatment groups. Random sampling has to do with how cases to study are randomly chosen from the population, while randomization has to do with how already-chosen participants are randomly assigned to experimental treatment groups. It depends on the context; they are very similar\n\n\n\n\n\nClick for explanations of solution above.\n\nThe words seem similar, but are distinct technical terms!\nRandomization has to do with how values of a “treatment” variable of special interest are assigned: randomly!\nRandom sampling means that the individuals (whether they are people, places, or things…) in the study were selected from the population at random.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sample and Study Design</span>"
    ]
  },
  {
    "objectID": "design.html#observational-vs-experimental-studies",
    "href": "design.html#observational-vs-experimental-studies",
    "title": "19  Sample and Study Design",
    "section": "19.4 Observational vs Experimental Studies",
    "text": "19.4 Observational vs Experimental Studies\nWe mentioned earlier that the key distinction for us will be between observational and experimental studies. Hopefully you’ve already started to form a mental map of the differences. Here’s one of the most clear, concise, complete statements of the main ideas I’ve ever heard:\n\n\nYou can also watch outside this tutorial.\nAnd now, a one-minute review of the basics:\n\n\nYou can also watch outside this tutorial.\nBy now, you should be able to provide a short definition of each study type and note key differences between observational and experimental studies.\n\nEven more\nIf you would like to know more about some of the other study types featured in the infographics but not discussed in detail so far, you can check out the optional video below:\n\n\nYou can also watch outside this tutorial.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Sample and Study Design</span>"
    ]
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "20  Model Planning for Inference",
    "section": "",
    "text": "20.1 Motivation\nJust as we imagine before we start coding to create graphics, we ought to think before we start fitting models.\nTraditional ways of interpreting statistical results are premised on the idea that you made a plan, got some data, fitted the model you planned, and want to draw conclusions.\nIf, instead, you got data, scrutinized the data, fitted lots of different models, and now want to report results from the one that fitted best…well, generally things tend to go wrong. This is especially true if you use the data to lead you from a more complex to a simpler model. As Harrell (2015) points out in section 4.3, the problems are huge:\nHow can we avoid these problems? Some more insight will come when we consider model assessment and selection in future sections. For now, we need to remember:\nAnd remember, this is al if inference is the goal of modeling - things are quite different if you are doing predictive modeling.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#motivation",
    "href": "plan.html#motivation",
    "title": "20  Model Planning for Inference",
    "section": "",
    "text": "Uncertainty underestimated (overconfidence: standard errors and confidence intervals too small, \\(R^2\\) too big, unfounded confidence that associations are real when they may not be).\nSpurious relationships look important and slope estimates are biased high\nIf testing hypotheses, p-values too small\n\n\n\nFitting and interpreting one well-considered, sensible model is prefereable to trying many things and then trying to choose among them later.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#response-and-predictors",
    "href": "plan.html#response-and-predictors",
    "title": "20  Model Planning for Inference",
    "section": "20.2 Response and Predictors",
    "text": "20.2 Response and Predictors\nA regression model is our attempt to quantify how a response variable of interest changes when a set of predictor variables change.\nSo, to begin, we need to identify our (one) response variable – the thing we are most interested in measuring or predicting or describing or understanding.\nThen, we need to identify a set of predictor variables that we expect to be associated with changes in the response. (If we are planning an experiment, they should be variables we can collect data on; if working with data already collected, they must be in or derived from the data available.)\nHow do we choose which predictors to include, and how many?",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#expertise",
    "href": "plan.html#expertise",
    "title": "20  Model Planning for Inference",
    "section": "20.3 Expertise",
    "text": "20.3 Expertise\nFirst, rely on experts and previous experience. If you know the context of the problem well, you have a good sense of the predictors that will be of interest. If you don’t, then you should consult experts (or published work on the topic).\nThere are also practical limits on the number of predictors you can reasonably consider, given a dataset.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#dataset-size-n15-rule",
    "href": "plan.html#dataset-size-n15-rule",
    "title": "20  Model Planning for Inference",
    "section": "20.4 Dataset Size (n/15 rule)",
    "text": "20.4 Dataset Size (n/15 rule)\nOne important consideration, when planning a regression model, is: How many predictors can I reasonably include?\nIt depends on the size of the dataset: it takes several observations to get a good estimate of any statistics, so it makes sense that fitting a model with lots of predictors will require a bigger dataset. Each additional observation may add a little bit more capacity for fitting a more complex model.\nAnd if you try to fit too many, the chances of overfitting increase. Overfitting is when you model noise as well as signal, capturing in your model apparent relationships that actually exist only in the current dataset, not in reality.\nFor linear regression, Harrell (2015, Chapter 4.6) offers a rule of thumb: the number of parameters being estimated, \\(p\\), should be less than \\(\\frac{n}{10}\\) or \\(\\frac{n}{20}\\). To give just one standard rule of thumb, we should aim for \\(p &lt; \\frac{n}{15}\\). \\(n\\) is the sample size (number of rows in the dataset).\nThis “n/15 rule” is a very rough rule of thumb - sometimes it seems you can get away with estimating a few more parameters than it says, and sometimes fewer (particularly in the case of categorical predictors where the observations are not evenly distributed across combinations of categories). But it gives us a reality check and a starting point for planning.\nRemember, n/15 is a ceiling – and upper limit on the number of parameters you could estimate. It’s not a goal! You may have less.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#causation-revisited",
    "href": "plan.html#causation-revisited",
    "title": "20  Model Planning for Inference",
    "section": "20.5 Causation Revisited",
    "text": "20.5 Causation Revisited\nIn most intro stat courses, students learn to repeat statements like: “Correlation doesn’t imply causation, and only randomized experimental studies can draw causal conclusions.”\nWell…kind of.\nIn data science, big observational datasets (collected in the absense of a structured study design) are really common. There are also many scenarios of interest where randomized experiments are just not possible on practical and ethical grounds.\nSome blatant examples: experiments in which people were randomly assigned to dislocate their shoulders to investigate factors influencing recovery, or start smoking to see if they get cancer, or expose themselves to someone with a contagious disease to see if they become ill. In many situations, observational work is the only real option. What’s a researcher to do?\nIn recent decades the field of causal inference has made great strides in thinking intelligently about how best to make the most reliable conclusions possible about cause and effect, when observational data is all you have. To begin to understand, we have to define a few terms: direct causation vs. indirect causation, and three alternative situations: confounders, mediators, and colliders. This field is new and technical, but the next video is about the most concise and clear primer I know of (with concrete examples).\n\n\nYou may also watch it outside this tutorial.\n\nMediators, Moderators, Precision Covariates\nOK, wait a sec. The video didn’t cover mediators! A mediator is another term for what the video called an “indirect cause” – a link in the middle of a causal chain. The mediator explains or is part of the process by which an upstream cause leads to an effect.\nThere are two other variable types it may be useful to name, too:\n\nPrecision covariates affect the response variable of interest, without having any causal links at all to the “main” predictor of interest. We include them in models when we can, to get more precise estimates.\nModerators interact with the main predictor of interest. The cause-effect relationship between the main predictor and the response varies depending on the value of the moderator variable. (Moderators might additionally affect the predictor and/or the response directly, so they may share some features with precision covariates and confounders.)",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#whats-a-causal-diagram",
    "href": "plan.html#whats-a-causal-diagram",
    "title": "20  Model Planning for Inference",
    "section": "20.6 What’s a Causal Diagram?",
    "text": "20.6 What’s a Causal Diagram?\nA causal diagram is a picture mapping the causal relationships between key variables. They are used when researchers are interested in quantifying causal relationships between variables – not just, “is X associated with a change in Y?” but “does X cause a change in Y?”\nTo make causal conclusions with confidence outside the context of a randomized, controlled experimental study is a big challenge, and mapping out starting assumptions about relationships between variables is just the first part.\nEven if we are not necessarily trying to make causal conclusions (we won’t be here, with observational data), when you model relationships of several variables, a causal diagram helps you make smart, thoughtful choices about the ones you include in your model and the ones you leave out.\nThe diagram surfaces your starting assumptions about relationships between variables. It makes your assumptions transparent (to others and to you!) and guides choices about what to include/exclude from a model.\nPeople also call causal diagrams DAGs (for Directed Acyclic Graphs) since they are a specific application of that [broader] type of mathematical graph. So, every causal diagram is a DAG, but in math there can be DAGs that are not statistical causal diagrams.\nIn a causal diagram each box is a variable, and arrows connect causes to effects (with the arrowhead pointing from the cause to the effect).",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#diagrams-in-r",
    "href": "plan.html#diagrams-in-r",
    "title": "20  Model Planning for Inference",
    "section": "20.7 Diagrams in R?",
    "text": "20.7 Diagrams in R?\nA short note about drawing causal diagrams in R. The tools to do this are rapidly evolving, and for now, many of them can be a bit frustrating and idiosyncratic.\nTo keep things simple, an option is to just draw your diagrams by hand, and take a photo of them (if on paper) or save the image file (if electronic). If using a server, you’ll need to upload the image, but then you can pretty easily include your image in a Quarto file.\nIf you’d like to create your diagrams in R, you might want to consider using mermaid or graphviz diagrams, which Quarto supports.\nAnother somewhat popular option is the R package dagitty, although take care (its diagrams are not quite as pretty or customizable, and it sometimes throws inscrutable errors because of buried conflicts with other packages we use).\nYou are welcome to explore these options if you’re motivated, but hand-drawn diagrams are fine for our course – for us, the point is not making camera-ready diagrams, but using diagrams to plan our models well.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#variable-types",
    "href": "plan.html#variable-types",
    "title": "20  Model Planning for Inference",
    "section": "20.8 Variable Types",
    "text": "20.8 Variable Types\nIn a multi-variable analysis, there is often one key variable of interest (measuring “response” or “outcome” or “effect”) and another one which may influence it, and is the focus of greatest research interest (a key “predictor” or “cause”).\nBut there are generally other variables in the mix that are somehow relevant to understanding the predictor – response relationship. How can we classify (and diagram) them?\nIn all examples below, X is the predictor of greatest interest and Y is the response. (A is the other variable.) The definitions below all depend on a specific X and Y having been chosen: the other variable types are defined relative to the X-Y relationship.\n\nPrecision Covariate\nHere’s an example of A acting as a precision covariate. Precision covariates are also known as competing exposures.\n\n\n\n\n\n\n\n\n\nBased on the diagram, can you explain in words what a precision covariate is?\n\n\nClick for one definition…\n\nA precision covariate affects only the response variable.\n\nBest practice is to include precision covariates in a model trying to measure the size of the effect of X on Y, to get the most precise estimates possible…but if you don’t, it won’t bias your estimate of the size of X’s influence on Y. Can you explain why?\n\n\nMediator\nHere are two (slightly different) examples of A acting as a mediator. Based on the diagram (and the code), explain in words what a mediator is.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the diagrams, can you explain in words what a mediator is?\n\n\nClick for one definition…\n\nA mediator is part of a chain of causes and effects, between the main predictor of interest and the response.\n(You may also see mediation chains elsewhere in a diagram/scenario…for example, a chain of three causes/effects that together act as a precision covariate.)\n\nHow is this different from a precision covariate?\n\n\nHmm, not sure – show me how it’s different!\n\nA mediator is part of a chain of causes and effects, between the main predictor of interest and the response.\nIt affects the response, but unlike a precision covariate, it’s also affected by the predictor of interest.\n\nIf you have a situation like the first picture above, where there are no branches/only one single path from your predictor to the response, include only your predictor and that’s it.\nBUT if there are mediator(s) and if there are branches in the path(s), like the second picture above that’s when you have options.\nIf you want to estimate the total effect of the upstream predictor of interest, exclude the mediator(s). If you want to distinguish and quantify effects along several pathways (via the mediator in question, and another way), then include one variable along each branch whose influence you want to measure.\nBoth can be valid. Researchers have to choose which they want to do.\n\n\nConfounder\nBelow is another causal diagram, in which A is a confounder.\n\n\n\n\n\n\n\n\n\nCan you define “confounder” in words?\n\n\nClick for one definition…\n\nA confounder affects both the predictor and the response.\n\nIf there is a confounder present and you do not include it in your model, then you may wrongly conclude X causes Y.\nSo best practice is to include all confounders in a model.\n\n\nCollider\nCollider bias caused by collider variable A looks like:\n\n\n\n\n\n\n\n\n\nA collider is affected by both the predictor and response. (Their influence arrows “collide” at the collider…)\nIncluding a collider in a model designed to measure association between X and Y can induce association where there is actually none so best practice is to make sure you do not include colliders in models.\n\n\nModerator\nA variable that moderates (increases or decreases the size of) the causal link between two other variables is a Moderator.\n(If you have studied interactions before in the context of regression models, a cause and a moderator interact to influence the effect.)\nUnfortunately in our diagrams there is not really an adequate way to draw moderators – for good reasons – but here’s a diagram (from Yoon 2020) to help get a sense of the situation:\n\n\n\n\n\n\n\n\n\nIf you think you have a moderator variable, best practice is to include it in your model interacting with X. (We will return to interactions in much more detail later on.)\n\n\nMore\nThere are more details and more complicated scenarios, of course. This is all you need to master to start with. If you want to explore more, check out https://cran.r-project.org/web/packages/ggdag/vignettes/bias-structures.html",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#more-reference-drill",
    "href": "plan.html#more-reference-drill",
    "title": "20  Model Planning for Inference",
    "section": "20.9 More Reference & Drill",
    "text": "20.9 More Reference & Drill\nCheck out the materials and interactive questions at: http://dagitty.net/learn/graphs/roles.html",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "plan.html#summary-causal-diagrams",
    "href": "plan.html#summary-causal-diagrams",
    "title": "20  Model Planning for Inference",
    "section": "20.10 Summary: Causal Diagrams",
    "text": "20.10 Summary: Causal Diagrams\nSoooo….what? How will this affect our work? Well, we will often be interested in the association or relationship between two key variables, a potential cause or “predictor” x and a potential effect or “response” y. Unless we have data from a randomized experimental study, we need to be clever about which other variables to include in our analysis to get the best, most accurate estimate of the x - y relationship:\n\nIf there are any confounders we should include them in our models and analysis. This is often called controlling for the confounders’ effects.\nIf there is a mediator, you may have a choice.\n\nIf there are no branches/only one single path from your predictor to the response, include only your predictor and that’s it.\nIf there are branches in the path, that’s when you have options. If you want to estimate the total effect of the upstream predictor of interest, exclude the mediator(s). If you want to distinguish and quantify effects along several pathways (via the mediator in question, and another way), then include one variable along each branch whose influence you want to measure.\n\nIf there are any colliders they should not be included in our models and analysis - including them would actually reduce the accuracy of our assessment of the x - y relationship\n\nSeems simple enough, right? There’s a catch. To decide whether something might be a collider or a confounder, you have to rely on your knowledge or belief about what causes what. Even experts don’t always agree. There is not usually a definite right answer that is easy to agree on. This is one thing that makes causal inference with observational data so hard, and contentious!\nBut closing your eyes to the issue and skipping the step of making a causal diagram doesn’t solve anything at all. Whatever model you fit is consistent with some causal diagrams, and not others. So by drawing the diagram as best you can, and designing your model accordingly, you are being transparent and explicit about your assumptions instead of keeping yourself and your audience ignorant that those assumptions exist, and have consequences.\nMy personal advice is: if in doubt, and if the data allow it, control for all the potentially important variables you can. The exception, of course, if is you are sure or quite sure the variable is a collider - in that case you definitely MUST exclude it.",
    "crumbs": [
      "Study Design & Model Planning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Planning for Inference</span>"
    ]
  },
  {
    "objectID": "lm-fit.html",
    "href": "lm-fit.html",
    "title": "21  Fitting Linear Models in R",
    "section": "",
    "text": "21.1 Learning Outcomes\nThis section, we’ve already considered strategies for planning a regression model - how many variables, and which ones, should be included as predictors? How does an analyst decide?\nNow, here, we will also learn to fit the models in R and examine the results.\nBy the end of the section you will:",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#learning-outcomes",
    "href": "lm-fit.html#learning-outcomes",
    "title": "21  Fitting Linear Models in R",
    "section": "",
    "text": "Fit a multiple linear regression to a dataset in R, using multiple quantitative and/or categorical predictors\nState the equation of a linear regression model, based on a model description or an R model summary",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#text-reference",
    "href": "lm-fit.html#text-reference",
    "title": "21  Fitting Linear Models in R",
    "section": "21.2 Text Reference",
    "text": "21.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 1.4-1.6\nCourse Notes Chapter 1\nIntro to Modern Statistics Chapter 3*\nEcological Models & Data in R Chapter 9\nStatistical Modeling: A Fresh Approach Chapters 6-8",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#quick-review-of-lines",
    "href": "lm-fit.html#quick-review-of-lines",
    "title": "21  Fitting Linear Models in R",
    "section": "21.3 Quick review of lines",
    "text": "21.3 Quick review of lines\nConsider the line below.\n\n\n\n\n\n\n\n\n\nYou may remember that lines have equations that look like\n\\[y = mx + b\\]\nwhere \\(m\\) is the slope and \\(b\\) is the \\(y\\)-intercept (the value of \\(y\\) when \\(x = 0\\)). So we can read off an equation for our line by figuring out the slope and the intercept.\n\nIntercept and Slope\nWhen \\(x = 0\\) on the line we just looked at, \\(y = 2\\), so the intercept is \\(2\\). It’s indicated by the red dot below:\n\n\n\n\n\n\n\n\n\nThe slope is computed as “rise over run”. We can compute this by comparing the red and blue dots and doing a little arithmetic.\n\n\n\n\n\n\n\n\nAny two points on the line give the same slope. Chose two different points on the line and recompute the rise and the run. You should get the same slope.\nSo the equation for our line is\n\\[y = 4x + 2\\]\nStatisticians like to write this a different way (with the intercept first and with different letters):\n\\[ y = \\beta_0 + \\beta_1 x\\]\n\\[y = 2 + 4x\\]\nSo \\(\\beta_0\\) (read “beta zero”) is the intercept and \\(\\beta_1\\) (read “beta one”) is the slope.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#simple-linear-regression-equation",
    "href": "lm-fit.html#simple-linear-regression-equation",
    "title": "21  Fitting Linear Models in R",
    "section": "21.4 Simple Linear Regression Equation",
    "text": "21.4 Simple Linear Regression Equation\nIn your intro stat course (whenever that was!) you likely learned to fit simple linear regression models, and hopefully also to check the conditions that must hold for such models to give reliable results. (If you didn’t learn that condition-checking, known as model assessment, don’t worry - stay tuned for next week!) Some of the material early in our course may be review for some, but we will build on it rapidly too.\nHere, we start with a reminder of the form of the linear regression equation, which is a small elaboration of the equation of a line we just saw:\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon, \\epsilon \\sim \\text{Norm}(0, \\sigma)\\]\n\n\\(y\\) is the response variable\n\\(x\\) is the predictor (or explanatory) variable\n\\(\\beta_0\\) is the intercept. To fit the model to a specific dataset, we have to estimate the numeric value of this parameter.\n\\(\\beta_1\\) is the slope. To fit the model to a specific dataset, we have to estimate the numeric value of this parameter.\n\\(\\epsilon\\) are the residuals (or errors).\n\nThey measure the vertical distance between an observed data point (\\(y_i\\) is the \\(i\\)th one) and the predicted (or fitted) value on the line for the same predictor variable value (\\(\\hat{y}_i\\)). Much more about residuals later one!\nResiduals have a normal distribution with mean 0 and standard deviation \\(\\sigma\\) (some value that we can estimate once we have the slope and intercept: we compute the fitted values \\(\\beta_0 + \\beta_1x\\) and subtract from the observed response variable values to get the residuals, then estimate their standard deviation \\(\\sigma\\).)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#multiple-predictors",
    "href": "lm-fit.html#multiple-predictors",
    "title": "21  Fitting Linear Models in R",
    "section": "21.5 Multiple Predictors",
    "text": "21.5 Multiple Predictors\nOften, statisticians consider more than one \\(x\\) variable. Why? Well, most response variables we are interested are associated with not just one but many different variables – in the language we just learned, many variables could be potential causes, confounders, mediators, or moderators (but not colliders…because we would not put those in the model).\nWith multiple predictors, the linear regression equation looks more like\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots \\beta_p x_p + \\varepsilon\\] \\[\\varepsilon \\sim \\text{Norm}(0, \\sigma)\\]\nwhere\n\n\\(\\mathbf{Y}\\) is the response variable value,\nthe \\(\\beta_0\\) is the intercept,\n\\(\\beta_1, \\dots \\beta_p\\) are the \\(p\\) “slopes”,\n\\(x\\)s are the \\(p\\) predictor variable values, and\n\\(\\varepsilon\\) are the “errors” (since real data doesn’t all fall exactly on a line).\n\nWhen we use data to estimate the \\(\\beta_i\\)’s, we may write it this way:\n\\[ \\hat Y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots \\hat{\\beta}_p x_p\\]\n\nThe hats on top of the the \\(\\beta\\)s and \\(Y\\) indicate that they are being estimated from data (they are statistics or estimates rather than parameters now!)\nNote that the error term is gone now: we know exactly what our estimate \\(\\hat Y\\) is, with no error.\n\nBut \\(\\hat Y\\) isn’t (usually) identical to \\(Y\\). The difference is our friend the residual:\n\n\\[\n\\begin{align*}\n\\mbox{residual} = \\hat\\varepsilon & = Y - \\hat Y \\\\\n& = \\mbox{observed reponse} - \\mbox{model response}\n\\end{align*}\n\\]\nAgain, the hats indicate that we are making estimates using data (hats basically always mean this in statistics). Those “beta hat” and “Y hat” values come from our fitted model. The \\(\\hat \\beta\\)s are called the coefficients (or, more precisely, the estimated coefficients) or the “beta hats”.\n\nYou will sometimes encounter slightly different notation for linear models. Two common things are:\n\nThe use of \\(b_0, b_1, \\dots, b_p\\) instead of \\(\\hat \\beta_0, \\hat \\beta_1, \\dots \\hat \\beta_p\\).\nDropping of hats. In particular, the hat is often left off of the response variable when the model is presented with the numerical coefficient estimates plugged in.\n\nAnd the models themselves go by different names: regression model, least squares model, least squares regression model, etc.\nDon’t let these little differences distract you. If you focus on the important concepts, you should be able to navigate the notation, whichever way someone does it.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#matrix-formulation",
    "href": "lm-fit.html#matrix-formulation",
    "title": "21  Fitting Linear Models in R",
    "section": "21.6 Matrix Formulation",
    "text": "21.6 Matrix Formulation\nThe form we just saw for the simple linear regression equation can be pretty simply and intuitively extended to the case of more than one predictor:\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots \\beta_p x_p + \\epsilon\\]\nwhere \\(\\mathbf{Y}\\) are the response variable values, the \\(\\beta\\)s are the \\(p + 1\\) intercept and slopes, \\(x\\)s are the \\(p\\) predictor variable values, and \\(\\epsilon\\) are the errors or residuals.\n\nA problem\nBut…this form of the regression equation doesn’t keep track of the individual data points. The \\(\\mathbf{Y}\\) up there (capitalized because it’s a random variable) is a vector, not a scalar – it’s a list of \\(n\\) response variable values. Same for the data. So maybe we could say something like:\n\\[\\mathbf{Y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots \\beta_p x_{ip} + e_i \\]\nbut…OH, those double subscripts are already getting cumbersome!\n\n\nA solution\nWe can fix that by writing our regression equation as a matrix equation. Let \\(\\mathbf{i} = &lt;1, 2, 3, \\dots n&gt;\\) be the indices identifying each observation in the data, and \\(\\mathbf{j} = &lt;0, 1, 2, 3, \\dots p&gt;\\) the indices identifying the different (intercept and) slope values.\nNow let \\(\\mathbf{Y}\\) be a length \\(n\\) column vector of response variable values, and \\(\\mathbf{\\beta}\\) a length \\(p\\) column vector of parameter values.\n\\(\\mathbf{X}\\) is a \\(n\\) by \\((p+1)\\) matrix, whose first column is a length \\(n\\) vector of ones, and whose other columns are length \\(n\\) vectors of values of the \\(jth\\) predictor variable. \\(\\mathbf{X}\\) is often called the model matrix.\nWe then have the equation:\n\\[ \\mathbf{Y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\]\n(the same as before, but using vectors and a matrix to avoid all those pesky subscripts).\n\n\nComprehension Check: Model Matrix Size\n\n\nConsider a regression model with 4 predictor variables being fitted to a dataset with 46 observations. What will be the size of the model matrix X?\n\n 4 x 47 46 x 4 46 x 3 4 x 46 46 x 5\n\n\n\n\n\nClick for explanations of solution above.\n\nIf you said…\n\n46 x 3, Think about what each column represents.\n46 x 4, Do not forget the intercept!\n4 x 46 or 4 x 47, It is one row per datapoint and one column per beta…",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#toy-dataset-pigeons",
    "href": "lm-fit.html#toy-dataset-pigeons",
    "title": "21  Fitting Linear Models in R",
    "section": "21.7 Toy Dataset: Pigeons",
    "text": "21.7 Toy Dataset: Pigeons\nFor the next few examples, we’ll consider a very small dataset on racing pigeons from Michigan. Their unirate scores measure their racing performance (smaller is better).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnirate\nOwnerName\nSex\nAge\n\n\n\n\n2.864\nKENNIER PALMIRA\nH\n1.9\n\n\n2.965\nTOM RICHARDS\nH\n1.5\n\n\n4.304\nKENNIER PALMIRA\nH\n2.3\n\n\n5.027\nTOM RICHARDS\nC\n2.2\n\n\n9.189\nTOM KUIPER\nC\n2.8",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#fitting-a-normal-distribution",
    "href": "lm-fit.html#fitting-a-normal-distribution",
    "title": "21  Fitting Linear Models in R",
    "section": "21.8 Fitting a normal distribution",
    "text": "21.8 Fitting a normal distribution\nLet’s try to reformulate a familiar exercise – fitting a normal distribution to a dataset – as a linear regression problem.\nWe might want to consider a model where the pigeon Unirate scores are \\(iid\\) (independent, identically distributed) samples from a \\(N(\\mu, \\sigma)\\) distribution.\n\n\nHow many predictors will our pigeon normal-distribution model have?\n\n Two 4 x 46 One 4 x 47 None\n\n\n\n\n\nClick for explanations of solution above.\n\nWhat predictor(s) does the unirate score depend on in this model? Well…none, in fact!\n\n\nA no-predictor model\nIt can make sense to think of the residuals \\(\\mathbf{\\epsilon}\\) of a linear regression having a \\(N(0, \\sigma)\\) distribution – they are normally distributed, and the model is right on average, but not spot on for every data observation. This gives us the “intercept-only” regression model:\n\\[ \\mathbf{Y} = \\mu \\mathbf{1} + \\epsilon,\\]\n\\[\\epsilon \\stackrel{iid}{\\sim} N(0, \\sigma)\\]\nHere \\(\\mathbf{X}\\) is just a column vector of ones, since there are no predictors, and the intercept \\(\\beta_0\\) is \\(\\mu\\), the mean Unirate score.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#one-predictor",
    "href": "lm-fit.html#one-predictor",
    "title": "21  Fitting Linear Models in R",
    "section": "21.9 One Predictor",
    "text": "21.9 One Predictor\nOf course, we can also fit models with one or more predictor variables.\nOne model we could fit might have the equation:\n\\[\n\\text{Unirate} = -5.4 + 4.8\\text{Age},\n\\]\n\\[\n\\epsilon \\sim N(0, 1.3)\n\\]\nA model matrix for this model is shown below.\n\\[\\begin{bmatrix}\n1 & 1.9 \\\\\n1 & 1.5 \\\\\n1 & 2.3 \\\\\n1 & 2.2 \\\\\n1 & 2.8 \\\\\n\\end{bmatrix}\\]\nfor the pigeon data:\n\n\n\n\n\n\n\n\n\n\n\nUnirate\nOwnerName\nSex\nAge\n\n\n\n\n2.864\nKENNIER PALMIRA\nH\n1.9\n\n\n2.965\nTOM RICHARDS\nH\n1.5\n\n\n4.304\nKENNIER PALMIRA\nH\n2.3\n\n\n5.027\nTOM RICHARDS\nC\n2.2\n\n\n9.189\nTOM KUIPER\nC\n2.8\n\n\n\n\n\n\n\nWhat is the a verbal description of regression model corresponding to the equation above?\n\n A regression in which age predicts unirate score A different model with no predictors, but Gamma distributed errors A regression where the response variable is the pigeon age instead of unirate score\n\n\n\n\n\nClick for a hint…\n\nThink: which variable(s) are included in the equation?",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#categorical-predictors-2-categories",
    "href": "lm-fit.html#categorical-predictors-2-categories",
    "title": "21  Fitting Linear Models in R",
    "section": "21.10 Categorical Predictors (2 categories)",
    "text": "21.10 Categorical Predictors (2 categories)\nLet’s consider another model: one where both Age and Sex (“H” for hen and “C” for cock) predict Unirate score.\nHow can we formulate a model matrix now?\nTake a moment to brainstorm before you reveal the answer. A hint (especially for data science or computer science types): some people call this trick “one hot encoding.”\n\nAnother hint\nAnother hint, if you haven’t gotten it yet: Think of a way to encode the information from the Sex variable numerically by changing it from a categorical variable to a logical one (zeros and ones).\n\n\nThe solution\nThe idea is to convert our categorical variable into an indicator variable that is 1 if an observation fits in a certain category, and 0 otherwise (i.e., 0 if the observation is in the other category).\n\n\n\n\n\n\n\n\n\n\n\nUnirate\nOwnerName\nSex\nAge\n\n\n\n\n2.864\nKENNIER PALMIRA\nH\n1.9\n\n\n2.965\nTOM RICHARDS\nH\n1.5\n\n\n4.304\nKENNIER PALMIRA\nH\n2.3\n\n\n5.027\nTOM RICHARDS\nC\n2.2\n\n\n9.189\nTOM KUIPER\nC\n2.8\n\n\n\n\n\nIf we did that, our model for Unirate score as a function of Age and Sex would have the model matrix:\n\\[\\begin{bmatrix}\n1 & 1.9 & 1 \\\\\n1 & 1.5 & 1\\\\\n1 & 2.3 & 1\\\\\n1 & 2.2 & 0\\\\\n1 & 2.8 & 0\\\\\n\\end{bmatrix}\\]\n\n\nWhat would be the best name for this new numeric variable indicating sex?\n\n Cock Hen\n\n\n\n\n\nClick for an explanation of the answer above.\n\nSince “H” is encoded as 1, that would be a better name.\nCocks are encoded as 0s, which usually corresponds to “FALSE.” Best to name the variable after the value that 1 (or TRUE) stands for.\n\nWhat will the model equation look like now? Well, to make the equation work, we are representing the categorical Sex variable using a logical indicator variable that is 1 if an observation is in one chosen category, and 0 if it’s the other.\nSo Sex should not appear in our equation!\nInstead, we will use \\(I_{H}\\) (or we could call it \\(I_{hen}\\)), with the capital-I-with-a-subscript being a common way to notate indicator variables. For completeness and clarity, we will also make sure to define that notation as part of our statement of the equation! So we have:\n\\[\\text{Unirate} = -1.8 + 3.6 \\text{Age} - 1.5 I_{H},\\] \\[\n\\text{where } \\epsilon \\sim N(0, 1.3) \\text{ and}\n\\]\nwhere \\(I_{H}\\) is an indicator variable that is 1 if the pigeon is a hen, and 0 if it is a cock.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#categorical-predictors-with-more-categories",
    "href": "lm-fit.html#categorical-predictors-with-more-categories",
    "title": "21  Fitting Linear Models in R",
    "section": "21.11 Categorical Predictors with More Categories",
    "text": "21.11 Categorical Predictors with More Categories\nWhat if we have a categorical predictor with more than two categories?\nNo problem – we just need more than one indicator variables!\n\n\nHow many indicator variables would be needed to construct the model matrix for a regression with one predictor, which is a categorical variable with 5 categories?\n\n 4 3 5\n\n\n\n\n\nClick for an explanation of the answer above.\n\nIf we have indicator variables for 4 of the 5 categories and they are all 0, then the observation must be from the fifth category. Another way of saying this is that one of the categories is “included in the intercept”.\n\nThe category that is included in the intercept value is often called the “base” level.\nWhat would this look like for our pigeons? What if the trainer affected the pigeon’s performance, and we had data on pigeons from three owners with initials “KP”, “TR”, and “TK”?\nBelow are the model parameter estimates (the “\\(\\beta\\)s”). How would you write the model equation?\n\nIntercept \\(\\hat{\\beta_0} = -4.4\\)\nAge: \\(\\hat{\\beta_1} = 3.6\\)\nSex: \\(\\hat{\\beta_2} = 0.46\\)\nOwner: for “TK”, \\(\\hat{\\beta_3} = 3.5\\) and for “TR”, \\(\\hat{\\beta_4} = 1.5\\)\nresidual standard deviation: 1.2\n\n\n\nAfter you’ve made an attempt, click for the solution…\n\n\\[\\text{Unirate} = -4.4 + 3.6 \\text{Age} - 0.46 I_{H} + 3.5 I_{TK} + 1.5 I_{TR},\\] \\[\n\\text{where } \\epsilon \\sim N(0, 1.2) \\text{ and}\n\\]\n\n\\(I_{H}\\) is an indicator variable that is 1 if the pigeon is a hen, and 0 if it is a cock.\n\\(I_{TK}\\) is an indicator variable that is 1 if the pigeon is owned by TK, and 0 otherwise. and \\(I_{TR}\\) is an indicator variable that is 1 if the pigeon is owned by TR, and 0 otherwise. (So a pigeon owned by “KP” would have both \\(I_{TK}\\) and \\(I_{TR}\\) = 0…that’s how we can notate three categories with 3-1 = 2 indicator variables!)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#finding-the-betas",
    "href": "lm-fit.html#finding-the-betas",
    "title": "21  Fitting Linear Models in R",
    "section": "21.12 Finding the Betas",
    "text": "21.12 Finding the Betas\nOK, but how do we go from a model plan and dataset to an actual fitted model? In other words, how do we estimate the \\(\\beta\\)s and find the best-fitting regression model?\nTo find the “best-fitting” regression model for a dataset, we need to first define a metric to measure how “well” a line fits, and then find the \\(\\beta\\)s (intercept and slope(s)) that maximize it. (Actually, we’ll come up with a way to measure the mismatch between a line and the data, and find the \\(\\beta\\)s that minimize it - but it’s the same idea.)\nIn other words, our goal at the moment is to figure out how to estimate the \\(\\beta\\)s.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#least-squares-visually",
    "href": "lm-fit.html#least-squares-visually",
    "title": "21  Fitting Linear Models in R",
    "section": "21.13 Least Squares (visually)",
    "text": "21.13 Least Squares (visually)\nOne idea is to choose the “best-fit” line as the one that minimizes the sum of squared residuals.\nThis method is often called Least Squares Estimation (or Ordinary Least Squares).\nFirst, check out Ordinary Least Squares Estimation (explained visually). Be sure to take advantage of the interactive elements to play around!)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#least-squares-practically",
    "href": "lm-fit.html#least-squares-practically",
    "title": "21  Fitting Linear Models in R",
    "section": "21.14 Least Squares (practically)",
    "text": "21.14 Least Squares (practically)\nNext, try your hand at off-the-cuff least squares estimation. Visit the PhET interactive simulator and:\n\nPick an example from the central upper pull-down menu (or create your own dataset) and:\n\nChoose your best-guess slope and intercept (menu on the right)\nCompare your result with the best-fit line (menu on the left). How close were you? Why/how do you think you went wrong?\nView the residuals, and the squared residuals, for the best-fit line.\nVerify that you understand exactly what the residuals and the SSE = RSE = sum of squared residuals are measuring. In what sense does the minimal-SSE line come “closest” to the data points?\n\nRepeat the exercise for at least one more example.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#least-squares-explained",
    "href": "lm-fit.html#least-squares-explained",
    "title": "21  Fitting Linear Models in R",
    "section": "21.15 Least Squares (explained)",
    "text": "21.15 Least Squares (explained)\nOptionally, if you would like one more presentation of the idea of least squares fitting, watch the (slightly boring but very clear) StatQuest video explanation:\n\n\n(You can also watch directly on YouTube if you prefer.)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#in-r-lm",
    "href": "lm-fit.html#in-r-lm",
    "title": "21  Fitting Linear Models in R",
    "section": "21.16 In R: lm()",
    "text": "21.16 In R: lm()\nSo, now we understand the principle of least squares estimation. But we certainly won’t employ it via guess-and-check to actually fit models to real data!\nIn fact, either via calculus or linear algebra, it’s possible to obtain formulas for the slope and intercept that minimize the SSE for a given dataset. (And, of course, software knows how to use them.)\nThe function we’ll use to fit a linear regression model in R is lm().\nThe first input to lm() (and basically all other R functions that fit regression models) is a model formula of the form:\n \n\n\nlm ( &yy ~ &xx , data = mydata )\n\n\n \nHow do we fill in the empty boxes?\n\nModel Formula: Left Hand Side\nThe left hand side of the formula is simplest: we just need to provide the name of the response variable that we want to model.\n \n\n\nlm (  Y  ~ &xx , data = mydata )\n\n\n \nFor example, if we use dataset MI_lead and our response variable is ELL2012, the skeleton of our formula would look like:\n\nmy_model &lt;- lm( ELL2012 ~ _______, data = MI_lead)\n\nDataset note: The MI_lead dataset stores CDC-provided data on Michigan kids’ blood lead levels, by county, from 2005 and 2012. The dataset is at https://sldr.netlify.app/data/MI_lead.csv and also provides:\n\nCounty name\nProportion kids with elevated blood lead levels, ELL in years 2005 and 2012\nthe Difference in proportion kids with high lead levels between the two years (2012-2005)\nThe proportion of houses in the county that were built before 1950 (and thus more likely have lead paint), PropPre1950\nWhich Peninsula of MI the county is in\n\n\n\nModel Formula: Right Hand Side\nOn the other side of the formula (after the tilde symbol \\(\\sim\\)), we need to specify the name of the predictor variable(s).\nWhile this initial example is a simple linear regression (“simple” means just one predictor), it’s possible to have multiple ones, separated by +.\n\nmy_model &lt;- lm(ELL2012 ~ ELL2005 + Peninsula, \n               data = MI_lead)\nsummary(my_model)\n\n\nCall:\nlm(formula = ELL2012 ~ ELL2005 + Peninsula, data = MI_lead)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0048244 -0.0013235 -0.0006547  0.0012533  0.0072038 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.0012767  0.0003786   3.372  0.00116 ** \nELL2005         0.1576778  0.0311305   5.065 2.67e-06 ***\nPeninsulaUpper -0.0006220  0.0008028  -0.775  0.44083    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.002407 on 78 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2758,    Adjusted R-squared:  0.2573 \nF-statistic: 14.86 on 2 and 78 DF,  p-value: 3.417e-06\n\n\n\n\nPractice\nYour turn: fit a few linear regression models of your own. You can use the MI_lead data, or use one of the other suggested datasets (already read in for you here):\n\nelephantSurvey\ngapminder_clean\nHaDiveParameters\n\nEach time, consider:\n\nHow do you choose a response and a predictor variable?\n\nUsually the response is the main variable of interest, that you would like to predict or understand.\nThe predictor might cause, or be associated with, changes in the response; it might be easier to measure. Or, we might want to test whether it is associated with changes in the response or not.\n\nSee if you can express a scientific question which can be answered by your linear regression model.\n\nFor example, a model with formula ELL2012 ~ ELL2005 would answer, “Does the proportion of kids with elevated lead levels in a county in 2005 predict the proportion in 2012? (Do the same locations have high/low levels in the two years?)”\n\nDon’t forget to consider your understanding of causal relationships between variables of interest in planning your model - consdier drawing a causal diagram to help you decide which predictors should be included.\n\n\n\n\n\n\n\n\n\n\n\nIntercepts\nA (potentially non-zero) intercept is always included in all lm() models, by default.\nIf you wish, you can specifically tell R to include it (which doesn’t change the default behavior, but just makes it explicit). You do this by using a right-hand-side formula like 1 + predictor:\n\nmy_model &lt;- lm(ELL2012 ~ 1 + ELL2005, data = MI_lead)\n\n\n\nOmitting the Intercept\nYou can omit estimation of an intercept by replacing that 1 with a 0 or a -1. This will force the intercept to be 0 (line goes through the origin).\nThe 0 makes sense to me, because it’s like you’re forcing the first column of the model matrix to contain zeros instead of ones, multiplying the intercept by 0 to force it to be 0.\n(I’m not sure of the logic of the -1.)\nFor example,\n\nmy_model &lt;- lm(ELL2012 ~ 0 + ELL2005 + Peninsula,\n               data = MI_lead)\n\nBut usually, we want to estimate an intercept and we don’t need this code.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#interpreting-summarylm...",
    "href": "lm-fit.html#interpreting-summarylm...",
    "title": "21  Fitting Linear Models in R",
    "section": "21.17 Interpreting summary(lm(...))",
    "text": "21.17 Interpreting summary(lm(...))\nOnce you’ve fitted an lm() in R, how can you view and interpret the results? You may have already noticed the use above of the R function summary(). Let’s consider in more detail…\n\n\n(You can also watch directly on YouTube if you prefer.)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#model-equation-practice",
    "href": "lm-fit.html#model-equation-practice",
    "title": "21  Fitting Linear Models in R",
    "section": "21.18 Model Equation Practice",
    "text": "21.18 Model Equation Practice\nYou should be able to use lm() to fit a linear model, and then use summary() output to fill in numerical parameter estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), \\(\\hat{\\beta}_3\\), \\(\\hat{\\beta}_n\\), and \\(\\hat{\\sigma}\\) in the regression equation:\n\\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon, \\epsilon \\sim \\text{Norm}(0, \\sigma)\\]\n\nAn example model\nTo practice, let’s fit a simple linear regression model in R. For data, let’s consider a dataset containing scores of 542 people who chose to take an online nerdiness quiz. Higher scores mean more nerdiness. The participants also provided their ages. Variables in the data include score and age. Does someone’s age predict their nerdiness score?\nPlot the data and use lm() to fit a simple linear regression model (just one predictor) to explore this question. The code to read in the dataset is provided for you.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nnerds &lt;- read.csv('https://sldr.netlify.app/data/nerds.csv')\ngf_point(_____ ~ _____, data = ______)\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nnerds &lt;- read.csv('https://sldr.netlify.app/data/nerds.csv')\ngf_point(score ~ age, data = nerds)\nnerd_model &lt;- lm(score ~ age, data = nerds)\nsummary(nerd_model)\n\n\n\n\n\n\nWhat is beta0 for the model you just fitted?\n\n -0.061 0.00092 95.8 0.22 15.81\n\nWhat is beta1 for the model you just fitted?\n\n 15.81 95.8 -0.061 0.00092 0.22\n\nWhat is sigma, the standard deviation of the residuals, for the model you just fitted?\n\n 0.00092 15.81 95.8 -0.061 0.22",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-fit.html#linear-regression-conditions",
    "href": "lm-fit.html#linear-regression-conditions",
    "title": "21  Fitting Linear Models in R",
    "section": "21.19 Linear Regression Conditions",
    "text": "21.19 Linear Regression Conditions\nWhat’s next? Before we interpret the results of a model, we have to be confident that we trust the model is appropriate enough for the data, and fits well enough, to produce results that will be reliable.\nSo, next module, we’ll learn to do model assessment (checking that conditions are met to verify that a model is appropriate for a particular dataset).\nWe will also go into much more detail about how to interpret our fitted model!",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Fitting Linear Models in R</span>"
    ]
  },
  {
    "objectID": "lm-assess.html",
    "href": "lm-assess.html",
    "title": "22  Assessment for Linear Models",
    "section": "",
    "text": "22.1 Section Learning Outcomes\nKnowing how to plan and fit a model isn’t the end of the process (at all). What’s next? This time, we explore how to assess whether a model really is appropriate for a particular dataset, and thus reliable. This section focuses very specifically on that important task.\n(After this, with the question of model assessment settled favorably, we can finally turn to model interpretation - what do the results mean, and how can we communicate the insights the model provides via words and visualizations?)\nBy the end of this section you will:",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#section-learning-outcomes",
    "href": "lm-assess.html#section-learning-outcomes",
    "title": "22  Assessment for Linear Models",
    "section": "",
    "text": "List conditions under which linear regression is appropriate, and check whether they hold for a specific dataset and model",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#text-reference",
    "href": "lm-assess.html#text-reference",
    "title": "22  Assessment for Linear Models",
    "section": "22.2 Text Reference",
    "text": "22.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 1.3\nRegression Modeling Strategies Chapter 2.7\nCourse Notes Chapter 2",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#linear-regression-conditions",
    "href": "lm-assess.html#linear-regression-conditions",
    "title": "22  Assessment for Linear Models",
    "section": "22.3 Linear Regression Conditions",
    "text": "22.3 Linear Regression Conditions\nBefore making any statistical inferences with our regression model, we should verify that all necessary conditions for its use are met. Otherwise, we risk presenting and interpreting inaccurate estimates and un-trustworthy conclusions!\nIn this case, there are 5 conditions:\n\nRepresentative sample: the sample data must be representative of the population of interest (as for any statistical inference).\n\n4 more conditions are more specific to linear regression:\n\nLinearity of the predictor-response relationship: there is not a clear non-linear trend in a scatter plot of the response vs. the predictor.\nIndependence of residuals: Knowing the value of one residual doesn’t help you predict the others; there are no lurking variables that affect the accuracy of the model predictions.\nNormality of residuals: The residuals should follow a normal distribution with mean 0.\nError variance is constant: The variance (spread) of the residuals does not change with predictor variable value, or with fitted (on-the-line) value. Other words for this are constant variance of residuals, or homoscedasticity.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#example-model",
    "href": "lm-assess.html#example-model",
    "title": "22  Assessment for Linear Models",
    "section": "22.4 Example Model",
    "text": "22.4 Example Model\nLet’s review and practice in the context of a specific example with a particular dataset and model.\nTo begin, we need to fit a linear regression model in R, using the function lm(). We’ll use the lead dataset from https://sldr.netlify.app/data/FlintWaterTestJanMar2016.csv.\nThis dataset contains water quality test results from a sample of households in Flint, Michigan. Water samples were collected between Jan. 30, 2016 - May 6, 2016. More details about the data collection can be found on the US EPA website.\nHere are the first few rows of the dataset:\n\n\n\n\n\n\nThe first input to lm is a formula of the form \\(y \\sim x_1 + x_2 + ...\\), where y is the name of the response variable, and \\(x_1\\), \\(x_2\\), … \\(x_n\\) are the predictor or explanatory variables. The second input to lm is data, the name of your dataset.\nAs an example, we’ll fit a model with response variable Lead and predictors Fluoride, Water_Filter, and Month. Correct the code below accordingly:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nlead_model &lt;- lm(Lead ~ Fluoride + Water_Filter + Month, data = lead)\n\n\n\n\nThe R code above fits a linear regression model of the form:\n\\[ y = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\epsilon,\\] where \\(\\epsilon \\sim N(0, \\sigma)\\). (The residuals \\(\\epsilon\\) follow a normal distribution with mean 0 and some standard deviation \\(\\sigma\\).)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#model-summary-and-coefficients",
    "href": "lm-assess.html#model-summary-and-coefficients",
    "title": "22  Assessment for Linear Models",
    "section": "22.5 Model Summary and Coefficients",
    "text": "22.5 Model Summary and Coefficients\nTo extract just the coefficient or parameter values (the “\\(\\beta\\)s”) from the fitted model object lead_model, we can use coef():\n\n\n\n\n\n\n\n\nTo get a full summary, use summary():\n\n\n\n\n\n\n\n\nNotes (Reminders!) about the summary:\n\nThe fitted model parameters are in the “Estimate” column of the coefficient table.\nThe last column of the coefficient table contains p-values. The first row is the p-value for a test of the null hypothesis that the true intercept, \\(\\beta_0\\), is 0 (this is not usually of much interest). The second row is the p-value for a test of the hypothesis that \\(\\beta_1\\), the slope for the first predictor variable, is 0. This is usually of interest, because it tells us whether there is a “real” linear relationship between the two variables.\n\\(\\hat{\\sigma}\\) (an estimate of the standard deviation of the residuals from the data) is near the bottom of the output, called “Residual standard error.”",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#regression-conditions-again",
    "href": "lm-assess.html#regression-conditions-again",
    "title": "22  Assessment for Linear Models",
    "section": "22.6 Regression conditions (again)",
    "text": "22.6 Regression conditions (again)\nFor a linear regression model to be appropriate for a dataset, the conditions that must be met are:\n\nThe data are a representative sample of the population of interest.\nLinear relationship between each predictor and the response variable (or at least no apparent non-linear relationship).\nIndependence of residuals\nNormality of residuals\nError (residuals) variance is constant\n\nBut what do each of those (pretty technical) statements mean, and how can we check them?",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#checking-linearity",
    "href": "lm-assess.html#checking-linearity",
    "title": "22  Assessment for Linear Models",
    "section": "22.7 Checking linearity",
    "text": "22.7 Checking linearity\nTo verify that the relationship between our predictor variable(s) and our response variable is linear, we can examine a few plots.\nDATA SCATTER PLOT(S)\nFirst, we can make scatter plots of the data - specifically, the response variable as a function of each predictor variable. We want to verify that there are no apparent non-linear trends in the plot.\n\n\n\n\n\n\n\n\nRESIDUALS VS. FITTED PLOT\nNext, we can plot the residuals from the fitted model as functions of:\n\nThe fitted values (predicted values \\(\\hat{y}\\) for each data point in the dataset)\n(Optionally) Each predictor\n\nIn these plots, we should see no trends – linear or nonlinear. They should look like relatively random scatter.\nWe can use the R function resid() to get the residuals from the fitted model object, and fitted() to get the predictions.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#checking-independence",
    "href": "lm-assess.html#checking-independence",
    "title": "22  Assessment for Linear Models",
    "section": "22.8 Checking Independence",
    "text": "22.8 Checking Independence\nIndependence of residuals means that knowing the value of one residual does not help you to predict the value of the next residual.\nWhen data are collected over time and space, sometimes the residuals are not independent over time (or space) – for example, maybe series of positive residuals tend to occur together, or series of negative residuals together. How can we detect this kind of non-independence?\nFirst, make sure that your dataset is sorted in order of time (if you are worried about non-independence over time) or space (if you are worried about possible non-independence over space). (The lead dataset is already sorted in temporal order - you can verify this by looking at the variable Decimal_Date.)\nThe autocorrelation function will help us.\nThis function computes a value akin to a correlation coefficient for the dataset, indicating how much it is correlated with itself at different “lags”. What does this mean?\nLag 0 is the correlation of the data with itself.\n\n\n\n\n\n\n\n\n\nOf course the correlation at lag 0 is ALWAYS 1: the dataset is identical with itself.\nWhat about larger lags?\nTo compute the ACF at lag 1, we slide one copy of the dataset forward one observation, and compute the correlation again:\n\n\n\n\n\n\n\n\n\nThis tells us how well we can predict one observation in the data, based on the preceding observation.\nLag 2 scoots the data one more position:\n\n\n\n\n\n\n\n\n\n…And so on.\nOutside this website, you can plot the ACF for many lags using the function s245::gf_acf(~fitted_model). (Here we have used acf(), but it will make a non-ggplot/non-ggformula type plot that is a little less pretty.)\n\n\n\n\n\n\n\n\nThe horizontal dotted lines are approximate 95% confidence boundaries for how large the ACF values (for lags greater than 0) should be, if the data are really independent.\nIn general, to verify the independence condition of linear regression, we want to confirm that not many of the ACF values for lags greater than 0 exceed those horizontal “fences” by very much. (One or two ACF values that just barely “poke above the fence” is not a major violation, especially as it is known that the method used to determine the “fences” is known to be a bit too conservative – even independent data will have ACF values that pass them more than 5% of the time.)\nIn the plot above, all the “spikes” are within the “fences”, so there is no indication of non-independence in the residuals (given the current sort order of the data).",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#checking-normality-of-residuals",
    "href": "lm-assess.html#checking-normality-of-residuals",
    "title": "22  Assessment for Linear Models",
    "section": "22.9 Checking Normality of Residuals",
    "text": "22.9 Checking Normality of Residuals\nTo check whether the residuals look like a sample from a normal distribution, look at a histogram:\n\n\n\n\n\n\n\n\nNote: The distribution doesn’t have to look perfectly normal – it has to look like it might reasonably be a sample from a normal distribution. A consequence of that is that as sample size gets smaller, even less-normal-looking residual distributions are quite commonly observed, and we can be even more generous in judging this condition.\nWhatever the sample size, look for very notable skew and/or outliers in the residual distribution. If the residual distribution is unimodal and reasonably symmetric, it is probably okay.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#checking-error-variance",
    "href": "lm-assess.html#checking-error-variance",
    "title": "22  Assessment for Linear Models",
    "section": "22.10 Checking Error Variance",
    "text": "22.10 Checking Error Variance\nFor linear regression to be appropriate for data, there should be homoscedasticity, or constant variance of the residuals. This means that the spread of the data points around the best-fit line should be constant - the width of the point cloud around the line should not be less in some places, and more in others.\nOr, alternatively (and easier to see visually), the spread of the residuals around a horizontal line at y = 0 on a residuals vs fitted plot should be constant.\n\n\n\n\n\n\n\n\nIf this condition does not hold, the most common pattern to see is a “trumpet” shape like the one above, where the residuals become more spread out for larger fitted values. However, any non-constant spread is a cause for concern.\n\nVariable by Variable\nIt is also a good idea to plot residuals as a function of each predictor. Constant variance should hold here just as it does when plotting as a function of fitted value.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-assess.html#model-assessment-checking-conditions",
    "href": "lm-assess.html#model-assessment-checking-conditions",
    "title": "22  Assessment for Linear Models",
    "section": "22.11 Model Assessment: Checking Conditions",
    "text": "22.11 Model Assessment: Checking Conditions\n\nTo review and practice, consider these more detailed explanations of each condition, accompanied by details about which plots to use to check each one and how to interpret them:\n\n\n(You can also watch directly on YouTube if you prefer.)",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Assessment for Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html",
    "href": "lm-interp.html",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "",
    "text": "23.1 Section Learning Outcomes\nKnowing how to plan and fit a model isn’t the end of the process (at all). What’s next? This section we’ve explored how to assess whether a model really is appropriate for a particular dataset, and thus reliable.\nRemember that if a model doesn’t pass assessment - if any of the conditions go unmet - then the model can’t be trusted to provide reliable results at all.\nBut if a model does pass assessment, we can finally turn to model interpretation - what do the results mean, and how can we communicate the insights the model provides via words and visualizations?\nAlso part of this topic is something commonly called model selection. That term is a bit confusing given the approach taught in our course, as we won’t actually select certain variables to include/exclude from a model at this stage.\nRather, we are deciding whether and in what way each candidate predictor relates to the response variable. So for us, “model selection” is all about interpreting and understanding the model results, including using inferential statistics.\nBy the end of the section you will:",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#section-learning-outcomes",
    "href": "lm-interp.html#section-learning-outcomes",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "",
    "text": "Create and interpret prediction plots for multiple linear regression models\nInterpret confidence intervals for linear regression coefficients and in prediction plots\nArticulate how different modeling goals (prediction vs inference) influence how an analyst engages in the modeling process\nInterpret p-values or information criteria to do model selection for linear regression models",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#text-reference",
    "href": "lm-interp.html#text-reference",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "23.2 Text Reference",
    "text": "23.2 Text Reference\nRecommended reading for the materials covered in this section can be found in:\n\nBeyond Multiple Linear Regression Chapter 1.6\nRegression Modeling Strategies Chapters 2.3, 4, and 5.1\nCourse Notes Chapter 1.8-1.11 and Chapter 3",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#prediction-plots",
    "href": "lm-interp.html#prediction-plots",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "23.3 Prediction Plots",
    "text": "23.3 Prediction Plots\nOnce we have fitted a regression model and it passes assessment checks, how can we use it to draw conclusions?\nOne of the most useful tools we can have is a picture of what the model tells us about the relationships between our response and predictor(s).\nNote: in the video, it’ll suggest you use the function ggpredict() (which still works) but it’s been superseded - now the preferred function to use is predict_response(). It has the same inputs.\nHow can we get one?",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#prediction-vs-inference",
    "href": "lm-interp.html#prediction-vs-inference",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "23.4 Prediction vs Inference",
    "text": "23.4 Prediction vs Inference\nIn this course we will focus on modeling for statistical inference, rather than for the purpose of making predictions for new scenarios based on our fitted model. Why? And why does the distinction matter? Before we dive into how to do inference, let’s discuss.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#inference-tldr",
    "href": "lm-interp.html#inference-tldr",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "23.5 Inference, TL;DR",
    "text": "23.5 Inference, TL;DR\n\n\n\nOoh, Slow down…\nMuch of the rest of this section fills in a lot of details that were skimmed over in the very brief overview in that video. Focus first on understanding and applying what was in that video - the rest is elaboration.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#inference",
    "href": "lm-interp.html#inference",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "23.6 Inference",
    "text": "23.6 Inference\n\nOur goal: establish a method to generate confidence intervals and carry out tests for linear models, especially \\(\\beta_0\\) and \\(\\beta_i\\), the intercept and slope(s). In fact, we focus almost entirely on the slopes, but the exact same methods work for the intercept too (it’s just that the slope is almost always of much more practical interest than the intercept).\nWe want to be able to test:\n\\[H_0: \\beta_1 = 0\\]\n\\[H_A: \\beta_1 \\neq 0\\]\nIf the null hypothesis above is true, that means there is no linear association between the predictor and response variables – informally, it means the predictor is useless at predicting the response.\nIf we reject the null, that suggests there is a real association between the two variables (and the predictor is “worthwhile” statistically to help predict the response).\nBefore we carry out any tests, let’s establish an example model.\n\nExample\nWe will use the bonobos dataset as an example. You may recall the dataset from earlier examples. Briefly, the dataset is from a 2019 paper by J.S. Martin and colleagues on bonobo face measurements. The authors worked with 117 bonobos in European zoos and recorded data including their Sex, Age in years, weight, and some measurements about their face:\n\n\n\n\n\n\n\n\n\nFinally, they also measured each bonobo’s assertiveness score (AssR) and dominance score (normDS).\nThe dataset is at https://sldr.netlify.app/data/bonobo_faces.csv and so you could read the dataset in to R by running:\n\nbonobos &lt;- readr::read_csv('http://sldr.netlify.app/data/bonobo_faces.csv')\n\n\n\nPractice\nWe might wonder whether size (as measured by weight) is a good predictor of the dominance score, normDS. Graphically:\n\ngf_point(normDS ~ weight, data = bonobos)\n\n\n\n\n\n\n\n\nFit the corresponding regression model in R and view its summary(). Note, the dataset bonobos is already read in for you here:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFill in the blanks with the right formula, dataset name, and fitted model object name!\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nbonobo_lm &lt;- lm(normDS ~ weight, data = bonobos)\nsummary(bonobo_lm)\n\n\n\n\n\n\nWhat is the intercept estimate for the model you just fitted?\n\n 0.023 0.011 2.008 0.926 1.031\n\nWhat is the slope for the model you just fitted?\n\n 2.008 0.023 0.926 0.011 1.031\n\nWhat is sigma, the standard deviation of the residuals, for the model you just fitted?\n\n 1.031 0.011 2.008 0.926 0.023\n\n\n\n\n\nSE of Sampling Distributions\n\nAs you may have noticed, the summary() of an lm() (or glmmTMB()) regression model fit includes not only estimates of the parameters \\(\\beta_0\\) and \\(\\beta_1\\), but also the standard errors of the corresponding sampling distributions! (Hooray.) These measure the standard deviation we’d expect to see in our parameter estimates from sample to sample, if we were somehow able to collect data many many times. They can be used to calibrate our uncertainty in our estimates of the \\(\\beta\\)s, and they are found in the coefficient table, labelled \"Std. Error\".\n\nnormDS_model &lt;- lm(normDS ~ weight, data = bonobos)\nsummary(normDS_model)\n\n\nCall:\nlm(formula = normDS ~ weight, data = bonobos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3794 -0.8138 -0.2354  0.6043  2.0207 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  2.00774    0.92555   2.169   0.0337 *\nweight       0.01122    0.02334   0.481   0.6322  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.031 on 66 degrees of freedom\nMultiple R-squared:  0.003491,  Adjusted R-squared:  -0.01161 \nF-statistic: 0.2312 on 1 and 66 DF,  p-value: 0.6322\n\n\n\n\nWhat is the SE for the slope, for the regression to predict normDS as a function of weight for our bonobo dataset?\n\n 1.031 0.926 0.023 2.008 0.011\n\n\n\n\n\nCI for the slope\n\nAlthough we won’t derive the reasons why in detail, the sampling distribution for the slope has a t distribution with \\(n - 2\\) degrees of freedom (where \\(n\\) is the number of rows in the dataset).\nSo, after all that, we could find a CI for a regression slope according to:\n\\[ \\hat{\\beta}_1 \\pm t_*SE(\\hat{\\beta}_1)\\] Where \\(t_*\\) is a critical value from a \\(t(n-2)\\) distribution, and SE(\\(\\hat{\\beta}_1\\)) is the standard error of the slope from the coefficient table in the model summary().\nAs an example, find a 95% CI for the slope of our bonobo regression model.\nWe can do this somewhat manually:\n\nbm &lt;- lm(normDS ~ weight, data= bonobos)\nsummary(bm)\n\n\nCall:\nlm(formula = normDS ~ weight, data = bonobos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3794 -0.8138 -0.2354  0.6043  2.0207 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  2.00774    0.92555   2.169   0.0337 *\nweight       0.01122    0.02334   0.481   0.6322  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.031 on 66 degrees of freedom\nMultiple R-squared:  0.003491,  Adjusted R-squared:  -0.01161 \nF-statistic: 0.2312 on 1 and 66 DF,  p-value: 0.6322\n\nCI95 &lt;- -0.14278 + c(-1,1) * \n  qt(0.975, df = nrow(bonobos) - 2) * 0.06668\nCI95\n\n[1] -0.275910915 -0.009649085\n\n\nOr… we will generally take advantage of the fact that there’s a function for that:\n\n\n\n\n\n\n\n\nNotice the shortcut – the R function confint() returns CIs (with a default confidence level of 95%).\nWe can also omit specifying which predictor(s) we want CIs for and confint() will return them all…\n\n\n\n\n\n\n\n\n\n\nTest for a slope\n\nWe can also test the null hypothesis \\(H_0: \\beta_1 = \\beta_{1\\text{null}}\\) (where \\(\\beta_{1\\text{null}}\\) is some hypothetical slope value of interest) using the standardized test statistic\n\\[ t = \\frac{\\hat{\\beta}_1 - \\beta_{1\\text{null}}}{SE(\\hat{\\beta}_1)}\\]\nThe most common value for \\(\\beta_{1\\text{null}}\\) is \\(0\\), because if \\(\\beta_1 = 0\\) that means that there is no relationship between the predictor and response and the predictor is useless as a predictor. If we can reject that null hypothesis, we can conclude that the predictor does have some utility.\nTo practice, you could carry out a two-sided test of \\(H_0: \\beta_1 = 0\\) for the bonobo regression as shown in the hints below (if you learned R in your intro stat course and remember your intro stat pretty well).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nnormDS_model &lt;- lm(normDS ~ weight, data= bonobos)\nsummary(normDS_model)\nt_stat &lt;- (-0.14278 - 0) / (0.06668)\np_val &lt;- 2 * pt(t_stat, df = nrow(bonobos) - 2, lower.tail = TRUE)\np_val\n\n\n\n\nORRRR…A shortcut (which you can take!): Notice that this p-value is already in the model summary (in the last column of the coefficient table, labelled “Pr(\\(&gt;\\vert\\)t\\(\\vert\\))“).\n\n#shortcut: just use model summary\nnormDS_model &lt;- lm(normDS ~ weight, data= bonobos)\nsummary(normDS_model)\n\n\nCall:\nlm(formula = normDS ~ weight, data = bonobos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3794 -0.8138 -0.2354  0.6043  2.0207 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  2.00774    0.92555   2.169   0.0337 *\nweight       0.01122    0.02334   0.481   0.6322  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.031 on 66 degrees of freedom\nMultiple R-squared:  0.003491,  Adjusted R-squared:  -0.01161 \nF-statistic: 0.2312 on 1 and 66 DF,  p-value: 0.6322\n\n\nWhat does it all mean? The p-value of this test measures the strength of evidence in our data against the null hypothesis of no association. Smaller p-values can be considered stronger evidence (and ones bigger than ~0.05-0.1 give weak to no evidence against the null).\nHere, the p-values is really, really big (about 0.6), so we have no evidence against the null hypothesis of no association between a bonobo’s weight and normDS score.\nBut that doesn’t mean we can’t conclude anything!\nConcluding “we have no evidence, based on this data, of any association between weight and dominance score” is an answer to the question of whether they are associated. The answer just seems to be…no.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp.html#whats-next",
    "href": "lm-interp.html#whats-next",
    "title": "23  Interpretation and Inference for Regression Models",
    "section": "23.7 What’s next?",
    "text": "23.7 What’s next?\nWell, we have explored some ways of using linear regression model results to make inferences about whether or not a quantitative predictor is associated with a response variable. The ones we’ve seen have, hopefully, helped you to understand the models a bit better, too!\nBut we don’t have enough tools in our tool-box yet.\nConsider, for example: what if you have a categorical predictor with multiple possible values? We don’t yet have a tool that lets us address whether such a predictor is associated with the response, nor to compare multiple candidate models.\nContinue forward to learn approaches that can take us further!",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interpretation and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html",
    "href": "lm-interp-more.html",
    "title": "24  Selection and Inference for Regression Models",
    "section": "",
    "text": "24.1 Section Learning Outcomes\nEarlier, we started but didn’t finish talking about model selection, and making inferences based on linear regression models.\nWhat if we had a model with several predictors, or a categorical predictor, rather than just one quantitative predictor?\nThis tutorial will give a short overview of a few common model selection tools.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#section-learning-outcomes",
    "href": "lm-interp-more.html#section-learning-outcomes",
    "title": "24  Selection and Inference for Regression Models",
    "section": "",
    "text": "ANOVA??\nNote that for simple one-predictor linear regression, the p-value reported at the very end of the model summary is the same as the one for the slope in the coefficient table.\nThis is only the case when there’s only one, quantitative predictor, since the test at the bottom is testing \\(H_0: \\text{all the slopes in the model are 0 for all predictors}\\). But…it’s reported along with an F-statistic – which reminds us of the one-way ANOVA possibly encountered in a previous stats course. As you later fit models with more than just one predictor, ANOVA does come up as one approach for testing hypotheses about predictor-response associations!",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#case-study-gratitude",
    "href": "lm-interp-more.html#case-study-gratitude",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.2 Case Study: Gratitude",
    "text": "24.2 Case Study: Gratitude\nA growing body of research has documented the effects that practicing gratitude can have on people – providing not just spiritual and mental-health benefits, but even improving physical health.\n\n\n\nDataset\nWe will dive further into this topic – learning about ANOVA and some other tests along the way – via a case study where researchers tried to collect data to answer:\n“How does gratitude affect peoples’ mental and physical well-being?”\nThe data are simulated based on the results of a paper published in 2003:\n\n\n\nCan We Induce Gratitude?\nTo understand whether gratitude can cause improvements in mental and physical health and practices, the researchers needed to do a randomized study, in which they somehow controlled peoples’ gratitude levels. How did they do that?\nFirst, they recruited nearly 200 college students to participate in their study.\nThey asked participants to complete a weekly journal, writing lists of things that they were grateful for. 1/3 of study participants were randomly assigned to this group - the gratitude group.\nOther participants were asked to write about things that annoyed them instead – this was the hassles group.\nFinally, a control group – the events group – just wrote about events in the past week that had affected them (could be either positive or negative).\nBefore delving into any other questions, the researchers had to verify that the gratitude group actually felt more grateful than the other groups…\n\n\nMore Design Details\nIn addition to the journals, all the students had to complete weekly surveys about their behavior and state of mind.\nFor example, they had to state how often (on a scale of 1 to 5) they experienced each of a set of feelings over the preceding week:\n\n\n\n\n\n\n\n\n\n\n\nPulling out Gratitude\nThe researchers combined scores from the words thankful, appreciative, and grateful to assess participants’ gratitude. In our dataset, this value is called gratitude_score.\n\n\n\n\n\n\n\n\n\n\n\nThe Data\nHow do the data actually look?\n\n\n\n\n\n\n\n\n\nIt seems like perhaps the gratitude_score is higher for the gratitude group and lower for the others, but we really need statistical inference to have more confidence in our judgment of whether the difference is real or if the differences between groups could just be the result of random sampling variation.\n\n\nOur Questions\nAs Emmons and McCullough did, our first job will be to test whether the mean gratitude_score is the same for the different groups in the study – the gratitude group, the hassles group, and the events group.\nTo build up to the question we want to answer (comparing all three groups), we will consider a simplified case with only two groups: the gratitude and hassles groups. This is only for learning purposes and not something we’d do in a real analysis!\nWe also have other potential quantitative predictors in the dataset we could include in our models. We will take advantage of that to review CIs and tests for a quantitative predictor, too.\n\n\nReview: Quantitative Predictors\nFor example, we could consider a model to predict gratitude_score as a function of life_rating (a score measuring positivity about one’s life as a whole).\n\n\n\n\n\n\n\n\n\n\nlife_model &lt;- lm(gratitude_score ~ life_rating, \n                 data = grateful)\nsummary(life_model)\n\n\nCall:\nlm(formula = gratitude_score ~ life_rating, data = grateful)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.891 -1.466 -0.098  1.219  5.899 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.2008     0.8828   8.157 4.22e-14 ***\nlife_rating   0.5295     0.1806   2.932  0.00377 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.98 on 194 degrees of freedom\nMultiple R-squared:  0.04243,   Adjusted R-squared:  0.03749 \nF-statistic: 8.596 on 1 and 194 DF,  p-value: 0.003775\n\n\n\n\nReview: CI for Slope\nFind a 95% confidence interval for the slope coefficient of the life_model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nlife_model &lt;- lm(gratitude_score ~ life_rating, \n                 data = grateful)\nconfint(life_model)\n\n\n\n\n\n\nReview: Test for Slope\n\n\nYou test the hypothesis that the slope of the life_model is zero. What is the p-value of the test?\n\n 0.0424 0.00377 4.22e-14 Something else\n\n\n\n\n\nCategorical?\n\nIf we consider a categorical variable with only two categories, everything is exactly the same (except the interpretation of the slope coefficient, which now gives the difference in means between the two categories).\nFor example, we can reduce the dataset to exclude the third events group, and then model gratitude_score as a function of group to see if the gratitude and hassles groups have different average gratitude scores.\nOf course we would never normally do this! But right now, for learning purposes, we could use a categorical variable with 2 categories…and ours has three. So don’t try this filtering/subsetting at home!\n\ngrateful2 &lt;- grateful |&gt;\n  filter(group != 'events') |&gt;\n  mutate(group = factor(group))\ngf_boxplot(gratitude_score ~ group, \n           data = grateful2,\n           orientation = \"x\")\n\n\n\n\n\n\n\n\n\n\nPractice\nFit the model and, just as in the previous example, find a 95% CI for \\(\\beta_1\\) and test \\(H_0: \\beta_1 = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ntwo_group_model &lt;- lm(gratitude_score ~ group, \n                      data = grateful2)\nsummary(two_group_model)\n# if you wanted to pull out just the p-value (advanced move)\ncoefficients(summary(two_group_model))['grouphassles', 'Pr(&gt;|t|)']\n\n\n\n\nEasy peasy! As we mentioned before, the only real difference between this model and the one with a quantitative predictor is that we end up with an indicator variable instead of one with many different numeric values, so the interpretation of the slope coefficient \\(\\beta_1\\) is different.\n\n\nIn our model to compare the gratitude and hassles groups, the grouphassles coefficient estimate is about -1.54. What does this mean?\n\n Something else When the amount of 'hassles' increases by one unit, the gratitude_score goes down by 1.54 units. The mean gratitude_score for people in the hassles group is -1.54. The mean gratitude_score is 1.54 units lower in the hassles group than in the gratitude group. The mean gratitude score is 1.54 units higher in the hassles group than in the gratitude group.\n\n\n\nBut what if we have a categorical predictor with MORE THAN TWO categories? We need a test that incorporates differences between all the categories at once, so the tools we have so far aren’t enough.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#hypotheses-for-anova",
    "href": "lm-interp-more.html#hypotheses-for-anova",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.3 Hypotheses for ANOVA",
    "text": "24.3 Hypotheses for ANOVA\nWe want to test:\n\\[H_0: \\mu_{gratitude} = \\mu_{events} = \\mu_{hassles}\\]\nIn other words, our null hypothesis is that the means of all groups are the same. (The \\(\\mu\\)s are the true population means for each of the groups.)\nThe alternate hypothesis is that at least one pair of groups has different means.\nHow do we translate this set-up into a linear regression model? We’re considering the model in which we predict gratitude_score with group:\n\ngrat_by_group &lt;- lm(gratitude_score ~ group, data = grateful)\nsummary(grat_by_group)\n\n\nCall:\nlm(formula = gratitude_score ~ group, data = grateful)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.527 -1.208 -0.128  1.160  7.016 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      9.5317     0.2383  40.006  &lt; 2e-16 ***\ngroupgratitude   1.1096     0.3369   3.293  0.00118 ** \ngrouphassles    -0.4280     0.3357  -1.275  0.20382    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.921 on 193 degrees of freedom\nMultiple R-squared:  0.1036,    Adjusted R-squared:  0.0943 \nF-statistic: 11.15 on 2 and 193 DF,  p-value: 2.612e-05\n\n\nThere are a few equivalent ways of setting this up (depending on which group is included in the intercept), but in R the code above will yield a model like:\n\\[ y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nwhere \\(x_1\\) and \\(x_2\\) are indicator variables for the gratitude and hassles groups:\n\\[ x_1 = \\begin{cases} 1 \\text{,  if group is gratitude}\\\\ 0 \\text{,  otherwise} \\end{cases}\\]\n\\[ x_2 = \\begin{cases} 1 \\text{,  if group is hassles}\\\\ 0 \\text{,  otherwise} \\end{cases}\\]\nIn this framework, \\(\\beta_0\\) is the mean for the events group. \\(\\beta_1\\) and \\(\\beta_2\\) are the differences in means between groups (gratitude and events, and hassles and events). So our intention with this ANOVA is to test \\(H_0: \\beta_1 = \\beta_2 = 0\\). If this is true, then the intercept \\(\\beta_0\\) will be the overall mean gratitude_score.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#test-stat-for-anova-f-ratio",
    "href": "lm-interp-more.html#test-stat-for-anova-f-ratio",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.4 Test Stat for ANOVA: F-ratio",
    "text": "24.4 Test Stat for ANOVA: F-ratio\nThe big challenge here is to come up with a test statistic – one number that measures evidence against the null hypothesis that all the group means are the same.\nHow can we define a good test statistic to measure how different more than two numbers are?\nThe optional video below will walk through a derivation of the F-ratio statistic.\n\nPart 1: What we need to measure\nNote: in this video, \\(x\\) is used for the variable whose mean is being estimated; for us it would make more sense to use \\(y\\) but I’m trusting you to make the translation…\n\n(You can also watch directly on YouTube if you prefer.)\n\n\nPart 2: How we measure it\nNote: again in this video, \\(x\\) is used for the variable whose mean is being estimated; for us it would make more sense to use \\(y\\) but I’m trusting you to make the translation…\n\n(You can also watch directly on YouTube if you prefer.)\n\n\nSampling Distribution of F, Simulated\nAll right, we can compute the F-ratio now. And we know that the bigger it is, the stronger the evidence we have against the idea that the categorical predictor is not associated with the response.\nBut…how big will it be, even if there is no association?\n\n\nIf the null hypothesis is true, what will the value of the F-ratio be?\n\n Close to 0 Very Large Close to 1 It depends…\n\nIf the population group means are actually very different, with little variation within groups, what will the value of the F-ratio be?\n\n It depends… Close to 1 Close to 0 Very Large\n\n\n\n\n\nClick for more explanations…\n\nIf \\(H_0\\) is true, then SSG may be ‘close to 0’ because the group means will be very close to the overall mean. So, MSG and the whole F-ratio will also be near 0 - but since it MSG is not exactly zero, it matters a lot what the corresponding value of MSE is…\nF will be close to 1 if the null is true and the sample sizes in the groups are equal. MSE and MSG will BOTH depend on the overall variance of the data. In terms of intuition about the F ratio, though, it can be more useful to think about what happens to MSG when \\(H_0\\) is true (it gets smaller, and so so does F…). Bigger F is more evidence against \\(H_0\\).\nIf \\(H_0\\) is very wrong, then SSG will be relatively large because the group means will be quite different from the overall mean. And SSE will be relatively small, if there is little within-group variation. So the ratio will be (big / small) = VERY BIG, providing strong evidence against the null hypothesis and allowing us to conclude that the group means are NOT the same.\n\nAgain, we know that a large value of F will let us reject the null. But how big is “large”?\nWe’d need to know the sampling distribution of F (when \\(H_0\\) is true) in order to judge.\n\n\nLetting R do all the work\n\nWe won’t go into the details of the calculation here; we’ll just note that there’s an R function to automate it all.\n\n\nAnova Table (Type II tests)\n\nResponse: gratitude_score\n          Sum Sq  Df F value    Pr(&gt;F)    \ngroup      82.30   2  11.152 2.612e-05 ***\nResiduals 712.15 193                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBut notice – all the quantities that we used in our derivation of the F-statistic are right there in the ANOVA output table!\n\nThe “group” row gives the SSG and its df (and the F stat and p-value)\nThe “Residuals” row gives the SSE and its df. (Residuals is another statistical term for errors.)\nThe “Sum Sq” column corresponds to “SS” terms (“MS” terms are not in the table, but can be computed from SSX and df from the table)\nThe “F value” is the F-ratio (test statistic)\nThe “Pr(&gt;F)” is the p-value of the test\n\n(A classic intro-stat textbook ANOVA problem is to give you a partially-filled-in ANOVA results table, and ask you to fill in the missing values.)\n\nR Note: Anova(), not anova()!\nBe careful not to use anova() to carry out tests related to predictors in a regression model.\nanova() does “sequential” tests; for example, if you fit a model y ~ pred1 + pred2 + pred3, for pred1 anova() will compare the intercept-only model to the one with pred1 in it, but for pred2 it will compare the model with pred1 and pred2 in it to the one with only pred2, and for pred3 it will compare the model with all three predictors to the one without pred3. ACK!\nIn other words, with anova(), the order in which predictors are listed greatly affects the hypotheses tested, and the test results. THIS IS (ALMOST ALWAYS) NONSENSE. You want Anova() which is in the package car.\n\n\nModel Comparisons\nIn other cases, if we want to compare two particular models with the same response variable but different predictors, we can just fit them both and then compare them with the syntax below.\n\nanova(model1, model2)\n\n(Yes, little-a anova() is what you want this time.)\nWhy might we want to do this? Maybe, for example, you have two different predictors that measure a similar quantity in different ways and you want to know which one works better as a predictor?\nThis anova(model1, model2) approach also works for any of the above cases (removing parameters), if you manually fit the two models.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#akaikes-information-criterion",
    "href": "lm-interp-more.html#akaikes-information-criterion",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.5 Akaike’s Information Criterion",
    "text": "24.5 Akaike’s Information Criterion\nInformation theory has influenced many fields, including statistics. In 1974, Japanese statistician Hirotugu Akaike\n\nused information theory to derive a new criterion, now known as Akaike’s Information Criterion (AIC), to allow comparison of a set of statistical models fitted to a dataset.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#aic-definition",
    "href": "lm-interp-more.html#aic-definition",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.6 AIC Definition",
    "text": "24.6 AIC Definition\nAIC is defined as:\n\\[ \\text{AIC} = 2k - 2(\\hat{\\ell})\\]\nWhere \\(k\\) is the size of the model (number of parameters being estimated), and \\(\\hat{\\ell}\\) is the maximum value of the (base \\(e\\)) log-likelihood fuction.\n\nThe first term, \\(2k\\), inflates the AIC value, causing larger models to have worse AIC values. This is often thought of as a “penalty” on model size.\nThe second term, \\(-2\\hat{\\ell}\\), means that (better-fitting) models with higher log-likelihoods have better AIC values.\n\nR function AIC() computes the AIC for fitted lm() models.",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#aic-practice",
    "href": "lm-interp-more.html#aic-practice",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.7 AIC Practice",
    "text": "24.7 AIC Practice\nWe can use AIC to choose the “best” of a pair (or a larger group) of models.\nFor example, before we used ANOVA to decide whether group (“gratitude”, “hassles”, and “events”) is a good predictor of gratitude score:\n\n\nAnova Table (Type II tests)\n\nResponse: gratitude_score\n          Sum Sq  Df F value    Pr(&gt;F)    \ngroup      82.30   2  11.152 2.612e-05 ***\nResiduals 712.15 193                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can make the same comparison with AIC.\nFirst, we find the AIC for the full model with the group predictor:\n\n\n[1] 817.0972\n\n\nHmmm…this number means nothing in isolation. The only way to get meaning from it is to compare it to the AIC for another model fitted to the same exact dataset and see which is better (smaller).\nDo the rest of the comparison: what is the AIC of the intercept-only model, and which model is better according to AIC?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints…\n\n\n\n\n\nNote: working on your own dataset in a qmd file, you’d probably fit each model, give the fitted model object a name, and then use AIC(model_name)…\nAIC(lm(gratitude_score ~ 1, data = grateful))\nAIC(lm(gratitude_score ~ group, data = grateful))\n\nAIC(lm(gratitude_score ~ 1, data = grateful)) -\nAIC(lm(gratitude_score ~ group, data = grateful))\n\n\n\n\n\n\nWhat is the difference in AIC between the two gratitude models you just fitted?\n\n 17.4; the model with the group predictor has lower AIC. The AIC values for the two models are about the same 17.4; the intercept-only model has lower AIC 834.5; the model with the group predictor has lower AIC.\n\nWhich model is better, according to AIC?\n\n Model with group predictor They are both about the same Intercept-only model\n\n\n\nRemember – smaller AIC is better!",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "lm-interp-more.html#ic-how-big-a-difference",
    "href": "lm-interp-more.html#ic-how-big-a-difference",
    "title": "24  Selection and Inference for Regression Models",
    "section": "24.8 IC, How big a difference?",
    "text": "24.8 IC, How big a difference?\nStrictly and simply using AIC, we can say that a model with lower AIC is better (regardless of how small the difference between the AIC values is).\nIn practice, if using AIC to choose between two models, analysts often require some minimal AIC improvement to justify adding an additional parameter/predictor to a model. (Common thresholds are 2, 3, or maybe 6). So, one might not prefer a larger model unless it reduces AIC by at least 2-3 units.\nSo, if we are comparing two models (say, with and without a key predictor of interest), if the model with the predictor is better by well over 2-3 AIC units, we consider that pretty convincing evidence of an association between that predictor and the response.\nFor a lot more excellent information about practical use of AIC, the interested reader can check out the classic book by Burnham and Anderson:",
    "crumbs": [
      "Multiple Linear Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Selection and Inference for Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html",
    "href": "interactions.html",
    "title": "25  Interactions in Regression Models",
    "section": "",
    "text": "25.1 Module Learning Outcomes\nWe’ve already seen how to include multiple predictors - both categorical and quantitative - in a regression model. We know how to deal with confounders, mediators, and even colliders in terms of model planning. But what about moderators? Two variables have an interaction - or, one moderates the effect of the other - when the size or direction of the first predictor variable’s effect on the response changes depending on the value of the second predictor. Interactions are a bit tricky to understand and interpret, so we’ll spend some time with them (which will also allow us time to hone our model planning, fitting, assessment, and interpretation skills).\nBy the end of the module you will:\nThis module is a little bit lighter in content load than the last few…in part to leave room for the complicated concept of interaction to sink in, and in part to give you some time to catch your breath and catch up if needed!",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#module-learning-outcomes",
    "href": "interactions.html#module-learning-outcomes",
    "title": "25  Interactions in Regression Models",
    "section": "",
    "text": "Define interactions in the context of a regression model\nGain facility at using causal diagrams to identify variables as mediators, moderators, confounders, colliders\nPractice choosing which to include in a regression model\nUse appropriate notation to fit models with interactions\nInterpret output of a fitted model with interactions, making appropriate use of prediction plots and other model selection tools\nPractice and gain confidence at the regression modeling process, including planning, fitting, assessment, and interpretation/selection phases",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#text-reference",
    "href": "interactions.html#text-reference",
    "title": "25  Interactions in Regression Models",
    "section": "25.2 Text Reference",
    "text": "25.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 1.6.6\nCourse Notes Chapter 8\nRegression Modeling Strategies Chapters 2.3.2, 2.7.2\n\nIt’s suggested that you consider consulting these chapters after doing this tutorial, with particular focus on any topics you found most challenging.",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#interactions-defined",
    "href": "interactions.html#interactions-defined",
    "title": "25  Interactions in Regression Models",
    "section": "25.3 Interactions Defined",
    "text": "25.3 Interactions Defined\nTwo predictors interact when you need to know values of both in order to make an accurate prediction of the response variable value.\nPredictors can interact in any type of regression model (so this chapter could really be placed almost anywhere in our course).",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#interactions-what-are-they",
    "href": "interactions.html#interactions-what-are-they",
    "title": "25  Interactions in Regression Models",
    "section": "25.4 Interactions: what are they?",
    "text": "25.4 Interactions: what are they?\nOk, so we just saw a definition of interactions. We also learned before about moderation, which is another term for interaction. But it’s a tricky concept to fully understand, so let’s see some more explanations.\nHere’s a pretty energetically-delivered video (not specifically made for our course this time, but useful) that explains what interactions are, with some helpful examples.\nChallenge: Watch for a graph in the video where the data shown doesn’t match the verbal interpretation! (I think this is probably because of a labeling mistake or a data-simulation mistake rather than dishonesty or anything…)\n\n\n\nAnother (optional) interaction video\nStill scratching your head and wondering exactly what an interaction is?\nIt’s a challenging concept for almost everyone, so I want you to hear definitions straight from me, plus have access to our texts, plus hear definitions from several other people…below is another video option. I think it’s a bit more boring than the previous one, but if you want another brief review, it’s worth the 5-minute watch…",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#set-examples",
    "href": "interactions.html#set-examples",
    "title": "25  Interactions in Regression Models",
    "section": "25.5 SET Examples",
    "text": "25.5 SET Examples\nWe need to see some more examples of this. To do so, let’s consider a dataset called teach_beauty, on course evaluation scores (eval) for a lot of university professors. You can find a copy of the dataset at: https://sldr.netlify.app/data/ProfEvaltnsBeautyPublic.csv.\nThese data are included in a textbook by Andrew Gelman of Columbia University, and they might be real course evaluations from Columbia (or they might be simulated; we’re not sure).\nSome additional information is given about each faculty member, including\n\nwhether their native language is English or something else: native_language\na ranking of their beauty (somehow, we don’t know how)\ntheir age\nwhether they tended to dress in a formal way\nwhether or not they are female\nwhether or not they are White/Caucasian (race_eth)\n\nA bunch of research has suggested that scores from student evaluations of teaching (SETs) don’t really measure teaching skill well (for example, Spooren et al. 2013, Hornstein 2017, and Kreitzer & Sweet-Cushman 2022). So, it could be interesting to consider what they do actually measure…and a number of studies have found discrepancies in which faculty who are younger, female, and more tend to get lower scores.\nHere, we will not dive into a full model plan and causal diagram, but just play with and explore a few example model to help deepen our understanding of how interactions work.\nSince we’re doing “night science” and not focused on later inference, we can take a peek at the data. Feel free to make some additional graphs if you like!\n\n\n\n\n\n\n\n\n\nCategorical-Quantitative Interaction\nThat graph you just saw might show evidence of an interaction…\n\n\n\n\n\n\n\n\nEval may go up as beauty increases, but the slope of the relationship seems like it might be different for the female group.\nThis would be an interaction between beauty and female.\n\n\nCategorical-Categorical Interaction\nWe just saw an interaction between categorical and quantitative variables. What about a categorical variable modulating the effect of another categorical variable?\n\n\n\n\n\n\n\n\nPerhaps Informal Dress affects eval scores, but really only if the teacher is not female – for females, formal dress doesn’t make a difference either way.\nThe effect of formal dress is different depending on the value of female. This is an interaction between formal and female.\n\n\nQuant-Quant interactions?\nYes, these are possible, but very hard to visualize and conceptualize.\nBasically, it would mean that the slope of the line for one predictor changes gradually as the value of a second variable changes.\nIn other words, the response variable depends not only on each of the two predictors, but also on their product.\nYou’ll see one example later on, but generally I recommend you initially avoid interactions of two quantitative variables unless you are pretty sure you need one and have confidence in your ability to interpret the result.\n\n\nR code\nIf you want to include an interaction term in a model in R, in the model forumla, use a * rather than a + between the predictors that (may) interact. For example, consistent with our exploration above, we might try:\n\n\n\n\n\n\n\n\nNotice the additional indicator variables in the coefficient table/model equation. These function to adjust the effects of the beauty predictor depending on the values of formal and female, which interact with it. (More details on that equation-y stuff later.)\nWe could use IC-based model selection to determine whether including these interactions in a model is important or not.\n\n\n\n\n\n\n\n\nIn the case of the particular model we fitted, the “best” model starting from this full model is actually one without interactions. If you want to explore the dataset further, you will find that actually a model where age, beauty AND female interact fits much better…I encourage you to give that a try!\n\n\n\n\n\n\n\n\n\n\nCautionary note\nIf you include an interaction in a regression model, you must also include the corresponding “fixed effects” – this means if you have an indicator variable/slope term for an interaction in your model, you must also have the indicator variables/slopes corresponding to the individual predictors. Our fitting functions (lm(), glm(), glmmTMB(), etc.) are smart enough to ensure this for you. So is dredge(). (It would take effort to mess this up in R.)",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#bee-examples",
    "href": "interactions.html#bee-examples",
    "title": "25  Interactions in Regression Models",
    "section": "25.6 Bee Examples",
    "text": "25.6 Bee Examples\nIt’s nearly impossible to have too many examples when introducing the idea of interactions. So let’s see some more…\nWe will use data from a 2019 paper by Adam Dolezal and colleagues, entitled Interacting stressors matter: diet quality and virus infection in honeybee health (https://doi.org/10.1098/rsos.181803). Its abstract reads:\n\n\nHoneybee population declines have been linked to multiple stressors, including reduced diet diversity and increased exposure to understudied viruses. Despite interest in these factors, few experimental studies have explored the interaction between diet diversity and viral infection in honeybees… In laboratory experiments, we found that high-quality diets have the potential to reduce mortality in the face of infection with Israeli acute paralysis virus (IAPV).\n\n\nYou can find a copy of the dataset at: https://sldr.netlify.app/data/bee-virus.csv. Here, we could read in and prepare the data like so:\n\nbees &lt;- readr::read_csv('data/bee-virus.csv', show_col_types = FALSE) |&gt;\n  rename(Cage_id = `Cage Number`,\n         Virus = `Virus Treatment`,\n         Food = `Pollen Treatment`,\n         Experiment_id = `Experimental replicate code`,\n         Mortality = `72 hpi proportion mortality`) |&gt;\n  drop_na(Virus, Food, Mortality)\n\n\n\n\n\nCategorical-Quantitative Interaction\nWhat would it mean for the effect of Virus and the effect of Food to interact?\nHere’s a graphical exploration:\n\n\n\n\n\n\n\n\nIf food is “None” that is low-quality food with no pollen added (the other kinds have pollen, with “Poly” being the highest quality).\nSome cages were infected with a virus, and some were not.\nDo bees on different diets take a different survival hit when infected with the virus? We’d need to fit some models to find out!\nBefore we go further…\n\n\nWhat would it mean for the bees’ diet to modulate the effect of the virus on the bees? (If you’re struggling, refer to the data graph shown earlier, which may help you to understand…)\n\n Diet and virus interact if the effect of the virus on bee mortality rates is BIGGER for some diet(s) than others. An interaction between diet and virus means that diet and virus affect one another, that is, they are associated with each other None of the above Both diet AND virus infection have effects on the number of bees that die\n\n\n\n\n\nClick for explanations of solution above.\n\nAn interaction means that the proportion of bees killed by virus infection depends on the diet they are eating.\nHere, we might think that bees eating the best food are better able to survive infection, while those with the lowest quality of the food die more once infected.\nAll bees die more with virus than without, but the jump (proportion of extra death when the virus infection is present) is much bigger when the food is bad.\nIt’s not correct to say that “interaction means diet and virus are associated with each other…” Yikes! Sorry, no. This is a very common misconception. Two predictors ARE sometimes associated with each other. But whether they are or not, that is NOT an interaction. An interaction is when the value of one predictor ALTERS the relationship between the other predictor and the response variable.\nIt’s also not complete to say that there’s interaction if “both diet AND virus infection have effects on the number of bees that die.” This answer answer doesn’t describe an interaction – this could just describe an additive model, where diet and virus both affect mortality.\n\nOK! Let’s go ahead and fit that model:\n\n\n\n\n\n\n\n\nEek. What does it all mean?\n\n\nSelection: ANOVA\nYou’ll see some examples of how to write and understand the model equation a bit later. But for this example, let’s think about how we would make inferences – for example, how could we test whether Virus and Food really interact?\nOne approach would use ANOVA; see if you can code it…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nbee_int_model &lt;- lm(Mortality ~ ____ * ____, data = bees)\ncar::Anova(...)\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nbee_int_model &lt;- lm(Mortality ~ Virus * Food, data = bees)\ncar::Anova(bee_int_model)\n\n\n\n\nBased on the ANOVA results, do we have evidence that Virus and Food interact?\nWait…before you answer that…",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#selection-note-about-p-values",
    "href": "interactions.html#selection-note-about-p-values",
    "title": "25  Interactions in Regression Models",
    "section": "25.7 Selection: Note about p-values",
    "text": "25.7 Selection: Note about p-values\nWe have not spent much time reviewing how to interpret p-values; for one thing, we often use information criteria to compare models, and for another, interpreting p-values was hopefully covered in the intro stat course you once took. But, perhaps that was long, long ago. Or, perhaps you took a more traditional course with a different approach to interpreting p-values.\nFor this course, I strongly request that you:\n\nReport the exact p-value of any hypothesis test you carry out (not just “&lt; 0.05” or “&lt; 0.01” etc. – I crossed those out for emphasis!)\nBased on the size of the p-value you obtain, make a judgment call about the strength of evidence it gives you against the null hypothesis. (Remember, for regression models, the null is usually “there is no association between \\(X\\) and \\(Y\\),” or maybe today, “there is no interaction between \\(X\\) and \\(Z\\).”)\n\nThe smaller the p-value, the stronger the evidence; and p-values above about 0.05-0.1 are essentially no evidence against the null hypothesis.\n\n\nFor different practitioners, and in different contexts, different scales for strength-of-evidence provided by p-values have been suggested.\nIn A reckless guide to p-values Michael Lew discusses the history and practical application of p-values at length and suggests that “Strength of evidence against the null hypothesis scales semi-geometrically with the smallness of the P-value” as shown in his Figure 3:\n\n\n\n\n\n\n\n\n\nAlternately, in In defense of p-values, Paul Murtaugh suggests:\n\n\n\n\n\n\n\n\n\nThese should give you an idea of how to interpret p-values and draw conclusion from them, but a key point is to avoid memorizing some algorithm or rule to decide whether or not you have a “significant” result, and to avoid summarizing results with that single word – so, we do not simply say a conclusion is “significant” if it is based on a p-value below 0.05 (again, the words are crossed out for emphasis!) There are lots of reasons – see the articles linked above by Murtaugh and Lew if you want to read more.\nSo what do we say, if we are avoiding the fraught shorthand of calling it “significant”?\nA key best practice is to accompany all hypothesis testing results with a discussion of the “effect size” – for example, how big is the slope, or the difference between groups? Try to consider whether the size of the estimated difference is big enough to be of practical importance.\nNow, back to our question:\n\n\n\n\n\n\n\n\n\n\nBased on the ANOVA results, do we have evidence that Virus and Food interact?\n\n Some; with a p-value of about 0.02, the evidence is moderate but not completely compelling Yes, I'm basically sure Nope, most definitely NOT There is significant evidence (p &lt; 0.05)\n\n\n\n\n\nClick for explanations of solution above.\n\nThe p-value for the interaction term is 0.024. How strong or convincing is that as evidence against the null hypothesis that there’s no association?\nIf you said you were “basically sure” or that there was “definitely NOT” evidence, maybe reconsider your phrasing and level of certainty? We rarely want to sound that sure. Also, consider the exact p-value and the strength of evidence it provides…\nAlso remember that in our course, we avoid the word ‘significant’ like it’s contaminated! Because…it kind of is, by decades of dubious stats teaching and practice reinforcing the idea that hypothesis testing yield ironclad yes-or-no results. Try the strength-of-evidence way outlined above, instead…",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#selection-ic",
    "href": "interactions.html#selection-ic",
    "title": "25  Interactions in Regression Models",
    "section": "25.8 Selection: IC",
    "text": "25.8 Selection: IC\nWhat if we wanted to make the same comparison – of models with and without the interaction – but using information criteria instead of hypothesis testing?\nGive it a try…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nbee_int_model &lt;- lm(Mortality ~ Virus * Food, data = bees)\nbee_no_int_model &lt;- ...\nAIC(...)\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nbee_int_model &lt;- lm(Mortality ~ Virus * Food, data = bees)\nbee_no_int_model &lt;- lm(Mortality ~ Virus + Food, data = bees)\nAIC(bee_int_model, bee_no_int_model)\n\n\n\n\n\n\nBased on your results (using AIC), what do you conclude?\n\n The AIC is about 4 units lower for the model with the interaction, so the model with the interaction doesn't fit as well, and according to this analysis, there probably is NOT an interaction Since the AIC score is negative (-154) for the model with the interaction and also for the one without, we have some doubts about the validity of the model overall. Nope, most definitely NOT an interaction there! There's most definitely an interaction The AIC is about 4 units lower for the model with the interaction, so the model with the interaction fits a fair amount better and according to this analysis, there probably is an interaction\n\n\n\n\n\nClick for explanations of solution above.\n\nThe difference in AIC between the models is about 4, which is getting big enough that we think one model really does fit noticeably better than the other.\nSome points to remember:\n\nCarefully choose your level of certainty in presenting your results.\nLower AIC scores are better.\nThe answer focusing on the sign of the AIC value is basically nonsense in a sentence. Remember, for information criteria, it’s the relative value that matters - that is, the difference in IC values between models under comparison. The absolute AIC number for one model tells you…not much! Positive, negative, big, small…it’s only differences in IC values between models being compared that are informative for inference.\n\n\nAre the results the same with BIC? (not that you would normally do both – but just for practice, alter the code above to use BIC instead and then answer this question!) Since BIC enacts a different definition of “goodness” of model quality, the two will not necessarily always agree.\n\nCategorical-Quantitative Interaction\nWe can also interact a categorical and a quantitative variable. Then, what we essentially want is a different slope for the quantitative variable for each category.\nWe don’t have a quantitative variable in our dataset. I am adding a fake one ONLY to show how the code would work to fit a C-Q interaction model. You don’t need to be able to do this, of course…and in a real analysis it would be a TERRIBLE and dishonest idea! However, here for practice purposes only, we will do it.\n\n\n\nNow, fit a model with and effect of Virus and also a different slope for fake_qvar for each level of virus: one slope if there was no virus infection, and a different slope if there was viral infection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nbee_model &lt;- lm(Mortality ~ Virus * fake_qvar, data = bees)\n\n\n\n\nNow, since I made that fake_qvar data up totally at random, we expect that it shouldn’t really be associated with Mortality or interacting with Virus.\nCan you confirm that using either ANOVA or information criteria?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution(s):\n\n\n\n\n\n# you would likely do one of these three:\ncar::Anova(bee_model)\nbee_model_noint &lt;- lm(Mortality ~ Virus + fake_qvar, data = bees)\nAIC(bee_model, bee_model_noint)\nBIC(bee_model, bee_model_noint)\n\n\n\n\n\n\nQuant-Quant Interaction\nOne more time, I’ll say:\nProbably just don’t go there. It’s quite hard to make sense of.\nA quantitative-quantitative interaction means that the slope for one predictor variable changes continuously as the value of the other predictor changes…in other words, the response depends on both predictors plus it depends on their product. Keep going for one example of this later on, though.\n\n\nGoing Further\nAny other models or comparisons you want to try?\nFeel free to experiment a bit if so…",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#prediction-plots",
    "href": "interactions.html#prediction-plots",
    "title": "25  Interactions in Regression Models",
    "section": "25.9 Prediction Plots",
    "text": "25.9 Prediction Plots\nWhat about prediction plots for a model with interaction(s)? Certainly, we’d want to be able to show them…\n\n\nThe key thing to remember is that if predictors interact in a model, you shouldn’t show a prediction plot with any of them unless it shows all of them.\n\n\nLuckily, that’s easy to do with predict_response() – just make sure your terms input contains the names of both variables.\nFor example, with our bee model:\n\n\n\n\n\n\n\n\nIf you want to customize the plot a lot, you can omit the |&gt; plot() and make your own graph of the results table returned by predict_response().\nBut there’s one thing you may often want to try…see what happens if you change the order of the predictors in the terms list.\nWhich graph is easier to interpret, in your opinion?\nOften one way tells the story a lot better than the other, so it pays to consider carefully which to show (or try both, and then choose).",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#car-examples",
    "href": "interactions.html#car-examples",
    "title": "25  Interactions in Regression Models",
    "section": "25.10 Car Examples",
    "text": "25.10 Car Examples\nHow does the model equation work when you have an interaction? For each pair of interacting variables, you get an additional term in the model that is the product of the interacting variables.\nIf one or more of them are categorical, then there will be a new product term for every pair of indicator variables possible between the two variables! (So interactions between categorical variables with many groups can quickly lead to a need to estimate a LOT of parameters…)\nWe focus more on understanding what interactions are and when to include them in our models, but let’s spend just a bit of time considering the model equation with interactions.\nLet’s go!\nFor these examples, we’ll consider a toy example in which we want to model the price of a car as a function of its mileage, color, and age.",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#model-equations",
    "href": "interactions.html#model-equations",
    "title": "25  Interactions in Regression Models",
    "section": "25.11 Model Equations?",
    "text": "25.11 Model Equations?\n\nOne Quantitative Predictor\nThis one should be all review…no interactions yet.\n\n\n\n\nTwo Quantitative Predictors\nAgain, this is review - still no interactions\n\n\n\n\nInteracting Quantitative Predictors\nHere is the one example I promised you of two quantitative predictors interacting!\n\n\n\n\nCategorical-Quantitative Interaction\nFinally, what about a categorical variable interacting with a quantitative predictor?",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#interactions-what-are-they-again",
    "href": "interactions.html#interactions-what-are-they-again",
    "title": "25  Interactions in Regression Models",
    "section": "25.12 Interactions: what are they, again?",
    "text": "25.12 Interactions: what are they, again?\nThis might be a good time to watch that initial video one more time, to review and maybe pick up something you missed the first time through:",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "interactions.html#more-practice",
    "href": "interactions.html#more-practice",
    "title": "25  Interactions in Regression Models",
    "section": "25.13 More Practice",
    "text": "25.13 More Practice\nIt can be really challenging to understand what an interaction means and gain skills at using and interpreting them.\nHere is one way to see some additional examples, and collaborate with me and students from your class and even beyond! Check out the instructions and examples in our shared Google slide deck.",
    "crumbs": [
      "Interactions",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Interactions in Regression Models</span>"
    ]
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "26  Maximum Likelihood Estimation",
    "section": "",
    "text": "26.1 Module Learning Outcomes\nSo far, we have gotten quite a bit of practice with the practicalities of the regression modeling process. We have spent less time considering how our statistical software is actually doing the model fitting for us, or examining the mathematics used to do that fitting.\nNow is our chance to do both of those things! In addition to deepening our understanding of multiple linear regression, this week’s material will prepare us to understand and fit other types of models in coming modules.\nBy the end of the module you will:",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#module-learning-outcomes",
    "href": "mle.html#module-learning-outcomes",
    "title": "26  Maximum Likelihood Estimation",
    "section": "",
    "text": "Define “Likelihood”\nUnderstand and explain how maximum likelihood estimation is used to fit regression models\nRecognize several common PDFs and PMFs beyond the Normal, stating their support, parameters, and possible shapes",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#text-reference",
    "href": "mle.html#text-reference",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.2 Text Reference",
    "text": "26.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 2 and Chapter 3\nCourse Notes Chapter 4 and Chapter 5\nEcological Models & Data in R Chapter 4, Chapter 6\n\nIt’s suggested that you consider consulting these chapters after doing this tutorial, with particular focus on any topics you found most challenging.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#likelihood-to-compare",
    "href": "mle.html#likelihood-to-compare",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.3 Likelihood to Compare",
    "text": "26.3 Likelihood to Compare\nWhat’s “likelihood,” and how can we use it to compare models? Let’s find out…with seals.\n\n\nAs you move through the video, you may want to visit\n\nAn online normal distribution calculator\nOur class spreadsheet with likelihoods",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#recap",
    "href": "mle.html#recap",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.4 Recap",
    "text": "26.4 Recap\nSo far, we have seen that the likelihood of a dataset given a particular model…\n\nCan be used to measure model-data match\n(…and then as ingredient to AIC/BIC)\nCan be used to fit one model: which parameter estimates are “best”?",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#cheese-data",
    "href": "mle.html#cheese-data",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.5 Cheese Data",
    "text": "26.5 Cheese Data\nNext, this tutorial will guide you through an exercise designed to deepen your understanding of likelihood and maximum-likelihood estimation of linear regression models.\nWe will use a small dataset to try to answer: What makes cheese tasty? …while learning about Maximum Likelihood Estimation.\nThe dataset, called cheddar (and available in package faraway) has data on expert cheese tastiness ratings, as well as several chemical properties of each cheese.\n(Unfortunately, we don’t have a lot of detail about the units of measure or method of determining these properties – it is obvious that Dr. Faraway, who provided the data, is a statistician and not a chemist.)\nTo use this dataset, all you have to do is load the package faraway with library() and then you can start using the cheddar dataset immediately.\n\n\nRows: 30\nColumns: 4\n$ taste  &lt;dbl&gt; 12.3, 20.9, 39.0, 47.9, 5.6, 25.9, 37.3, 21.9, 18.1, 21.0, 34.9…\n$ Acetic &lt;dbl&gt; 4.543, 5.159, 5.366, 5.759, 4.663, 5.697, 5.892, 6.078, 4.898, …\n$ H2S    &lt;dbl&gt; 3.135, 5.043, 5.438, 7.496, 3.807, 7.601, 8.726, 7.966, 3.850, …\n$ Lactic &lt;dbl&gt; 0.86, 1.53, 1.57, 1.81, 0.99, 1.09, 1.29, 1.78, 1.29, 1.58, 1.6…\n\n\nBefore you continue, make 1-2 quick exploratory data plots to see whether you note any trends in taste depending on the H2S, Acetic acid, or Lactic acid content of the cheeses in the dataset.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#cheese-model",
    "href": "mle.html#cheese-model",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.6 Cheese Model",
    "text": "26.6 Cheese Model\nThere are just 30 data points, so we’d be best off considering a model with 1-2 predictors (at most!) and not all 3 that are present in the dataset.\nFor this exercise, let’s do just one predictor; we will choose one: H2S. (This is “night science” – fun and exploration – so we won’t worry about the fact that that choice may have been based on the data plots you just made!)\nAdjust the code below to fit a one-predictor linear regression model with our chosen predictor, and view its summary().",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#mle",
    "href": "mle.html#mle",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.7 MLE",
    "text": "26.7 MLE\nEarlier we used likelihood to compare two models fitted to the seal data (one with the ice_cover predictor, and one without).\nThat’s part of how AIC and BIC work.\nBut we can use likelihood another way, too: to “fit” a single model to data, figuring out which exact slope and intercept values are BEST for a given dataset and model.\nHow can we use likelihood to find these “best” slope and intercept parameter estimates, ourselves?\n\nCaveats\nWhat you’ll do next isn’t exactly what R does to fit an lm(). For the linear regression case, smart folks have used calculus and linear algebra to derive analytical solutions for the parameter estimates \\(\\hat{\\beta}\\) of a multiple linear regression model, in terms of the dataset - that’s why lm() can return results super fast.\nAnd in cases where R does need to maximize a likelihood to fit a model, it uses smarter methods than what we are about to do! We will just try a huge range of reasonable possibilities for slope and intercept, getting the likelihood of the data for every slope/intercept guess, and finally choosing the best among them. What are are about to do (it’s called a grid search) is a great illustration of the idea of maximum-likelihood estimation, but it is not the fastest algorithm to carry it out. (Stay tuned for an optimization course to learn smarter and more efficient algorithms.)\nYou don’t have to be able to replicate the R code in the following sections (up through the interactive plot). But do your best to UNDERSTAND what each code chunk is doing. I suggest adding making your own notes file with additional explanation or code comments as needed.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#guess-the-parameters",
    "href": "mle.html#guess-the-parameters",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.8 Guess the parameters",
    "text": "26.8 Guess the parameters\nBased on what you have seen in the scatter plots, you can probably give reasonable upper and lower boundaries on the intercept and slope for your regression line.\nDon’t use the estimates from the lm() summary to give unreasonably precise guesses - the point here is to give a wider range of somewhat-reasonable slope values that you can then “search” for the best parameter values.\nIn the code below, do you think a range of reasonable intercepts and slopes is covered? The function seq() takes inputs from (the minimum value to try), to (the maximum), and the amount to increment by. Due to the way the tutorial interface works, you can’t really change the values here, but if you run all the code on your own in your own Quarto document, you can try other options if you wish!\n\n\n\n\n\n\n\n\nEach row of MLE_grid give one set of guesses of a possible slope and intercept for our regression model.\nNow, we want to compute the likelihood of the dataset given each of those sets of parameters. We can compare the likelihoods to see which estimates are best!",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#compute-fitted-values",
    "href": "mle.html#compute-fitted-values",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.9 Compute fitted values",
    "text": "26.9 Compute fitted values\nHow will we compute the likelihood of the dataset, given a particular slope and intercept guess? Well, first we need to get the model residuals (since we compare those to a normal distribution in order to get the likelihood).\nAnd to get the residuals, we need to subtract the model-predicted values \\(\\hat{y}\\) from the observed values of the response \\(y\\).\nWe already have the response variable values \\(y\\), in the dataset. But we need the predicted values \\(\\hat{y}\\)! We have to compute them by hand, since we are not using lm() – we can’t use predict() without the fitted model object that comes from lm()!\nBut no problem, we can do this.\nOur model is:\n\\[y = \\beta_0 + \\beta_1x + \\epsilon\\]\nAnd according to this model, the predicted value of \\(y\\) (on average) is \\(\\beta_0 + \\beta_1x\\).\nWell, \\(x\\) is the predictor – we have data on that. We also have candidate values of \\(\\beta_0\\) (the intercepts) and \\(\\beta_1\\) (the slopes).\nWe will make our own function to compute fitted values, given a dataset and candidate slope and intercept. Change this to make sure it uses “your” predictor! Running this chunk will create a FUNCTION you can use later – so there will be no output yet.\n\n\n\n\n\n\n\n\nNow we can add a column to our MLE_grid with the fitted values for slope/intercept guess.\nThis will take a while to run.\nWhen it’s done, look what has happened: each row of the dataset (corresponding to one guessed slope and intercept) contains a list of all 30 fitted values! (Cool.)",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#compute-the-residuals",
    "href": "mle.html#compute-the-residuals",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.10 Compute the Residuals",
    "text": "26.10 Compute the Residuals\nOK. Shoot. We didn’t actually need the fitted values themselves: we need the residuals. We can compute them in a similar way, though. We just need to subtract the fitted values from the observed response variable values!\nHere is some code we can use to create a function to make this calculation for us:\nNow we can add a column to our MLE_grid with the residuals for each row (change the code below to use the predictor variable for your model). This will take a while to run. When it’s done, look what has happened: each row of the dataset contains a list of 30 residuals! (Still totally cool.)\n\n\n\n\n\n\n\n\n\n\nEstimate the residual standard error\nTo be able to get the likelihood from the residuals, we need to first estimate the residual standard error \\(\\sigma\\). The code below takes each list of 30 residuals and computes its standard deviation (sd).\n\n\n\n\n\n\n\n\n\n\nCompute the actual likelihood\nFinally, we are ready to compute the natural log of the likelihood of the dataset, given each proposed model (each “proposed model” is one set of candidate \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\)).\nFor each row, we need to compute the natural logarithm of the product of the likelihoods of all the residuals, in a normal distribution with mean 0 and sd \\(\\sigma\\). We will do the mathematical equivalent: find the sum of the natural logarithms of the normal likelihoods of each residual value.\nAnother explanation (add your own additional detail if needed):\n\nFor each single residual, we find its likelihood by finding the density of a normal (mean = 0, standard deviation = \\(\\sigma\\)) distribution for x = “our residual”\nWe take the natural log of each residual’s likelihood (working on a log scale reduces calculation problems for the computer)\nFor each row of MLE_grid – that is, for each proposed slope/intercept combination – we add up all the residual log-likelihoods to get one joint log-likelihood for the whole dataset\n\nBe patient, this will take a minute:",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#best-model",
    "href": "mle.html#best-model",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.11 Best Model?",
    "text": "26.11 Best Model?\nThe BEST model (according to maximum likelihood estimation) is the one with the BIGGEST log-likelihood. Which one is that?\nYes, we could just sort the data table by likelihood values. We could also…\nUse a plot to figure it out!\n\n\n\n\n\n\n\n\nWhy all the dots? Well, the intercept helps determine the likelihood, too…color by the models candidate intercept value to better see what I mean:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngf_point(log_likelihood ~ slope, data = MLE_grid,\n         color = ~intercept)\n\n\n\n\nOK, kind of helpful – we can start to see where the highest values are…\nMaking it interactive might help!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint:\n\n\n\n\n\nlibrary(plotly)\ngf_point(...) |&gt;\n  ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nlibrary(plotly)\ngf_point(log_likelihood ~ slope, data = MLE_grid,\n         color = ~intercept) |&gt;\n  ggplotly()\n\n\n\n\nIn the interactive plot, we can mouse over to find the exact slopes and intercepts that seem to have the highest likelihood.\nTo find the absolute top ones, we can also arrange() the dataset in order of descending likelihood and check out the top entries:",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#comparison",
    "href": "mle.html#comparison",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.12 Comparison",
    "text": "26.12 Comparison\nHow do the slope and intercept that you just found compare with the one from summary() and lm()?\n(If we did everything right, they should be quite close! We only estimated to one decimal place, though.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ncheese_model &lt;- lm(taste ~ H2S, data = cheddar)\nsummary(cheese_model)",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#mle-the-mathy-recap",
    "href": "mle.html#mle-the-mathy-recap",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.13 MLE, the Mathy Recap",
    "text": "26.13 MLE, the Mathy Recap\nThis section is more or less option, but provides a bit more mathematical explanation and presentation for those interested.\n\nThe Likelihood Function\nWell, if we want to find the \\(\\beta\\)s that maximize the likelihood, we’re going to need a likelihood function!\nThey often involve PDFs, and the only one of those that is involved in the linear regression equation is the normal distribution that we assume the residuals to follow.\nWe said that\n\\[ y = \\beta_0 + \\beta_1x + \\epsilon\\]\nwith\n\\[\\epsilon \\sim N(0,\\sigma)\\] Remember, the Normal probability density function is\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\frac{- (x - \\mu)^2}{2 \\sigma^2}}\\] In our case, the random variable \\(x\\) being modeled is the residuals, whose mean \\(\\mu\\) is 0.\nAnd so the \\(i\\)th residual is\n\\[e_i = y_i - \\beta_0 - \\beta_1x_i\\]\nThe likelihood of the \\(i\\)th residual \\(e_i\\) is the normal density from the normal PDF with mean 0, and standard deviation \\(\\sigma\\). So it turns out that in order to estimate the \\(\\beta\\)s this way, we will also have to estimate \\(\\sigma\\), the residual standard error.\nThe joint likelihood of the whole dataset is just the product of the likelihoods of all the individual residuals (i.e., all the individual datapoints); so we have\n\\[L(\\beta_0, \\beta_1, \\sigma; \\mathbf{x}, \\mathbf{y}) = \\prod_{i = 1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\frac{- (e_i - 0)^2}{2 \\sigma^2}} = \\prod_{i = 1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\frac{- (y_i - \\beta_0 - \\beta_1x_i)^2}{2 \\sigma^2}}\\]\nYour turn: take the natural logarithm of the expression above to get the log-likelihood. We’ll compare notes in the next section.\n\n\nLog-Likelihood Function\nThe log-likelihood function for simple linear regression (with just one predictor) is:\n\\[ \\ell(\\beta_0, \\beta_1, \\sigma; \\mathbf{x}, \\mathbf{y}) = \\sum_{i = 1}^{n} -log(\\sigma) - \\frac{1}{2} log(2\\pi) - \\frac{(y_i - \\beta_0 - \\beta_1x_i)^2}{2\\sigma^2}\\]\nIt is left as an exercise for you - if you are interested - to maximize the above expression using methods from calculus (find partial derivatives with respect to \\(beta_0\\) and \\(\\beta_1\\), set equal to 0, and solve for the parameters). Or, it can also be done using linear algebra. You should find that the estimates are identical to the ones you would get using least squares estimation, and/or using lm().\n\nAn estimate for the residual standard error\nYou can also use calculus to find an estimate of \\(\\sigma\\) using the log-likelihood. First, try differentiating the full log-likelihood with respect to \\(\\sigma\\) – again we’ll compare notes in the next section.\n\n\nWhat’s your derivative?\n\\[ \\frac{\\partial{\\ell}}{\\partial{\\sigma}} = \\frac{-n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1x_i)^2  \\]\n\\[ = \\frac{-1}{\\sigma^3} \\bigg{(} n\\sigma^2 - \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)^2 \\bigg{)} = \\frac{-1}{\\sigma^3} \\bigg{(} n\\sigma^2 - \\sum_{i=1}^{n} e_i^2\\bigg{)}\\]\nSetting this equal to 0, we immediately know that the \\(\\frac{-1}{\\sigma^3}\\) can not be 0, giving\n\\[0 = n\\sigma^2 - \\sum_{i=1}^{n} e_i^2\\]\n\\[ \\sigma^2 = \\frac{\\sum e_i^2}{n}\\]\nWhich seems very sensible – the mean of the sum of squared residuals estimates the variance.\nUnfortunately (as so often happens) this estimator is biased; an unbiased version that is more often used is:\n\\[ \\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{(n - 2)}\\]",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#probability-distributions",
    "href": "mle.html#probability-distributions",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.14 Probability Distributions",
    "text": "26.14 Probability Distributions\nThis may be a review, or it may be new to you.\nSo far, we’ve seen the Normal distribution in our course (and you likely saw a lot of that in your intro stat course, as well).\nThere are A LOT MORE options out there! Really, click the link to check out an overwhelming schematic.\nEach probability distribution is a function that matches possible values of a variable (usually depicted on the x-axis) with a measure of how often they occur (usually shown on the y-axis).\n\nDiscrete & Continuous Distributions\nWe can classify probability distributions into the categories of discrete and continuous distributions.\n\nDiscrete distributions model variable that can take on a discrete set of values (for example, the number of successes in n trials, or the number of birds spotted in the forest). For these, the function values are a set of probabilities that sum to 1.\nContinuous distributions model variables that can take on any numeric value within a specified range (for example, proportion between 0 and 1, or height of people in cm). For these, the function values are in “density” or “likelihood” units – they are scaled such that the total area under the functions’ curve is 1.\n\n\n\nSupport\nThe support of a distribution describes the range of values a variable can have (it might be 0-1, 0-\\(\\infty\\), \\(-\\infty\\) - \\(\\infty\\), or (for the Uniform!) values between some specified minimum and maximum).\nKnowing all this, you can choose a distribution that is a good fit for a variable by matching its type and support.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#many-pdfs",
    "href": "mle.html#many-pdfs",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.15 Many PDFs",
    "text": "26.15 Many PDFs\nHere are some flow charts that try to illustrate the variety of probability distributions that are out there, organized by their shape (symmetry/skew, number of modes…) and support:\n\n\n\n\n\nFigure 6A.15 from Damodaran’s ‘Probabilistic approaches to risk’, republished in Fong Chun Chan’s Blog\n\n\n\n\n(You can also view the image online.)\n\n\n\n\n\nFrom Ghanegolmohammadi et al. 2022, BMC Biology\n\n\n\n\n(You can also view the image online.)",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#distributions-in-r",
    "href": "mle.html#distributions-in-r",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.16 Distributions in R",
    "text": "26.16 Distributions in R\nIn R, each distribution has 2 functions we may use often. (Fill in the ___ with the (often shortened) name of the distribution.)\n\nd____() returns the value(s) of the function corresponding to given variable value(s). The result are in Likelihood units: either probability for discrete distributions, or density for continuous ones. The first input is x, the variable value(s). Examples: dnorm() for Normal, dpois() for Poisson, dbinom() for Binomial, dgamma() for Gamma, dbeta() for Beta…\nr____() returns random sample(s) drawn from the specified distribution. The first input is n, the number of samples to draw.\n\nTo get information about parameter/input names for a specific function, ask R for help…for example,",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#yikes",
    "href": "mle.html#yikes",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.17 Yikes",
    "text": "26.17 Yikes\nDon’t get overwhelmed. We know there are lots of distributions out there. But we will ease into it.",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#normal-gaussian",
    "href": "mle.html#normal-gaussian",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.18 Normal (Gaussian)",
    "text": "26.18 Normal (Gaussian)\nThe main distribution we’ll really use for the moment is the Normal or Gaussian distribution, which you should have seen before. It has parameters \\(\\mu\\), the mean or center; and \\(\\sigma\\), the sd (standard deviation) or spread.\nWhy the Normal?? The very short answer is that is comes up, and proves useful, often…both mathematically and with real-world data.\n\nQuestion: Drawing on previous knowledge (and maybe the charts above), is the Normal distribution discrete or continuous? What is its support?\n\n\nChallenge: Try to explain in words what the R code, and output, below mean.\n\n\n\n\n\n\n\n\nHint: remember you can also use gf_dist('dist_name', parameter_name = value, parameter2_name = value) to draw a probability distribution if it helps you visualize!",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#not-normal",
    "href": "mle.html#not-normal",
    "title": "26  Maximum Likelihood Estimation",
    "section": "26.19 Not Normal",
    "text": "26.19 Not Normal\nIn upcoming modules, we will start to learn about additional kinds of regression models that are appropriate when…\n\nThe response variable is count data\nThe response variable is binary data\nThe response variable is bounded between 0 and 1\n(maybe if we’re lucky) The response variable is continuous and quantitative, but only takes on non-negative values and often has a right-skewed distribution\n\nFor each of these, go back to the flow charts we just saw. Which probability distribution(s) will come in handy in each case? We’ll return to them soon!\nFor now, take the PDF/PMF Treasure Hunt sheet, consult the graphics from before with various probability distributions, and see if you can complete all the treasure hunt…this will be very useful later in the course as we build models for different response variable types!",
    "crumbs": [
      "Likelihood",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html",
    "href": "count-regression-1.html",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "",
    "text": "27.1 Module Learning Outcomes\nSo far, all our response variables have been continuous, quantitative values whose values were not bounded (they could be positive or negative, of any magnitude). At least…most of the models we tried that passed assessment probably had such response variables! But as you may have already noted, sometimes variables of interest are categorical, or contain count data, or can’t take on negative values, and so on. This module is our first chance to model a different type of response variable. We begin with count data, using Poisson regression (briefly) before moving on to the more widely-applicable negative binomial regression.\nBy the end of the module you will:",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#module-learning-outcomes",
    "href": "count-regression-1.html#module-learning-outcomes",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "",
    "text": "Outline the mathematical adjustments needed to go from multiple linear regression to regression for count data\nUnderstand how Poisson regression models can be fitted with maximum likelihood estimation\nExplain the difference between Poisson and Negative Binomial generalized linear models (GLMs), noting advantages/disadvantages and choosing an appropriate one\nPractice planning a count regression model using a causal diagram and the n/15 rule\nFit Poisson and Negative Binomial GLMs in R\nBegin doing model assessment for count data GLMs",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#text-reference",
    "href": "count-regression-1.html#text-reference",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.2 Text Reference",
    "text": "27.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapters 4-5\nCourse Notes Chapter 6\nEcological Models & Data in R Chapter 9.4\n\nIt’s suggested that you consider consulting these chapters after doing this tutorial, with particular focus on any topics you found most challenging.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#data-1-ebola",
    "href": "count-regression-1.html#data-1-ebola",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.3 Data #1: Ebola",
    "text": "27.3 Data #1: Ebola\nThis dataset comes from the paper, WASH activities at two Ebola treatment units in Sierra Leone by Michaela Mallow, Lee Gary, Timmy Jeng, Bob Bongomin Jr, Miriam Tamar Aschkenasy, Peter Wallis, Hilarie H. Cranmer, Estifanos Debasu, & Adam C. Levine (https://doi.org/10.1371/journal.pone.0198235).\nThe authors state,\n\n\nThe 2014 outbreak of Ebola virus disease (EVD) in West Africa was the largest in history. Starting in September 2014, International Medical Corps (IMC) operated five Ebola treatment units (ETUs) in Sierra Leone and Liberia. This paper explores how future infectious disease outbreak facilities in resource-limited settings can be planned, organized, and managed by analyzing data collected on water, sanitation, and hygiene (WASH) and infection prevention control (IPC) protocols…The IMC WASH/IPC database contains data from over 369 days. Our results highlight parameters key to designing and maintaining an ETU. High concentration chlorine solution usage was highly correlated with both daily patient occupancy and high-risk zone staff entries; low concentration chlorine usage was less well explained by these measures. There is high demand for laundering and disinfecting of personal protective equipment (PPE) on a daily basis and approximately 1 (0–4) piece of PPE is damaged each day.\n\n\nWe might be able to confirm some of their findings.\nVariables in the dataset include:\n\nMedStaffHRZ, High risk zone staff entries by medical staff\nWASHStaffHRZ, High risk zone staff entries by WASH staff (for cleaning, etc.)\nTotalStaffHRZ1, Total number of staff entries to high risk zone\nPatient Occupancy (number of people being treated)\nTreatment Center Location\nFreshWater amount used\nTotalWater amount used\nNumber of CoverallsUsed (Coveralls are body suits used to prevent infection)\nAmount of PPEDamaged (PPE is personal protective equipment like masks, gowns)\nAmount of HighChlorine disinfectant used\nAmount of LowChlorine disinfectant used\nAmount of ScrubsLaundered\n\nNote that since not all variables were recorded at all locations at all times, there are many missing values in this dataset. Also, you’re advised to ignore (not use) any other variables not included in the list above.\n\nResearch Question\nIs there an association between the number of N95 masks used and the number of patients being treated in the facility?\n\n\nData Access\nYou can find this data at: https://sldr.netlify.app/data/ebola-WASH.csv\n\nwash_data &lt;- read_csv('https://sldr.netlify.app/data/ebola-WASH.csv',\n                      show_col_types = FALSE)\nglimpse(wash_data)\n\n\n\nCausal Diagram\nTake a moment to sketch a causal diagram for this situation.\nI did…and I’d argue that to answer our research question, with the number of N95 masks (N95Used) as the response variable and number of patients (Occupancy) as the key predictor of interest, the only other predictor it makes sense to include in our model is the number of medical staff entering the high-risk zone (MedStaffHRZ).\nAfter making your own diagram, do you agree? If not, what would you do instead?\n\n\nDiagram notes\n\n\n\n\n\nflowchart LR\n  A[Occupancy] --&gt; B(N95Used)\n  C[MedStaffHRZ] --&gt; B\n  C --&gt; D[PPEDamaged]\n  A --&gt; E[Items Used]\n  A --&gt; F[PPEDamaged]\n  A --&gt; G[Water Use]\n  C --&gt; E\n  C --&gt; F\n\n\n\n\n\n\nIn my diagram, the number of med staff entering the HRZ affects the number of masks used directly and also via the number of items of PPE damaged. Patient occupancy affects the number of N95s directly, too.\nOther than that, occupancy affects the number of other items used or washed or damaged, and also the amount of water (of all kinds) used. The number of med staff present also affects the number of various items used/washed/damaged. Location might complicate matters, but we only actually have data on N95 masks used from one location of the two studied. So I can’t identify any confounders or other variables that must be included to model what we would like to model.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#a-bad-idea-multiple-linear-regression-model",
    "href": "count-regression-1.html#a-bad-idea-multiple-linear-regression-model",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.4 A bad idea: multiple linear regression model",
    "text": "27.4 A bad idea: multiple linear regression model\nThis isn’t going to work. We’re just trying it to observe how exactly it doesn’t work. Spoiler: the model fitting will run, happily. But the model will fail assessment and thus will be useless…for regression modeling, “the code ran” is not the same as “the modeling ‘worked’”!\nReview the results and see what issues you spot.\n\nwash_data &lt;- wash_data |&gt;\n  select(N95Used, MedStaffHRZ, Occupancy) |&gt;\n  drop_na()\n\nwash_model &lt;- lm(N95Used ~ MedStaffHRZ + Occupancy, \n              data = wash_data)\nsummary(wash_model)\n\n\nCall:\nlm(formula = N95Used ~ MedStaffHRZ + Occupancy, data = wash_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.9692  -4.2177  -0.6894   3.0177  24.4712 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.7081     1.4706   4.562 1.09e-05 ***\nMedStaffHRZ   1.1292     0.1207   9.352  &lt; 2e-16 ***\nOccupancy     1.3012     0.1996   6.520 1.16e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.885 on 141 degrees of freedom\nMultiple R-squared:  0.6581,    Adjusted R-squared:  0.6533 \nF-statistic: 135.7 on 2 and 141 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#problems-with-the-linear-model",
    "href": "count-regression-1.html#problems-with-the-linear-model",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.5 Problems with the linear model",
    "text": "27.5 Problems with the linear model\nWhat problems do we have with this model (its appropriateness, or goodness of fit to the data)?\n\nNon-constant error variance, with a classic “trumpet” shape\nNon-normality of residuals\nWe didn’t see it here, but in similar cases we may see: some predicted values are less than 0 (which is impossible! N95s cannot magically become unusued; we can’t use a negative number of them).\nWe also note a problem with residual independence, but don’t worry too much about that part today as it’s a separate problem from the rest.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#another-example",
    "href": "count-regression-1.html#another-example",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.6 Another Example",
    "text": "27.6 Another Example\nLet’s consider another example where we might want to fit a regression model, but the response variable is also count data.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#poisson-regression",
    "href": "count-regression-1.html#poisson-regression",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.7 Poisson Regression",
    "text": "27.7 Poisson Regression\nSo what can we do instead of the standard multiple regression model we already know (and love?)?",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#poisson-model-assessment",
    "href": "count-regression-1.html#poisson-model-assessment",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.8 Poisson Model Assessment",
    "text": "27.8 Poisson Model Assessment\nWhat conditions must hold for a Poisson regression model to be appropriate?\n\nResponse variable (y) contains count data\nLinearity: \\(log(\\lambda_{i})\\) is a linear function of the predictor(s) \\(x_1\\), \\(x_2\\), … \\(x_n\\).\nMean = Variance\nIndependence (of residuals)\nThere is not a condition specifying a PDF that the residuals should follow. (No normality condition.)\n\nLet’s review and learn how to check these conditions. How is model assessment different for a Poisson model?",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#problem-overdispersion",
    "href": "count-regression-1.html#problem-overdispersion",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.9 Problem: Overdispersion",
    "text": "27.9 Problem: Overdispersion\nWe will actually not really continue using Poisson regression. Wait! Why not?\nIt turns out that the mean = variance condition of Poisson regression is pretty restrictive, and rarely met in practice. Most often, the residuals are actually overdispersed: the variance is greater than the mean by some amount.\nCases are super rare where it’s not worth estimating just one extra parameter to account for this (if it’s there…which it nearly always is).\nI’m suggesting there’s some variation on a Poisson regression - another count regression model that does account for overdispersion.\nThere is! It’s called Negative Binomial regression (you got it – it uses the Negative Binomial distribution instead of the Poisson, but is otherwise very similar). Some people use quasi-Poisson regression for the same purpose, but our go-to model for any dataset where our response variable is count data will always be the negative binomial model. This allows us to continue fitting models via maximum-likelihood estimation, without resorting to the quasi-likelihood approach the quasi-Poisson model has to empoy.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#negative-binomial-models",
    "href": "count-regression-1.html#negative-binomial-models",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.10 Negative Binomial Models",
    "text": "27.10 Negative Binomial Models\nWhat’s a NB regression and how do we fit one?",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#equations",
    "href": "count-regression-1.html#equations",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.11 Equations",
    "text": "27.11 Equations\nIn the videos above, there may be some errors in how the model equation is presented - specifically, how the residuals are included in the equation.\nAs the videos say, these models do still have residuals! But, to clarify, we might want to write the model equations as shown below.\n\nPoisson Model\n\\[\nlog(\\lambda_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots \\beta_k x_{ki}\n\\]\n\\[\ny_i \\sim Poisson(\\lambda_i)\n\\]\nAnd the residual for the ith data point is \\(e_i\\):\n\\[\ne_i = y_i - e^{(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots \\beta_k x_{ki})}\n\\]\n\n\nNegative Binomial Model\n\\[\nlog(\\mu_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots \\beta_k x_{ki}\n\\]\n\\[\ny_i \\sim NB1(\\mu_i, \\phi) \\text{ or } y_i \\sim NB2(\\mu_i, \\phi)\n\\]\nAnd the residual for the ith data point is \\(e_i\\):\n\\[\ne_i = y_i - e^{(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots \\beta_k x_{ki})}\n\\]\n(but it’s not correct to tack the residuals on to the RHS of the equation…)",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-1.html#whats-next",
    "href": "count-regression-1.html#whats-next",
    "title": "27  Regression for Count Responses: Formulation",
    "section": "27.12 What’s Next",
    "text": "27.12 What’s Next\nNext week, we’ll continue working with count regression a bit more, considering:\n\nOffsets\nPractice with model selection\nPractice with prediction plots & interpretation\n\nFor now, enjoy your brand new skills:\n\nFitting regression models to count data\nUsing DHARMa scaled residuals to check mean-variance conditions during model assessment",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Regression for Count Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html",
    "href": "count-regression-2.html",
    "title": "28  Interpreting Count Regression",
    "section": "",
    "text": "28.1 Module Learning Outcomes\nThis module we continue working with regression models for count data, delving further into model assessment and interpretation and practicing. First, we need more practice identifying when to use a count model; but even more, we need practice carrying out the full modeling process with this new data type.\nBy the end of the module you will:",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#module-learning-outcomes",
    "href": "count-regression-2.html#module-learning-outcomes",
    "title": "28  Interpreting Count Regression",
    "section": "",
    "text": "Gain additional confidence and experience doing regression with count data\nInterpret the results of count regression models using model summaries and prediction plots\nIncorporate results of model selection for count regression models into overall model interpretation\nDefine an offset in the context of a count regression model, make informed decisions about when and when not to include one, and fit models with offsets in R.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#text-reference",
    "href": "count-regression-2.html#text-reference",
    "title": "28  Interpreting Count Regression",
    "section": "28.2 Text Reference",
    "text": "28.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapters 4-5\nCourse Notes Chapter 6\nEcological Models & Data in R Chapter 9.4\n\nIt’s suggested that you consider consulting these chapters after doing this tutorial, with particular focus on any topics you found most challenging.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#selection-for-count-models",
    "href": "count-regression-2.html#selection-for-count-models",
    "title": "28  Interpreting Count Regression",
    "section": "28.3 Selection for Count Models",
    "text": "28.3 Selection for Count Models\nThe selection procedure is essentially the same for negative binomial (or Poisson) models as for the linear models we’ve seen before.\n\nHypothesis Tests for Count Models\nThis process is much the same as it was for multiple linear regression models - from the car::Anova() function we use to carry out the tests all the way to the interpretation.\nHere’s an example…Remember our Ebola WASH data and model from last module? Add your own code below to do ANOVA-based model selection\n\n\n\n\n\n\n\n\n\n\nWhat do you learn from the ANOVA results? Select all correct answers.\n\n There is very strong evidence that the patient occupancy is associated with the number of medical staff entering the high-risk zone According to these results, there is no evidence of any of the predictors being associated with the response. There is not enough data to draw any conclusions based on this analysis, and further study is required. There is very strong evidence that the number of N95 masks used is associated with the number of medical staff entering the high-risk zone There is very strong evidence that the patient occupancy is associated with the number of N95 masks used",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#model-selection-with-aic-bic",
    "href": "count-regression-2.html#model-selection-with-aic-bic",
    "title": "28  Interpreting Count Regression",
    "section": "28.4 Model selection with AIC, BIC",
    "text": "28.4 Model selection with AIC, BIC\nNormally, we’d choose one model selection approach (not do several and then have to compare them and choose between them).\nBut here, just for practice, let’s also try selection using AIC. Add the code you need to check whether Occupancy is associated with the number of N95Used in your wash_model.\nFirst we have to make sure there are no missing datapoints in any variables being used!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nwash_model2 &lt;- glmmTMB(N95Used ~ ..., \n              data = wash_data,\n              family = nbinom2(link = 'log'))\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nwash_model2 &lt;- glmmTMB(N95Used ~ MedStaffHRZ, \n              data = wash_data,\n              family = nbinom2(link = 'log'))\nAIC(wash_model, wash_model2)\n\n\n\n\n\n\nWhy does the model selection code for AIC-based selection (and not ANOVA) require us to first select only the variables used in the model and then drop all rows with missing values?\n\n AIC can only be compared for models with the exact same response variable (no transformations etc.!) AIC cannot deal with missing data, although BIC can. AIC can only be compared for models fitted to the exact same rows of data. It isn't really necessary - we only do that in order to be able to add model predictions to the dataset during assessment\n\nWhat do you learn from the AIC results? Select all correct answers.\n\n There is weak to no evidence that the patient occupancy is associated with the number of medical staff entering the high-risk zone There is very strong evidence that the patient occupancy is associated with the number of medical staff entering the high-risk zone There is very strong evidence that the patient occupancy is associated with the number of N95 masks used There is moderate evidence that the patient occupancy is associated with the number of N95 masks used According to these results, there is no evidence of any of the predictors being associated with the response.\n\n\n\n\n\nClick for some notes about solutions above.\n\nThe fact that AIC can only be compared for models with the exact same response variable is true, but not relevant here.\nWe do remove missing values for this reason, but we also need to do it for IC-based selection…which of the correct answers explains why?",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#prediction-plots",
    "href": "count-regression-2.html#prediction-plots",
    "title": "28  Interpreting Count Regression",
    "section": "28.5 Prediction Plots",
    "text": "28.5 Prediction Plots\nOnce you have fitted a count regression model, how do you interpret it?\nWhen we went to great lengths to make prediction plots for a linear regression model so we could “see” the slope of the predicted relationship, you may have wondered: why bother? I can just look at the slope estimate and get the same insight from that one number, since I know that if the predictor increases by one unit, then the expected response value changes by slope units…\nNow, with a more complicated model equation with a link function, it’s not so easy at all to interpret the model summary or model equation directly. In this situtation, we will really appreciate those prediction plots…\nThe easiest way to understand the effects of different predictors of a count regression is to look at plots of model predictions.\nHowever, as always we don’t want to overlay model predictions on a predictor-vs-response data scatterplot; in a model with more than one predictor, since predictors other than the one we are interested in will influence the predictions, introducing extra variation. Instead, we will make prediction plots again – we’ll construct a new (fake) dataset to make predictions for, in which all predictors but the one we are interested in are held constant.\nWe can use the predict_response() function to produce prediction plots as usual. Below is one for the WASH model and Occupancy; run it to see the prediction plot!\n\n\n\n\n\n\n\n\nThis picture also confirms what we learn from the model selection: The expected number of N95s used does clearly seem to depend on patient occupancy. (Remember, the shaded area tells us a plausible zone where the true line might fall, and the shaded zone in the Occupancy prediction plot definitely could not accommodate a line with 0 slope!) But it also tells us clearly, visually, how Occupancy and N95s used are associated: it would be hard to do the same with just the model summary table, but with this plot we see right away that if the occupancy goes up from 4 to 12 patients the number of masks expected to be used goes up from about 27 to about 37. So, these plots help us show what the model is telling us and provide a nice complement to (but not a replacement for!) visualizations of the actual data.\nNow…go back above and alter the code to make another prediction plot for MedStaffHRZ, please. What do you learn from that one?",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#offsets-why",
    "href": "count-regression-2.html#offsets-why",
    "title": "28  Interpreting Count Regression",
    "section": "28.6 Offsets: Why?",
    "text": "28.6 Offsets: Why?\nIn our model, the response variable was the number of N95 masks that were used, and we included MedStaffHRZ (number of medical personnel entering the high-risk zone) as one of the predictors.\nBut there’s another way to think about it…what if instead of modeling the number of masks used per health center, per day we wanted to model the number of masks used per medical staff member entering the HRZ?\nThere are many cases where we might want to make this adjustment for “effort” – consider, for example:\n\nDolphin surveys. We might want to know how many animals were seen per hour or per km the survey boat travelled…seeing 100 animals per day (if you worked for one hour of the day) is very different from seeing 100 per day (if you worked 12.5 hours)!\nBird counts. Similar rational as for the dolphins.\nSchool survey on crime example from last module. We previously thought to include the school size as a regular predictor of the number of thefts occurring in the school per year. But we could also think of modeling the number of thefts per school year, per school, per enrolled student.\n\nIn this case, it would be natural to adjust for “effort” because rows of the dataset contain counts per…something and the quantity of the something varies row to row.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#offsets-the-math",
    "href": "count-regression-2.html#offsets-the-math",
    "title": "28  Interpreting Count Regression",
    "section": "28.7 Offsets: The Math",
    "text": "28.7 Offsets: The Math\nAn intuitive way to model such a situation would be to use counts per unit effort as the response variable:\n\\[ log(\\frac{\\lambda_i}{effort}) = \\beta_0 + \\dots \\]\nBut notice that this is mathematically equivalent to including \\(log(effort)\\) as an “offset” on the right hand side of the regression equation:\nBy the laws of logarithms,\n\\[ log(\\frac{\\lambda_i}{\\text{effort}}) = log(\\lambda_i) - log(\\text{effort})\\]\nSo adding \\(\\log(\\text{effort})\\) to both sides of the model equation, we get:\n\\[ log(\\lambda_i) = \\beta_0 + \\dots + log(\\text{effort})\\]\nThis helps to explain why, in R code, offsets appear on the right-hand side of the model formula…",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#offsets-in-r",
    "href": "count-regression-2.html#offsets-in-r",
    "title": "28  Interpreting Count Regression",
    "section": "28.8 Offsets: in R",
    "text": "28.8 Offsets: in R\nThis is how we specify models with offsets in R:\n\noffset_mod &lt;- glmmTMB((counts ~ predictor1 + predictor2 +\n                    offset(log(effort)), \n                    family = negbinom2(link = 'log'))\n\nThe log() is important – don’t forget it! You do have to add it yourself (or add a new variable to your dataset that is the natural logarithm of the offset variable, and then use that). R doesn’t automatically take the log for you so you have to ensure it.\nWith our data, we could do this like:\n\n\n\n\n\n\n\n\nTry now to fit an alternative model to the WASH dataset, where rather than modeling the number of N95 masks used, you model the number of masks used per Medical Staff person entering the HRZ, with Occupancy still as a regular predictor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nwash_offset_model &lt;- glmmTMB(N95Used ~ Occupancy + offset(), \n              data = wash_data,\n              family = nbinom2(link = 'log'))\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nwash_offset_model &lt;- glmmTMB(N95Used ~ Occupancy + offset(log_MedStaffHRZ), \n              data = wash_data,\n              family = nbinom2(link = 'log'))\n\n\n\n\nGo back and run model selection for the Occupancy variable now. What do you learn?\n\n\nWhat does the model with the offset say about the relationship between Occupancy and the number of masks used per medical person entering the HRZ?\n\n Because of the offset in the model, selection is not possible in this case and an answer cannot be obtained. Unlike the model without the offset, there is basically no evidence of an association in this case. As we found earlier in the model without the offset, there is strong evidence of an association",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "count-regression-2.html#to-offset-or-not",
    "href": "count-regression-2.html#to-offset-or-not",
    "title": "28  Interpreting Count Regression",
    "section": "28.9 To Offset or Not…",
    "text": "28.9 To Offset or Not…\nA model with and offset is not comparable to one without an offset, because they effectively have different response variables!\nThis means that no, you cannot fit models with and without an offset and then somehow compare them to see which is “better”. Instead, you have to choose which response variable you really want to model – is it the total count (no offset), or the count-per-something (with offset)?\n\nDredge side-note\nNote: We mostly don’t use dredge()! But…if you use dredge() with a model with an offset, be sure to specify the offset as a “fixed” term, i.e. a term that must be included in all models:\n\ndredge(offset_mod, fixed = 'offset(log(effort))')\n\n\n\nHow Do I Decide?\nYou have to decide whether the response variable that you most want to model is the original counts, or the counts per…something, that is, adjusted for the amount of effort (in some sense) expended to obtain those counts.\nSometimes it’s not really clear: would it make more sense to model the number of N95 masks used per med-staff person in the HRZ using an offset, or to use MedStaffHRZ as a “regular” predictor? Using MedStaffHRZ as a “regular” predictor rather than an offset allows for the possibility that the response (number of masks used) depends on MedStaffHRZ, but doesn’t specify exactly what slope (on the link scale) this relationship must have. With an offset, you are effectively requiring a very specific mathematical relationship between the offset variable and the response, in which you are literally modeling the counts divided by the offset variable value. This assumes that if the offset variable goes up by a certain amount, the count will go up proportionally.\nSo in some cases, you clearly, definitely need an offset. For example, the dolphin survey example from earlier – you definitely want to model the number of dolphins seen per distance and time searched, not the number seen per boat trip regardless of how much area was searched or how long the excursion was.\nIn other cases, you will have to make a judgment call, in which case the “regular predictor” option might allow for more flexibility in the proposed relationship between the response and the offset variable.",
    "crumbs": [
      "Count Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interpreting Count Regression</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html",
    "href": "binary-regression-1.html",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "",
    "text": "29.1 Module Learning Outcomes\nAnother module, another response variable type! This module, we’ll consider how to model a response that is a binary categorical variable. We will start with the mathematics of the model equation and then see how to adjust our model planning, fitting, assessment, and interpretation to this new case. Initially, we will consider the simplest and maybe most common set-up for binary data: one-trial-per-row data.\nBy the end of the module you will:",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#module-learning-outcomes",
    "href": "binary-regression-1.html#module-learning-outcomes",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "",
    "text": "Understand how the regression equation must be modified to accommodate a binary response variable\nDistinguish between logit, probit, and complementary log-log link functions and justify the choice of one or the other for a model\nState how the n/15 rule is modified in the case of binary data (n is replaced by the number of successes or failures, whichever is smaller)\nDo model planning, fitting, assessment, and interpretation one-trial-per-row binary-response datasets\nComplete the whole modelling process (plan/explore, fit, assess, and select/interpret) for binary data",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#text-reference",
    "href": "binary-regression-1.html#text-reference",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.2 Text Reference",
    "text": "29.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 6\nCourse Notes Chapter 9\nRegression Modeling Strategies Chapters 10-11\n\nIt’s suggested that you consider consulting these chapters after doing this tutorial, with particular focus on any topics you found most challenging.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#data-source",
    "href": "binary-regression-1.html#data-source",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.3 Data Source",
    "text": "29.3 Data Source\nThe dataset used here is on Alaskan wood frogs, detailing some physical characteristics, habitat characteristics, and the number of developmental and other abnormalities found in the frogs. It was originally obtained from:[http://datadryad.org/resource/doi:10.5061/dryad.sq72d].\nWhat the researchers were really interested in studying was whether the frogs’ abnormalities were associated with several sources of stress in their habitat, including predators hunting the frogs and tadpoles as well as the amount of toxins contaminating their habitat. So, they also measured the abundance of frogs’ predators and levels of various contaminants in the water at the study sites where the frogs were captured.\nThe data file can be accessed online at: [http://sldr.netlify.com/data/frog-abnormalities.csv]\nAnd so can be read into R as shown below:\n\n\nRows: 9,011\nColumns: 17\n$ collection_id                 &lt;chr&gt; \"KNA1021-RASY-080712\", \"KNA1024-RASY-080…\n$ frog_id                       &lt;chr&gt; \"15\", \"13\", \"24\", \"5\", \"26\", \"47\", \"18\",…\n$ gosner_stage                  &lt;chr&gt; \"stage 45\", \"stage 44\", \"stage 45\", \"sta…\n$ tail_length                   &lt;dbl&gt; 2, 20, 1, 3, 17, 33, 16, 1, 2, 0, 40, 32…\n$ frog_comments                 &lt;chr&gt; NA, NA, NA, NA, NA, \"~ 3mm of right thig…\n$ abnormal                      &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Ye…\n$ bleeding_injury               &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ skeletal_abnormality          &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Ye…\n$ eye_abnormality               &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ surface_abnormality           &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ site                          &lt;chr&gt; \"KNA1021\", \"KNA1024\", \"DNR1069\", \"KEN109…\n$ date                          &lt;chr&gt; \"8/7/12\", \"8/8/12\", \"8/6/12\", \"8/6/12\", …\n$ year                          &lt;chr&gt; \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\"…\n$ coll_date                     &lt;chr&gt; \"6/8/12\", \"6/7/12\", \"6/3/12\", \"6/4/12\", …\n$ other_invert_relative_density &lt;dbl&gt; 607, 503, 788, 352, 171, 503, 2917, 503,…\n$ dragonfly_relative_density    &lt;dbl&gt; 6, 8, 3, 1, 6, 8, 100, 8, 66, 12, 35, 10…\n$ detectable_analytes           &lt;dbl&gt; 24.00000, 29.33333, 15.33333, 17.33333, …\n\n\nVariables in the dataset include:\n\nSome sample identifiers: collection_id, frog_id, site\nSome information about time of data collection: date, year, coll_date\nData on the size and developmental stage of the frog: gosner_stage, tail_length (which is longer for young frogs, that is, tadpoles)\nWhether or not the frog has any abnormality in general (abnormal), an injury (bleeding_injury), or a specific type of abnormality: skeletal_abnormality, eye_abnormality, surface_abnormality\nRelative abundance of invertebrate predators of frogs: dragonfly_relative_density and other_invert_relative_density\nA rough summary of water chemical testing results: detectable_analytes, which measures the average number of chemical contaminants (out of a set list that was tested for at every site) detectable in the sample",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#dataset-size-n15-revised",
    "href": "binary-regression-1.html#dataset-size-n15-revised",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.4 Dataset Size: n/15 revised",
    "text": "29.4 Dataset Size: n/15 revised\nFor linear regression and count regression, we used a rule-of-thumb that said our dataset should have about 15 rows for every model parameter we wanted to try to estimate. We called this the \\(\\frac{n}{15}\\) rule – take the number of rows in your dataset (\\(n\\)), divide by 15, and that’s about how many parameters you can expect to reliably estimate when fitting a regression model to that dataset.\nFor binary data, it’s a bit different. Imagine you had 10,000 rows of data and your response variable was the number of people who had eyes of two different colors. If you data matched the US population, you might expect to have about 67 “successes” – people with 2 different colored eyes. At first 10,000 seems like a huge sample, but counting the successes makes you realize you might not actually be able to estimate even the proportion with this trait – much less how it depends on lots of predictors – with your \\(n = 10000\\) dataset.\nWe have a different sample-size rule of thumb for binary data: call it \\(\\frac{m}{15}\\)…\nFirst, count the number of successes and the number of failures in your dataset (for the example above, it would be 67 and 9933). Take the smaller of the two numbers; call that \\(m\\). Divide by 15 to get a rough estimate of the number of parameter you’d be able to estimate well in a regression model fit to that data.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#causal-diagram",
    "href": "binary-regression-1.html#causal-diagram",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.5 Causal Diagram?",
    "text": "29.5 Causal Diagram?\nYou’re encouraged to pause here and read the paper’s abstract (below) – think about what causal diagram you’d draw and what model you’d plan to fit, if you were to try to replicate these results?\n\n\nThe repeated occurrence of abnormal amphibians in nature points to ecological imbalance, yet identifying causes of these abnormalities has proved complex. Multiple studies have linked amphibian abnormalities to chemically contaminated areas, but inference about causal mechanisms is lacking. Here we use a high incidence of abnormalities in Alaskan wood frogs to strengthen inference about the mechanism for these abnormalities. We suggest that limb abnormalities are caused by a combination of multiple stressors. Specifically, toxicants lead to increased predation, resulting in more injuries to developing limbs and subsequent developmental malformations. We evaluated a variety of putative causes of frog abnormalities at 21 wetlands on the Kenai National Wildlife Refuge, south-central Alaska, USA, between 2004 and 2006. Variables investigated were organic and inorganic contaminants, parasite infection, abundance of predatory invertebrates, UVB, and temperature. Logistic regression and model comparison using the Akaike information criterion (AIC) identified dragonflies and both organic and inorganic contaminants as predictors of the frequency of skeletal abnormalities. We suggest that both predators and contaminants alter ecosystem dynamics to increase the frequency of amphibian abnormalities in contaminated habitat. Future experiments should test the causal mechanisms by which toxicants and predators may interact to cause amphibian limb abnormalities. - Reeves et al. 2010, https://doi.org/10.1890/09-0879.1",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#causal-diagram-1",
    "href": "binary-regression-1.html#causal-diagram-1",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.6 Causal Diagram",
    "text": "29.6 Causal Diagram\nOne causal diagram consistent with the abstract, using variables from our dataset, might be based particularly on the statement that “toxicants lead to increased predation, resulting in more injuries to developing limbs and subsequent developmental malformations”…\n\n\n\n\n\n\n\n\n\nThink: what model formula might we use to figure out whether chemical contamination is associated with higher rates of skeletal_abnormalities?\n\nModel Formula\nWe might consider…\nThink about:\n\nWhy did I not include injuries? (Deformities of the skeleton persist, but the injuries that might mediate them will be much more short-lived and thus super rare to detect)\nWould you have included site? (We’ll think about whether and how to include such predictors later in the course in a lot more detail.)\nWhy isn’t the frog’s developmental stage in there? Younger frogs may be more likely to be attacked, and there may be other interactions there too. Not having thought this through more is probably a mistake! Can you do it, and propose a better model? Possible extra credit if you want to give it a try.\n\nBut assuming we go with the formula above… if we wanted to fit that model, we don’t know how yet.\nThe big problem is that our response, skeletal_abnormality, is a binary variable – categorical with two categories. Neither linear regression nor a count model will work at all. (You could try assigning value 0 for no abnormality and 1 for abnormality, but you will run into all sorts of assessment issues basically every time, not to mention that you’ll very frequently predict values below 0, above 1, and in between 0 and 1. We can do much better!)",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#model-equation",
    "href": "binary-regression-1.html#model-equation",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.7 Model Equation",
    "text": "29.7 Model Equation\nHow will our regression model equation work for binary response variables? First, let’s recap the other response variable types we’ve seen so far.\n\nLinear Regression\nRecall, for linear regression we fitted a model for continuous (numeric) response variable \\(y\\) according to:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon \\]\nwhere \\(x\\)s are the \\(k\\) predictor variables, \\(\\beta\\)s are the parameters to be estimated by the model, and \\(\\epsilon \\sim N(0,\\sigma)\\) are the model residuals.\n\n\nCount Regression\nWhen our response variable was a count variable, we modified our equation to:\n\\[log(\\lambda_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + ...\\beta_k x_{ki}\\]\npositing that \\(y_i \\sim Pois(\\lambda_i)\\) for Poisson regression; similarly for quasi-Poisson or negative binomial regression, we just replaced that Poisson distribution with a negative binomial distribution, \\(y_i \\sim NBinom(\\mu_i, \\sigma_i)\\).\nAnd of course, there are still residuals, defined as \\(e_i = y_i - \\hat{y}_i\\)…\n\n\nBinary Regression\nWhat if our response variable is logical, also known as binary, like it is now – a categorical variable with just two possible values? We will designate one of the two values a “success,” and then we want to predict the probability of success as a function of some set of predictors. What will our model equation look like in this case?\nFirst, we’ll need to think about what probability distribution might come into play to model a binary variable. The obvious choice is the binomial distribution, which models the observed number of “successes” as a function of the number of “trials” and the probability of success, \\(p\\), in each trial. Specifically, it says that the probability of observing \\(k\\) successes in \\(n\\) trials will be given by:\n\\[P(X = k) = p^k(1-p)^{(n-k)}\\]\nWe can use this, in much the same way we used the Poisson distribution and the negative binomial with count data! Again we need to adjust our model equation…\nWe CANNOT write\n\\[\\text{skeletal_abnormality} = \\beta_0 + \\beta_1\\text{dragonfly_relative_density} + ... \\]\nthe left-hand-side there, skeletal_abnormality, is a categorical variable so there’s just NO way it can “equal” a real number.\nBut hey, check out the parameters of the binomial distribution – specifically, \\(p\\). That’s the probability of “success”, or in our example, it could be the probability of having a skeletal abnormality. We could say\n\\[\\text{skeletal_abnormality}_i \\sim Binom(n=1, p_i)\\]\nIn other words, our categorical, binary response follows a binomial distribution with some probability of success \\(p_i\\) for each case \\(i\\), and those \\(p_i\\) will depend on our predictors. That actually makes sense!\nBut…we STILL can’t write\n\\[p_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k\\]\nBecause \\(p_i\\) is bounded between 0 and 1 and the right hand side of the equation is…not, at all.\nWe need a link function that takes values between 0 and 1 as input, and maps them to values along the real numberline. One such superhero function is the logit function, defined as \\(logit(p) = log(\\frac{p}{(1-p)})\\). This function maps probabilities to positive and negative real numbers, effectively “spreading them out” from the range 0-1 to the full range of real numbers.\nSo our binary regression equation is…\n\\[ logit(p_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k\\]\nHow does this equation relate back to our desired response variable again?\nWell, the \\(i\\)th observation of the response variable is assumed to follow a binomial distribution with probability \\(p_i\\) of “success” (\\(y_i \\sim Binom(n_i, p_i)\\)).\n\\(n_i\\) depends on the setup of the data and doesn’t need estimating – often n = 1 for every row of the dataset, as here where the response is a two-valued categorical variable and each row is one frog. We can think of each frog as the subject of one binomial trial, with success/failure meaning abnormality/lack of abnormality of the frog.\n(And there are still residuals, which are the difference between the observed and expected response, \\(e_i = I_{success} = p_i\\), where \\(I_{success}\\) is 0 if the \\(i\\)th data point is a “failure” and 1 if “success.”)",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#data-setup",
    "href": "binary-regression-1.html#data-setup",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.8 Data setup",
    "text": "29.8 Data setup\nWe would like to model the proportion frogs with abnormalities as a function of a set of covariates. The variable skeletal_abnormality has values “Yes” and “No”. In R, if we use this variable as our response, how will R determine which level (value of the variable) is a “success”?\nR uses the FIRST variable value as “failure” and the second as “success” – this kind of makes sense if you imagine coding 0 for failure and 1 for success (and then sorting in numeric/alphabetical order).\nIf you have a categorical variable with informative value labels…\nyou will need to make sure that the “base” (first) level is the one you want to equate with “failure”.\nSo, just to check, what values does our response variable have and which one is first and which second?\n\n\nNULL\n\n\nEEK! “NULL”? That can’t be right!\nWhen you read in data on categorical variables with read_csv(), it reads them in as “character” type variables.\nR has another type of variable that can be used for categorical data, too: “factor” type.\nWe need a factor to be able to use the levels() function. But more importantly: our model fitting functions for this section will only work with factor type response variables (or logical ones…but we’ll prefer factors.)\nSide note: you can get the unique values of a non-factor variable with function unique() instead…but not here.\nSo, we need to force our variable to be a factor, then pull out the levels()…\n\n\n[1] \"No\"  \"Yes\"\n\n\nIf you do need to rearrange the levels, one way to do it is to use the forcats::fct_relevel() function. Example:\n\n\n[1] \"No\"  \"Yes\"",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#fit-model",
    "href": "binary-regression-1.html#fit-model",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.9 Fit Model",
    "text": "29.9 Fit Model\nLet’s try fitting a model for skeletal_abnormality as we planned earlier.\n\nfrog_model &lt;- glmmTMB(skeletal_abnormality ~ \n                        detectable_analytes + \n                        dragonfly_relative_density +\n                        other_invert_relative_density,\n                 data = frogs,\n                 family = binomial(link = 'logit'))\nsummary(frog_model)\n\n Family: binomial  ( logit )\nFormula:          \nskeletal_abnormality ~ detectable_analytes + dragonfly_relative_density +  \n    other_invert_relative_density\nData: frogs\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1382.8    1406.8    -687.4    1374.8      2918 \n\n\nConditional model:\n                                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -2.802e+00  1.599e-01 -17.515  &lt; 2e-16 ***\ndetectable_analytes           -3.270e-03  6.075e-03  -0.538  0.59034    \ndragonfly_relative_density     5.209e-03  1.949e-03   2.672  0.00753 ** \nother_invert_relative_density -3.357e-05  1.073e-04  -0.313  0.75425    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#link-functions",
    "href": "binary-regression-1.html#link-functions",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.10 Link Functions",
    "text": "29.10 Link Functions\nHere, notice we have used the logit link function, which is the most common. However, there are other functions that translate proportions to real numbers, and are sometimes used in regression for binary data. Two common options are:\n\nProbit regression: link=‘probit’\nComplementary log-log regression: link=‘cloglog’\n\nThere are not closed-form expressions for the the probit and complementary log-log functions that are easy to write down, so that is why the exact functions are not given here. As shown below, the shapes of these three functions are very similar. So it may come as no big surprise that frequently they provide similar goodness of fit to data (according to IC). If that is the case, choose logit (which makes some of the interpretation of results easier).\n\n\n\n\n\n\n\n\n\nNote: figure is from [http://data.princeton.edu/wws509/notes].",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#assessment-conditions",
    "href": "binary-regression-1.html#assessment-conditions",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.11 Assessment: Conditions",
    "text": "29.11 Assessment: Conditions\nUnder what conditions is a logistic regression model appropriate?\n\nResponse variable is logical – you can characterize it as the outcome of a binomial trial (or a set of independent binomial trials).\n\nSome response variables can be expressed as proportions, but can not be well modelled with binomial regression. For example, you might take one-minute recordings in the woods and measure the proportion of each minute during which bird song was audible. The data will look like proportions, but you can’t think of them as binomial trials and should not model them with binomial regression (what is a “trial” here, and what is a “success”? Make sure you can answer those questions before using binomial regression!)\n\nLinearity: logit(p) should have a linear relationship with each predictor variable. (A bit hard to check predictor-by-predictor - you’d need to group observations some smart way, compute probabilities of succes in each group, and plot the logit() of that as a function of each predictor. We will generally rely on the scaled residual plot to detect issues with this condition, instead.)\nIndependence of residuals: Same as usual.\nMean-variance relationship: The Pearson or Deviance residuals will decrease as a function of fitted value, and should have approximately constant variance as a function of fitted value. But a basic residuals vs. fitted plot is of almost no use to us (almost always looks odd to the untrained eye in the same way; very hard to interpret). So we will use our scaled residual plot again for sure! Its derivation and interpretation is the same as it was in count regression!\nNO distributional assumptions about residuals to check.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#assessment-plots",
    "href": "binary-regression-1.html#assessment-plots",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.12 Assessment: Plots",
    "text": "29.12 Assessment: Plots\n\nCheck Residual Independence\n\n\n\n\n\n\n\n\n\nUh-oh! This looks very, very bad. The ACF values are FAR outside the confidence bounds at all lags, so the residuals are definitely not independent. We are, luckily, just using this model for practice and not inference!\n\n# DO NOT USE THIS IN YOUR OWN ASSESSMENT OF YOUR MODELS!\ngf_point(resid(frog_model, type='pearson') ~ fitted(frog_model)) |&gt;\n  gf_labs(title='frog_model', \n          y=' Pearson\\nResiduals',x='Fitted Values')\n\n\n\n\n\n\n\n\nOh, no! That is not easy to make sense of. The two “lines” in the residuals vs fitted plots correspond with the two possible values of the response variable in the data. But other than that…ummmm…\nRemember, scaled residual plots to the rescue!\n\n# ACUTALLY USE THIS WAY FOR MEAN-VARIANCE CONDITION\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.7. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nsim_frog_res &lt;- simulateResiduals(frog_model)\nplotResiduals(sim_frog_res, quantreg = FALSE)\n\n\n\n\n\n\n\n\nAhhh…there are no trends evident, so no clear indication of a problem the the (logit) linearity condition.\nThe mean-variance relationship also seems to be well-modeled, because the scaled residuals are uniformly distributed (vertically) between 0-1.\n\n\nHistogram?\nDo we need a histogram of residuals? NO! We don’t need this!\nThis model doesn’t make any assumptions about the distribution of the residuals. If you insist on making a histogram anyway (say, you forget you don’t need it) it will look really odd:\n\ngf_histogram(~resid(frog_model, type = 'pearson'), \n             bins=15) |&gt;\n  gf_labs(title = 'frog_model', \n          x = 'Residuals', y = 'Count')\n\n\n\n\n\n\n\n\nJust remember - there is not a strict distributional assumption about the residuals (in other words, they don’t have to follow, say, a normal distribution), so we don’t really have to make a histogram of them. The one here is shown just to help you remember that you don’t have to check it, and if you do, it will look “strange” (bimodal like this) yet it is nothing to worry about and there’s no need either to check it, or to worry about it.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#odds-ratios",
    "href": "binary-regression-1.html#odds-ratios",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.13 Odds Ratios",
    "text": "29.13 Odds Ratios\nThe odds (or odds ratio) is \\(\\frac{p}{1-p}\\) – the ratio of success to failure. So if P(success) = 0.75, then the odds will be \\(\\frac{0.75}{0.25}\\) = 3 or “three to one” – you will usually succeed three times for every failure.\nRemember, the logit function was \\(logit(x) = log(\\frac{p}{1-p})\\)?\nIn other words, the logit is the log of the odds ratio.\nThis means that the coefficients (the \\(\\beta\\)s!) of a binary regression model with logit link function have special interpretations in terms of odds ratios.\nRemember our model:\n\n\n Family: binomial  ( logit )\nFormula:          \nskeletal_abnormality ~ detectable_analytes + dragonfly_relative_density +  \n    other_invert_relative_density\nData: frogs\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1382.8    1406.8    -687.4    1374.8      2918 \n\n\nConditional model:\n                                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -2.802e+00  1.599e-01 -17.515  &lt; 2e-16 ***\ndetectable_analytes           -3.270e-03  6.075e-03  -0.538  0.59034    \ndragonfly_relative_density     5.209e-03  1.949e-03   2.672  0.00753 ** \nother_invert_relative_density -3.357e-05  1.073e-04  -0.313  0.75425    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo our model equation is…write the equation, y’all! See you on the next page after you do it…\n\nModel equation\n\\[ logit(p_i) = -2.8 - 0.0033\\text{detectable_analytes} + 0.0052\\text{dragonfly_relative_density}\\] \\[- 0.000034\\text{other_invert_relative_density}\\] Where \\(p_i\\) is the probability of a frog’s having a skeletal abnormality. This probability is different depending on the conditions (contamination level, predation pressure, etc.).\nAccording to this model, the log-odds (logit(p)) for a frog with zero detected analytes and zero invertebrate predators (dragonflies or others) present would be on average -2.8, so the odds of having a skeletal_abnormality for such a frog are \\(e^{-2.8} = 0.061\\).\nThat would be a kind of lucky frog, though! The median frog can expect to have a relative dragonfly density of about 20. If it still had zero other-invert predators, and no detected analytes, its expected logit(p) would be…I mean…you compute what it would be using the equation!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nlogit_p &lt;- -2.8 - 0.0033 * detectable_analytes + 0.0052 * dragonfly_relative_density -\n  0.000034 * other_invert_relative_density \nAbove, the model parameter estimates are filled in – now, what are the predictor variable values? Fill in those too…\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nlogit_p &lt;- -2.8 - 0 + 0.0052 * 20 -\n  0.000034 * 0\nlogit_p\n\n\n\n\nThe log-odds for the median-dragonflies, no-contaminants, no-other-invert-predator frog are -2.7, so the odds of it having a skeletal_abnormality are \\(e^{-2.7} = 0.067\\) – a bit higher than the other lucky frog.\nNotice – there’s another way of thinking about this, too.\nLet \\(\\beta_{dragon}\\) be the estimated parameter value for the dragonfly relative density slope, \\(\\beta_{dragon} = 0.0052\\). If the relative_dragonfly_density increases by 1, then the \\(logit(p) = log(\\frac{p}{(1-p)})\\) will increase by 0.0052.\nThink about exponentiating both sides of the model equation – then instead of the log of the odds, we have the odds on the left hand side; and on the right?\n\n\nModel equation on odds scale\n\\[ \\frac{p}{(1-p)} = e^{-2.8 - 0.0033\\text{detectable_analytes} + 0.0052\\text{dragonfly_relative_density}- 0.000034\\text{other_invert_relative_density}}\\] By the laws of logarithms, \\(e^{a+b} = e^ae^b\\) so we can rewrite that as:\n\\[\\frac{p}{(1-p)}  = e^{-2.8}e^{- 0.0033\\text{detectable_analytes}}e^{0.0052\\text{dragonfly_relative_density}} e^{- 0.000034\\text{other_invert_relative_density}}\\]\nand again by the laws of logarithms, \\(e^{ab} = (e^a)^b\\) so it’s\n\\[\\frac{p}{(1-p)}  = e^{-2.8}(e^{- 0.0033})^{\\text{detectable_analytes}} (e^{0.0052})^{\\text{dragonfly_relative_density}} (e^{- 0.000034})^{\\text{other_invert_relative_density}}\\]\nSo, \\(e^{\\beta_{dragon}} = 0.0052\\) is the multiplier on the odds ratio – every time the relative dragonfly density goes up by 1 unit, the odds of abnormality are multiplied by \\(e^{\\beta_{dragon}} = e^{0.0052} = 1.005214\\).\nAnd in general, \\(e^{\\beta}\\) is the multiplier on the odds ratio for a one-unit change in the predictor variable for which \\(\\beta\\) is the model coefficient.\n(Some folks are really into reporting model results from logistic regression - that is, regression for a binary outcome with a logit link – using these “odds ratio multipliers.” However, we will prefer to just look at, and present, prediction plots on the “natural” probability scale; odd and log-odds and odds ratio multipliers are hard for many to understand…)",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#model-selection",
    "href": "binary-regression-1.html#model-selection",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.14 Model Selection",
    "text": "29.14 Model Selection\nWe can do this just as usual. For example, if we want to use ANOVA:\n\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: skeletal_abnormality\n                               Chisq Df Pr(&gt;Chisq)   \ndetectable_analytes           0.2898  1   0.590339   \ndragonfly_relative_density    7.1417  1   0.007531 **\nother_invert_relative_density 0.0980  1   0.754254   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe small p-value of 0.008 gives pretty strong evidence that increased dragonfly abundance is associated with a small increase in probability of skeletal abnormalities.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#prediction-plots",
    "href": "binary-regression-1.html#prediction-plots",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.15 Prediction Plots",
    "text": "29.15 Prediction Plots\nShown here are example prediction plots. As usual, we can use predict_response() to draw them.\n\n\nData were 'prettified'. Consider using\n  `terms=\"dragonfly_relative_density [all]\"` to get smooth plots.\n\n\n\n\n\n\n\n\n\nThis is consistent with our model selection findings – there’s an increase in abnormalities as dragonfly abundance increases. However, remember that our model REALLY failed the residual independence check during model assessment, so we’re just making these plots and doing selection for show – we can’t draw any valid conclusions based on this model!\n\n\nData were 'prettified'. Consider using `terms=\"detectable_analytes\n  [all]\"` to get smooth plots.\n\n\n\n\n\n\n\n\n\nBased on this, like the ANOVA, but with the same failed-assessment caveat, we don’t see evidence of an association between our (very crude measure of) chemical contamination and frog abnormalities.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-1.html#celebrate",
    "href": "binary-regression-1.html#celebrate",
    "title": "29  Regression for Binary Responses: Formulation",
    "section": "29.16 Celebrate!",
    "text": "29.16 Celebrate!\nWe now have so many more tools for regression modeling than we began with! We can model…\n\ncontinuous responses with linear regression\ncount responses with negative binomial regression\nbinary responses with binary regression - probably with a logit link, but you can use “probit” or “cloglog” too if you want to\n\nStay tuned for another module when we will add more!",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Regression for Binary Responses: Formulation</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html",
    "href": "binary-regression-2.html",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "",
    "text": "30.1 Module Learning Outcomes\nWe continue our work on binary regression this module, but add a second possible data set-up: multiple-trials-per-row data.\nBy the end of the module you will:",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#module-learning-outcomes",
    "href": "binary-regression-2.html#module-learning-outcomes",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "",
    "text": "Interpret logistic regression coefficients in terms of odds ratios\nExamine a dataset and determine whether a given response variable is…count data, one-trial-per-row binary data, or multiple-trials-per-row binary data\nGain practice and confidence in carrying out the whole modelling process (plan/explore, fit, assess, and select/interpret) for binary data – in either the one-trial-per-row or multiple-trials-per-row format",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#text-reference",
    "href": "binary-regression-2.html#text-reference",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.2 Text Reference",
    "text": "30.2 Text Reference\nRecommended reading for the materials covered in this tutorial can be found in:\n\nBeyond Multiple Linear Regression Chapter 6\nCourse Notes Chapter 9\nRegression Modeling Strategies Chapters 10-11\n\nIt’s suggested that you consider consulting these chapters after doing this tutorial, with particular focus on any topics you found most challenging.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#binary-regression-data-with-more-than-one-trial-per-row",
    "href": "binary-regression-2.html#binary-regression-data-with-more-than-one-trial-per-row",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.3 Binary regression: Data with more than one trial per row",
    "text": "30.3 Binary regression: Data with more than one trial per row\nSo far, the dataset we used for binary regression had one “trial” per row: there was a categorical variable in the dataset with two categories, “success” and “failure” (for the frogs: Abnormal and Normal). We wanted to estimate the probability of “success” as a function of several predictors.\nIf there are multiple trials that all have the same predictor-variable values, we can group them together into one row of data, and just record the number of “trials” and the number of “successes”. (From this, we can also get the number of “failures” = “trials” - “successes”, if needed.) If we have a dataset that is stored in this format, we can still use glmmTMB() to fit a binary regression model. The R code to fit the model changes just a bit, and we are able to do better model assessment a bit more easily.\nAn example follows.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#data",
    "href": "binary-regression-2.html#data",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.4 Data",
    "text": "30.4 Data\nThe dataset used here is a reality-based simulated EIA dataset on duck sightings before and after wind farm installation (impact). Hourly, observers did up to 200 scans of the study area, and for each scan recorded whether duck(s) were seen (a success) or not (a failure).\nThe data file can be accessed online at:\nhttps://sldr.netlify.app/data/EIApropdata.csv\n\nduck_eia &lt;- read_csv('https://sldr.netlify.app/data/EIApropdata.csv',\n                     show_col_types = FALSE)\n\n\nglimpse(duck_eia)\n\nRows: 72\nColumns: 6\n$ successes &lt;dbl&gt; 172, 190, 191, 184, 191, 175, 185, 189, 181, 170, 183, 184, …\n$ trials    &lt;dbl&gt; 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, …\n$ day       &lt;dbl&gt; 7, 13, 27, 3, 20, 24, 15, 20, 22, 10, 13, 21, 2, 5, 18, 8, 1…\n$ month     &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, …\n$ impact    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ failures  &lt;dbl&gt; 28, 10, 9, 16, 9, 25, 15, 11, 19, 30, 17, 16, 17, 15, 22, 22…",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#checking-the-data-setup",
    "href": "binary-regression-2.html#checking-the-data-setup",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.5 Checking the data setup",
    "text": "30.5 Checking the data setup\nWe would like to model the proportion scans with ducks sighted as a function of a set of covariates. Each row of our dataset gives us the number of successes in some number of trials (and also gives the corresponding values of the covariates for ALL those trials). We can also use this kind of summary data with a logistic regression; we will just need to add a column for the number of failures:\n\nduck_eia &lt;- duck_eia |&gt; \n  mutate(failures = trials - successes)",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#dataset-size-fracm15",
    "href": "binary-regression-2.html#dataset-size-fracm15",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.6 Dataset Size (\\(\\frac{m}{15}\\))",
    "text": "30.6 Dataset Size (\\(\\frac{m}{15}\\))\nMaybe you noticed this dataset has only 72 rows and started wondering how many parameters we can afford to estimate. Good thinking! You might have the \\(frac{n}{15}\\) rule-of-thumb in mind.\nBut…remember, it’s more complicated than that!\nWe need to find the number of successes and failures observed, and divide the smaller of those by 15, to get the estimate of the number of parameters we may be able to estimate.\nOr…maybe not…maybe we’d like to have about 15 successes-or-failures-whichever-is-rarer for each combination of categorical predictors…that is, for each row of data…if we want to include them all? There’s not a clear consensus on how to think about “sample size” in this case.\nConsider the exploration below to help you decide: if you wanted, could you include day, month, and impact? All as categorical, or some or all quantitative? There are a quite a few judgment calls here and the choices can matter…but by all measures suggested, this dataset is actually pretty “large” after all.",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#fitting-a-model",
    "href": "binary-regression-2.html#fitting-a-model",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.7 Fitting a model",
    "text": "30.7 Fitting a model\nLet’s try fitting a model for proportion sightings as a function of day, month, and impact.\nWe need a response “variable” that is really 2 variables bound together: a column with the “successes” and a column with the “failures”.\nThese don’t have to be literally called successes and failures – you can use whatever variable names you like or are already present in your data – but the first one of the two should be successes (the thing you want to compute the proportion for) and the second failures.\nIn the model formula, cbind() binds together two columns from the dataset to characterize the response variable.\nTwo models are proposed below. Which one would you use? Why [do I argue] that duck_logit_model2 is a better choice?",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#checking-linearity",
    "href": "binary-regression-2.html#checking-linearity",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.8 Checking linearity",
    "text": "30.8 Checking linearity\nWhat should be linear here?\nWell, logit(p) (where p is the probability of success, for a given set of predictor-variable values) should be a linear function of the predictors.\nWe can actually check this graphically now that we have multiple trials per row of data! (But remember that the effects of other, unplotted predictors may also be influencing the plot that you see…)\nHere, we need to consider: Do we see a linear pattern (or no pattern)? Either is fine!\nIt’s a nonlinear trend that would cause concern.\nFor the month and day data here, we might also consider whether it would make more sense to fit either of them as a categorical predictor rather than numeric.\n(Numeric assumes a monotonic relationship: as the month number, for example, increases, the chance of “success” steadily increases or decreases – probably with a sharp, stark change as we cross over from 12 (December) to 1 (January). That seems silly, right? That’s why I suggested that the model with categorical month was “better” conceptually…)\nSeveral plots are shown below. Consider: which ones work better visually? Which ones correspond to considering day and month as categorical vs. numeric?\n\ngf_point(logit(successes/trials) ~ day, data = duck_eia) |&gt;\n  gf_labs(y = 'logit(p)')\n\n\n\n\n\n\n\ngf_point(logit(successes/trials) ~ month, data = duck_eia) |&gt;\n  gf_labs(y = 'logit(p)') |&gt;\n  gf_lims(x = c(1, 12))\n\n\n\n\n\n\n\ngf_boxplot(logit(successes/trials) ~ factor(day), \n           data = duck_eia, orientation = \"x\") |&gt;\n  gf_labs(y = 'logit(p)', x = 'Day Number')\n\n\n\n\n\n\n\ngf_violin(logit(successes/trials) ~ factor(month), data = duck_eia)|&gt;\n  gf_sina(alpha = 0.4) |&gt;\n  gf_labs(y = 'logit(p)', x = 'Month')",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#model-assessment",
    "href": "binary-regression-2.html#model-assessment",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.9 Model Assessment",
    "text": "30.9 Model Assessment\nWith data set up as proportions (many trials with the number of successes and failures in each row, rather than one row per trial), traditional model assessment plots are a bit more useful. Specifically, we could check the Pearson residuals vs. fitted plot for constant variance as a function of fitted value, to confirm that the mean-variance relationship matches what we expect. (Since the Pearson residuals are already adjusted for the expected variance, we should see approximately constant spread, with values ranging from about -2 to 2 (and not more than a few larger than \\(\\pm\\) 3.))\nHowever, we can also continue to use our scaled residual plot, interpreting it as we have for count and binary one-trial-per-row data…\n\nduck_sim &lt;- simulateResiduals(duck_logit_model2)\nplotResiduals(duck_sim, quantreg = FALSE)\n\n\n\n\n\n\n\n\nWhat do you think? It’s neither fantastic (conditions clearly met) nor terrible (definitely unmet).\n\nLinearity\nFor linearity (already checked in an easier-to-understand way above with the data plots), we don’t really see a clear linear or nonlinear trend here, so there’s no evidence of a problem in the scaled residual plot.\n\n\nMean-variance relationship\nIf the mean-variance condition is met, then the scaled residuals will be spread uniformly in the vertical (up-and-down) dimension.\nHere there seems to be a little bit of clumping at the top and bottom near transformed predicted values of 0.2-0.4 and a “bald spot” at the bottom near 0.6, but the number of points is not that large so this is probably OK.\nWe don’t have strong evidence of a problem with the mean-variance condition.\n\n\nResidual Independence\n\nacf(resid(duck_logit_model2), main = '')\n\n\n\n\n\n\n\n\nThis ACF plot helps us check the residual independence condition. All the ACF values for lags greater than 1 are well within the confidence band, so we have no evidence of dependence in the residuals (at least in the dataset’s current sort order). This condition seems to be met.\n\n\nOverall\nWe found no evidence of problems with any conditions, so our model passed assessment and we can expect it to produce valid conclusions. (Assuming the data itself was collected in an appropriate way and doesn’t contain lies!)",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#model-selection",
    "href": "binary-regression-2.html#model-selection",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.10 Model Selection",
    "text": "30.10 Model Selection\nWe can do model selection as usual. Here, it looks like the best model is the saturated (full) model.\nImagine we wanted to know if exposure to the environmental potential disturbance, impact, was affecting duck presence (controlling for daily and seasonal changes). We might do:\n\n\n\n\n\n\n\n\nWe might also try using model selection to help us decide whether to use quantitative or categorical month and/or day…we know there’s some influence, and want to choose the formulation that fits the data best, using “parameter estimating power” most efficiently.\nHere it looks like day as quantitative and month as categorical works best. Can you use update() and BIC() to show this is true?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nduck_mod_both_cat &lt;- update(duck_logit_model2, . ~ . -day + factor(day))\nduck_mod_day_cat &lt;- update(duck_logit_model, . ~ . -day + factor(day))\nBIC(duck_logit_model, duck_logit_model2, duck_mod_both_cat, duck_mod_day_cat)",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#conclusion",
    "href": "binary-regression-2.html#conclusion",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.11 Conclusion",
    "text": "30.11 Conclusion\n\nHere, our model passed assessment, so we can draw conclusions based on it and expect them to be valid (yay).\nModel selection showed that our key predictor, impact, really was associated with the response in some way.\n\nWait – how so? It is not super helpful to know that wind farm construction or operation affects duck presence, but then not know if the ducks are there more or less often!\n\nsummary(duck_logit_model2)\n\n Family: binomial  ( logit )\nFormula:          cbind(successes, failures) ~ day + factor(month) + impact\nData: duck_eia\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n    580.7     612.6    -276.4     552.7        58 \n\n\nConditional model:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      2.618402   0.123703  21.167  &lt; 2e-16 ***\nday             -0.011735   0.004058  -2.892 0.003829 ** \nfactor(month)2  -0.297341   0.135092  -2.201 0.027734 *  \nfactor(month)3  -0.014290   0.141766  -0.101 0.919711    \nfactor(month)4  -0.287870   0.135761  -2.120 0.033971 *  \nfactor(month)5  -0.258361   0.141491  -1.826 0.067851 .  \nfactor(month)6  -0.404561   0.132914  -3.044 0.002336 ** \nfactor(month)7  -0.141972   0.139208  -1.020 0.307799    \nfactor(month)8  -0.452316   0.132295  -3.419 0.000628 ***\nfactor(month)9  -0.632855   0.128257  -4.934 8.05e-07 ***\nfactor(month)10 -0.665992   0.129788  -5.131 2.88e-07 ***\nfactor(month)11 -1.136006   0.123840  -9.173  &lt; 2e-16 ***\nfactor(month)12 -0.949485   0.127996  -7.418 1.19e-13 ***\nimpact          -0.223631   0.048515  -4.609 4.04e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe \\(\\beta\\) for impact is negative. The values of impact are 0 (for none) and 1 (for impact is present). (We could have treated impact as officially categorical, although the mathematical result is equivalent.)\nSo, we think maybe when impact is present the chances of seeing ducks go down.\nBy how much, we’d have to use odds ratios or think quite hard…OR just make a quick prediction plot!\n\n\n\n\n\n\n\n\nAaah, now we see the value of these plots better!\nBu we also see the downside of lazily not forcing impact to be categorical.\nAll numerical results will be the same if we change it, but we’ll get a more interpretable plot that doesn’t suggest that impact can actually vary continuously between 0 and 1.\n(We could also plot the predict_response()) output by hand to do this, but it’s probably easier/less code to simply update the model.)\n\n\n\n\n\n\n\n\n(It could look even better if instead of using factor() on the fly we changed the original dataset at the start to make impact a categorical variable, maybe even with informative values instead of numeric coding…but that is left as an exercise for you!)\nNow that we have the prediction plot, we can conclude that impact is associated with the probability of duck presence; the evidence we have in our dataset says that we’d expect the probability of ducks being present to go down by about 1.5% in the presence of impact.\nSo, a difference that is pretty consistent and detectable, but small in absolute terms: instead of being seen 91.5 or so percent of the time, they will be seen about 90% of the time. If we wanted to emphasize that, we might even show our prediction plot with a y axis extending from 0-100%:\n\npredict_response(duck_logit_model2b, 'impact') |&gt;\n  plot() |&gt;\n  gf_labs(title = '',\n          y = 'Probability of Duck Presence') |&gt;\n  gf_lims(y = c(0,1))\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nYikes - we lost some of the pretty y axis formatting as percentages, and the error bars are still there but invisible because we’ve zoomed out so far! So, still some work to be done, but food for thought in terms of how to present results. Can you make the plot even better? I did the “percents” thing for you, but what else would you alter? Give it a try…",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "binary-regression-2.html#summary",
    "href": "binary-regression-2.html#summary",
    "title": "30  Binary Regression Another Way: Multiple Trials Per Row",
    "section": "30.12 Summary",
    "text": "30.12 Summary\nWe’ve now seen examples of fitting a binary regression model to data with “one trial per row” or “multiple trials per row” – those terms are just something invented for our course, but they do describe the two common data setups that are possible for binary data.\nWhy do we need both?\nWell, if you use data already collected by someone else, sometimes it will come to you in one format, and sometimes the other.\nIf you know basic data wrangling, it’s not hard to switch between them.\nBut you may want to keep the data in the format it comes in! Data fitted to the same dataset in one-trial-per-row format and multiple-trials-per-row format will yield the exact same parameter estimates. But model selection results will not always be identical, due to nuances in the way that sample size is determined in each case.\nIf someone set up the data with multiple trials per row, often that means that they think of all the trials in a row as a unit or group of some kind, and so it might make sense to respect that.\nAnother key take-away from this module is that if binary data is in a multiple-trials-per-row setup, you can easily mistake it for count data! You’ll notice a column associated with the response variable that has non-negative integer values in it and think, “count data”! But it isn’t!\nHow can you tell which is which and not be fooled?\nIn multiple trials per row binary data, there is a known, definite number of trials. That is, you always know the maximum number of “successes” (that is, the max variable value) for a given row. For example, if you watched 14 sites and marked whether ducks were sighted at each one, you might have “success” in spotting ducks at up to 14 of them and never more.\nIn true count data, there is no “ceiling” to the total counts that might be observed. If you are counting how many ducks you see in an hour, it might be really unlikely, but theoretically you could see any huge number of ducks. There is no set number of “trials” in which you could succeed in seeing ducks (or not); you just count as many as you see with no upper limit.\nGenerally, if you fit a count regression to binary data or a binary regression to count data, there will be problems with model assessment. So, look carefully to make sure what type of response variable you really have, and choose a model to match!",
    "crumbs": [
      "Binary Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Binary Regression *Another Way*: Multiple Trials Per Row</span>"
    ]
  },
  {
    "objectID": "cclicense.html",
    "href": "cclicense.html",
    "title": "DATA 545 Tutorial",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\n\n\nCC BY-SA 4.0"
  }
]